[
["ml.html", "Chapter 13 Machine Learning 13.1 Cross Valdiation 13.2 Naive Bayes 13.3 SVM 13.4 K-means 13.5 Gaussian Mixtures 13.6 PCA 13.7 Viterbi Algorithm 13.8 Gradient Descent 13.9 Nonparametric Bayesian Processes 13.10 Iteratively Reweighted Least Squares 13.11 Neural Netork", " Chapter 13 Machine Learning 13.1 Cross Valdiation Proof that k-fold CV has less variance around the mean than LOOCV – better for k-fold CV. 13.1.1 LOOCV N = 1000 # data size p = .10 # probability of a misclassification set.seed(100) d1 &lt;- rbinom(N,1,p) # bernoulli sampling: sample 1, check if it&#39;s a match mean(d1) ## [1] 0.113 mean(d1)*(1-mean(d1)) # var ## [1] 0.100231 var(d1) ## [1] 0.1003313 10-fold CV k = 10 # number of CV replications n = N/k # sample size for each k from data N set.seed(100) d2 &lt;- rbinom(N,n,p)/n # binomial sampling: from 1000, # sample 100, sum number that are wrong of 100, # divide by sample size to get error rate mean(d2) ## [1] 0.10191 (k/N)*mean(d2)*(1-mean(d2)) # var ## [1] 0.0009152435 var(d2) ## [1] 0.0009229749 note that 10xCV var is &lt; LOOCV var. 13.2 Naive Bayes sex &lt;- rep(c(&quot;M&quot;,&quot;F&quot;),each=4) # feature 1 h &lt;- c(6,5.92,5.58,5.92,5,5.5,5.42,5.75) # feature 2 w &lt;- c(180,190,170,165,100,150,130,150) # feature 3 f &lt;- c(12,11,12,10,6,8,7,9) # feature 4 df1 &lt;- data.frame(sex,h,w,f) uh &lt;- tapply(df1$h,df1$sex,mean) uf &lt;- tapply(df1$f,df1$sex,mean) uw &lt;- tapply(df1$w,df1$sex,mean) sh &lt;- tapply(df1$h,df1$sex,sd) sf &lt;- tapply(df1$f,df1$sex,sd) sw &lt;- tapply(df1$w,df1$sex,sd) new &lt;- data.frame(&quot;h&quot;=6,&quot;w&quot;=130,&quot;f&quot;=8) ps &lt;- table(df1$sex)/length(df1$sex) # P(F) P(M) phgs &lt;- dnorm(new$h,uh,sh) # P(h|F) P(h|M) pwgs &lt;- dnorm(new$w,uw,sw) # P(h|F) P(h|M) pfgs &lt;- dnorm(new$f,uf,sf) # P(h|F) P(h|M) postf &lt;- ps[1]*phgs[1]*pwgs[1]*pfgs[1] # P(sex|h,w,f) = P(sex)*P(h|sex)*P(w|sex)*P(f|sex) postm &lt;- ps[2]*phgs[2]*pwgs[2]*pfgs[2] if (postm &gt; postf) &quot;M&quot; else &quot;F&quot; ## [1] &quot;F&quot; 13.3 SVM library(kernlab) data(iris) train &lt;- iris train$y &lt;-ifelse(train[,5]==&quot;setosa&quot;, 1, -1) train &lt;- train[order(train$y, decreasing=TRUE),] X &lt;- as.matrix(train[,c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]) y &lt;- as.matrix(train$y) n &lt;- dim(X)[1] \\[ \\begin{aligned} \\max \\alpha &amp;W(\\alpha) = \\sum{\\alpha_1} - -.5 \\sum{y_i y_j \\alpha_i \\alpha_j x_i^T x_j}\\\\ &amp;\\text{s.t.} \\quad \\alpha_i \\ge 0\\\\ &amp;\\text{s.t.} \\quad \\sum_{\\alpha_i * y_i} = 0 \\end{aligned} \\] is equivalent to \\[ \\begin{aligned} \\min \\alpha - &amp;\\alpha + 0.5 \\alpha^T * H * \\alpha\\\\ &amp;\\text{s.t.} \\quad \\alpha \\ge 0\\\\ &amp;\\text{s.t.} \\quad A \\alpha \\le 0\\\\ &amp;\\text{where} \\quad H(i,j) = y_i y_j x_i^T x_j\\\\ &amp;\\text{where} \\quad A = y^T\\\\ &amp;\\text{note} \\quad \\max z \\equiv \\min -z \\end{aligned} \\] And ipop is \\[ \\begin{aligned} \\min_\\alpha c &amp;\\alpha + 0.5 x^T H x\\\\ &amp;\\text{s.t.} \\quad b \\le A \\alpha \\le b + r\\\\ &amp;\\text{s.t.} \\quad l \\le \\alpha \\le u\\\\ &amp;\\text{thus} \\quad c=-1\\\\ &amp;\\text{thus} \\quad u = \\infty, l=0 \\quad \\text{but will set $u$ to a large number}\\\\ &amp;\\text{thus} \\quad b=0, r=0 \\quad \\text{to remove this contraint} \\end{aligned} \\] H &lt;- matrix(NA,n,n) for (i in 1:n){ for (j in 1:n){ H[i,j] &lt;- y[i]*y[j]*t(X[i,])%*%X[j,] } } A &lt;- t(y) c &lt;- matrix(rep(-1,n)) l &lt;- matrix(rep(0,n)) b &lt;- 0 u &lt;- matrix(rep(1e5,n)) r &lt;- 0 alpha &lt;- primal(ipop(c,H,A,b,l,u,r)) nonzero &lt;- which(abs(alpha) &gt; 1e-5) w &lt;- matrix(NA,nrow=length(nonzero),ncol=ncol(X)) for (i in seq_along(nonzero)){ w[i,] &lt;- alpha[nonzero[i]]*y[nonzero[i]]*X[nonzero[i],] } w &lt;- colSums(w) b0 &lt;- -(max(sapply(1:sum(y==-1), function(i) matrix(w,ncol=2) %*% X[y==-1,][i,])) + min(sapply(1:sum(y==1), function(i) matrix(w,ncol=2) %*% X[y==1,][i,])))/2 slope &lt;- -w[1]/w[2] intercept &lt;- -b0/w[2] plot(X,pch=19,col=ifelse(1:n %in% nonzero,&quot;green&quot;,&quot;black&quot;)) # green ~ support vectors abline(intercept,slope,col=&quot;red&quot;) sigma = 1 rbf &lt;- rbfdot(sigma = sigma) H_rbf &lt;- kernelMatrix(rbf,X) 13.3.1 Manual rbf kernal XtX &lt;- X%*%t(X) # crossprod(t(X)) XX &lt;- matrix(1, n) %*% diag(XtX) D &lt;- XX - 2 * XtX + t(XX) H &lt;- exp(-D/(2 * sigma)) alpha &lt;- primal(ipop(c,H,A,b,l,u,r)) nonzero &lt;- which(abs(alpha) &gt; 1e-5) w &lt;- matrix(NA,nrow=length(nonzero),ncol=ncol(X)) for (i in seq_along(nonzero)){ w[i,] &lt;- alpha[nonzero[i]]*y[nonzero[i]]*X[nonzero[i],] } w &lt;- colSums(w) b0 &lt;- -(max(sapply(1:sum(y==-1), function(i) matrix(w,ncol=2) %*% X[y==-1,][i,])) + min(sapply(1:sum(y==1), function(i) matrix(w,ncol=2) %*% X[y==1,][i,])))/2 slope &lt;- -w[1]/w[2] intercept &lt;- -b0/w[2] plot(X) abline(intercept,slope,col=&quot;red&quot;) 13.4 K-means library(tidyverse) library(gganimate) distance &lt;- function(x,c){ d &lt;- apply(c,1,function(y) sqrt((x[1]-y[1])^2 + (x[2]-y[2])^2)) w &lt;- which.min(d) return(w) } set.seed(123) x1 &lt;- rnorm(100,5,1) y1 &lt;- rnorm(100,0,2) x2 &lt;- rnorm(100,10,1) y2 &lt;- 3 + rnorm(100,0,2) x3 &lt;- rnorm(100,0,1) y3 &lt;- 8 + rnorm(100,0,2) x4 &lt;- rnorm(100,6,1) y4 &lt;- 15 + rnorm(100,0,1) x5 &lt;- rnorm(100,5.5,.5) y5 &lt;- 8 + rnorm(100,0,1) data &lt;- data.frame(&quot;x&quot; = c(x1,x2,x3,x4,x5), &quot;y&quot; = c(y1,y2,y3,y4,y5), &quot;class&quot; = rep(1:5,each=length(x1))) c &lt;- matrix(c(min(data$x),max(data$x),min(data$x),max(data$x),mean(data$x), min(data$y),max(data$y),max(data$y),min(data$x),mean(data$y)),ncol=2) data.means &lt;- data.frame(cbind(c,0)) names(data.means) &lt;- c(&quot;x&quot;,&quot;y&quot;,&quot;class&quot;) print(true &lt;- ggplot(data,aes(x,y,colour=factor(class),size=2)) + geom_point(alpha=.7) + geom_point(data=data.means,colour=&quot;red&quot;) + geom_text(data=data.means,label=&quot;mean&quot;,vjust=2)) data$class &lt;- 0 c.old &lt;- 0 while (abs(sum(c-c.old)) != 0){ data$class &lt;- apply(data[,1:2],1,function(x) distance(x,c)) c.old &lt;- c c &lt;- cbind(tapply(data$x,data$class,mean),tapply(data$y,data$class,mean)) data.means &lt;- data.frame(cbind(c,0)) names(data.means) &lt;- c(&quot;x&quot;,&quot;y&quot;,&quot;class&quot;) } group_means$iteration &lt;- as.integer(group_means$iteration) ggplot(data=group_means, aes(x,y)) + geom_point(color=&#39;black&#39;,alpha=.7,size=8) + geom_point(data=dat,aes(x,y,color=as.factor(class)),alpha=.3,size=2) + scale_color_brewer(type=&#39;qual&#39;,palette=2) + theme_bw() + theme(legend.position=&#39;none&#39;) + transition_time(iteration) + labs(title = &#39;Iteration: {frame_time}&#39;, x = &#39;&#39;, y = &#39;&#39;) + ease_aes(&#39;linear&#39;) 13.5 Gaussian Mixtures library(tidyverse) data &lt;- c(rnorm(50,12,1),rnorm(50,4,1)) ua &lt;- .19; sa &lt;- .5 ub &lt;- .65; sb &lt;- .5 for (i in 1:1000){ pda &lt;- exp(-(data-ua)^2) pdb &lt;- exp(-(data-ub)^2) pa &lt;- pda/(pda+pdb) pb &lt;- pdb/(pda+pdb) ua &lt;- sum(pa*data)/sum(pa) sa &lt;- sum(pa*(data-ua)^2)/sum(pa) ub &lt;- sum(pb*data)/sum(pb) sb &lt;- sum(pb*(data-ub)^2)/sum(pb) cat(ua,sa,&quot;\\n&quot;,ub,sb,&quot;\\n&quot;) } set.seed(43) N &lt;- 500 p &lt;- rbinom(500,1,.3) y &lt;- (1-p)*rnorm(N,4,1) + p*rnorm(N,-1,1) mu1 &lt;- rnorm(1) mu2 &lt;- rnorm(1) p &lt;- .5 for (t in 1:2500){ gamma1 &lt;- dnorm(y,mu1,1) gamma2 &lt;- dnorm(y,mu2,1) gamma &lt;- (p*gamma2)/((1-p)*gamma1 + p*gamma2) mu_hat1 &lt;- sum((1-gamma)*y)/sum(1-gamma) mu_hat2 &lt;- sum(gamma*y)/sum(gamma) mu1 &lt;- mu_hat1 mu2 &lt;- mu_hat2 p &lt;- sum(gamma)/N } mu1 ## [1] -1.109935 mu2 ## [1] 4.000447 p ## [1] 0.6854409 library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.4 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x ggplot2::alpha() masks kernlab::alpha() ## x purrr::cross() masks kernlab::cross() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() iter &lt;- 1000 mu1_vector &lt;- vector(length=iter) mu2_vector &lt;- vector(length=iter) p_vector &lt;- vector(length=iter) for (t in 1:iter){ gamma1 &lt;- dnorm(y,mu1,1) gamma2 &lt;- dnorm(y,mu2,1) gamma &lt;- (p*gamma2)/((1-p)*gamma1 + p*gamma2) delta &lt;- rbinom(N,1,gamma) mu_hat1 &lt;- sum((1-delta)*y)/sum(1-delta) mu_hat2 &lt;- sum(delta*y)/sum(delta) mu1 &lt;- rnorm(1,mu_hat1,1) mu2 &lt;- rnorm(1,mu_hat2,1) p &lt;- sum(gamma)/N mu1_vector[t] &lt;- mu1 mu2_vector[t] &lt;- mu2 p_vector[t] &lt;- p } qplot(1:iter,mu1_vector,geom=&#39;line&#39;,colour=1) + geom_line(aes(1:iter,mu2_vector),colour=2) + theme(legend.position=&#39;none&#39;) + labs(title=&#39;mu&#39;,x=&#39;iteration&#39;,y=&#39;value&#39;) qplot(1:iter,p_vector,geom=&#39;line&#39;,colour=3) + theme(legend.position=&#39;none&#39;) + labs(title=&#39;p&#39;,x=&#39;iteration&#39;,y=&#39;value&#39;) 13.6 PCA set.seed(4131) x &lt;- 1:101 y1 &lt;- x[1:25] + rnorm(25,0,20) y2 &lt;- x[26:50] + rnorm(25,-2,5) y3 &lt;- x[51:75] + rnorm(25,10,7) y4 &lt;- x[76:101] + rnorm(26,0,15) x &lt;- scale(x) y &lt;- scale(c(y1,y2,y3,y4)) x &lt;- cbind(x,y) x &lt;- (x-mean(x))/sd(x) df &lt;- data.frame(cbind(data.frame(x),rep(LETTERS[1:4],c(25,25,25,26)))) names(df) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;group&#39;) ggplot(df,aes(x=x,y=y,colour=group)) + geom_point() evd &lt;- eigen(cov(x)) pc1 &lt;- evd$vectors[,1] pc2 &lt;- evd$vectors[,2] proj1 &lt;- x %*% pc1 proj2 &lt;- x %*% pc2 vare &lt;- sum(x[,1]*pc1[1])^2 + sum(x[,2]*pc1[2])^2 df2 &lt;- data.frame(cbind(proj1,0),df$group) names(df2) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;group&#39;) ggplot(df,aes(x=x,y=y,colour=group)) + geom_point(alpha=.3) + geom_point(data=df2,alpha=1) set.seed(4131) x1 &lt;- c(rnorm(100,68,5),rnorm(100,78,5),rnorm(100,63,4)) x2 &lt;- c(rnorm(100,215,40),rnorm(100,200,20),rnorm(100,125,15)) x3 &lt;- c(rnorm(100,280,50),rnorm(100,180,15),rnorm(100,95,15)) x1 &lt;- scale(x1) x2 &lt;- scale(x2) x3 &lt;- scale(x3) x &lt;- cbind(x1,x2,x3) df &lt;- data.frame(cbind(data.frame(x),rep(c(&quot;Compact Guys&quot;,&quot;Lanky Guys&quot;,&quot;Women&quot;),each=100))) names(df) &lt;- c(&#39;x1&#39;,&#39;x2&#39;,&#39;x3&#39;,&#39;group&#39;) evd &lt;- eigen(cov(x)) pc1 &lt;- evd$vectors[,1] pc2 &lt;- evd$vectors[,2] pc3 &lt;- evd$vectors[,3] proj1 &lt;- x %*% pc1 proj2 &lt;- x %*% pc2 proj3 &lt;- x %*% pc3 vare &lt;- sum(x[,1]*pc1[1])^2 + sum(x[,2]*pc1[2])^2 + sum(x[,3]*pc1[3])^2 df2 &lt;- data.frame(cbind(proj1,proj2),df$group) names(df2) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;group&#39;) ggplot(df2,aes(x=x,y=y,colour=group)) + geom_point(alpha=1) 13.7 Viterbi Algorithm init &lt;- log(c(.5,.5),2) trans1 &lt;- log(c(.5,.5),2) trans2 &lt;- log(c(.4,.6),2) vis1 &lt;- log(c(.2,.3,.3,.2),2) vis2 &lt;- log(c(.3,.2,.2,.3),2) seq &lt;- &quot;GGCACTGAA&quot; dic &lt;- c(&quot;A&quot;,&quot;C&quot;,&quot;G&quot;,&quot;T&quot;) viterbi &lt;- function(seq){ if (nchar(seq) == 1){ nt &lt;- which(dic == seq) print(comp &lt;- c(init[1] + vis1[nt], init[2] + vis2[nt])) return(c(comp)) }else{ nt &lt;- which(dic == substr(seq,nchar(seq),nchar(seq))) past &lt;- viterbi(substr(seq,1,nchar(seq)-1)) if (past[1] &gt; past[2]) ans &lt;- 1 else ans &lt;- 2 print(ans) print( choice &lt;- c( vis1[nt] + max(past[1] + trans1[1], past[2] + trans2[1]), vis2[nt] + max(past[1] + trans1[2], past[2] + trans2[2]) ) ) } } init &lt;- c(.5,.5) trans1 &lt;- c(.5,.5) trans2 &lt;- c(.4,.6) vis1 &lt;- c(.2,.3,.3,.2) vis2 &lt;- c(.3,.2,.2,.3) path &lt;- c(3,3,2,1) latent1 &lt;- 0 latent2 &lt;- 0 prev1 &lt;- init[1]*vis1[path[1]] prev2 &lt;- init[2]*vis2[path[1]] for (i in 2:length(path)){ prevtemp1 &lt;- prev1*trans1[1]*vis1[path[i]] + prev2*trans2[1]*vis1[path[i]] prevtemp2 &lt;- prev2*trans2[2]*vis2[path[i]] + prev1*trans1[2]*vis2[path[i]] prev1 &lt;- prevtemp1 prev2 &lt;- prevtemp2 } print(prev1 + prev2) ## [1] 0.00384315 13.8 Gradient Descent set.seed(12345) x &lt;- sample(seq(from = 0, to = 2, by = 0.1), size = 50, replace = TRUE) y &lt;- 2 * x + rnorm(50) x &lt;- x - mean(x) y &lt;- y - mean(y) X &lt;- cbind(1,x) y &lt;- as.vector(y) iter &lt;- 5000 a &lt;- 0.01 b &lt;- rep(0,ncol(X)) # b0 loss &lt;- matrix(0,nrow=iter,ncol=1) B &lt;- matrix(0,nrow=iter,ncol=1) for (i in 1:iter){ fb &lt;- (1/2) * norm(y - X %*% b,&quot;F&quot;) loss[i,] &lt;- fb grad.fb &lt;- -(t(X) %*% (y - X %*% b)) # flip sign to DESCEND b &lt;- b - a*grad.fb B[i] &lt;- b[2,] } out &lt;- pretty(c(min(x)-2,max(x)+6),10000) out2 &lt;- sapply(1:length(out), function(x) (1/2) * norm(y - X %*% rbind(0,out)[,x],&quot;F&quot;)) ggplot(tibble(b=B,loss=loss,iteration=1:length(b)) %&gt;% filter(iteration &lt; 50),aes(b,loss)) + ggplot(tibble(b=B,loss=loss,iteration=1:length(b)) %&gt;% filter(iteration &lt; 50),aes(b,loss)) + geom_line(data=tibble(x=out,y=out2),aes(x,y),alpha=.3,size=2) + geom_point(color=&#39;red&#39;,alpha=1,size=1.2) + geom_line(color=&#39;red&#39;,alpha=.8,size=.5) + transition_reveal(iteration,range=c(1L,25L)) + ease_aes(&#39;quadratic-out&#39;) + theme_bw() + xlim(0,3.75) + ylim(2,6) + theme(aspect.ratio=.5) + labs(title = &#39;&#39;, x = &#39;&#39;, y = &#39;Loss&#39;) 13.8.1 Linear Regression 13.8.1.1 SGD N &lt;- 50 X &lt;- cbind(1,runif(N,-1,1),runif(N,-1,1)) k &lt;- ncol(X) theta &lt;- rnorm(k) y &lt;- X %*% theta + rnorm(N) th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 0.01 for (i in 1:10000){ grad &lt;- matrix(rep(0,k),nrow=k) for (j in 1:N){ grad &lt;- grad - X[j,] %*% (y[j]-X[j,]%*%th) } th &lt;- th - eta*grad } t(th) ## [,1] [,2] [,3] ## [1,] 0.07688319 -1.346672 0.9435691 coef(lm(y ~ X[,-1])) ## (Intercept) X[, -1]1 X[, -1]2 ## 0.07688319 -1.34667198 0.94356907 13.8.1.2 Vectorized th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 0.01 for (i in 1:10000){ grad &lt;- t(X) %*% (X %*% th - y) th &lt;- th - eta*grad } t(th) ## [,1] [,2] [,3] ## [1,] 0.07688319 -1.346672 0.9435691 coef(lm(y ~ X[,-1])) ## (Intercept) X[, -1]1 X[, -1]2 ## 0.07688319 -1.34667198 0.94356907 13.8.2 Logistic Regression invlogit &lt;- function(x) 1/(1+exp(-x)) N &lt;- 500 X &lt;- cbind(1,runif(N,-1,1)) k &lt;- ncol(X) theta &lt;- rnorm(k) y &lt;- rbinom(N,1,invlogit(X %*% theta)) N &lt;- 500 X &lt;- cbind(runif(N,-1,1),runif(N,-1,1)) theta &lt;- c(1.5,-3) y &lt;- 1+ifelse(X %*% theta + rnorm(N) &lt; 0, 0, 1) 13.8.2.1 SGD th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 0.25 for (i in 1:5000){ g &lt;- matrix(rep(0,k),nrow=k) for (j in 1:N){ g &lt;- g - X[j,] %*% (y[j]-invlogit(X[j,]%*%th)) } th &lt;- th - eta*g } scores &lt;- X %*% th pred &lt;- ifelse(scores&gt;0,1,0) plot(X,col=y+1,pch=19) plot(X,col=pred+1,pch=19) 13.8.2.2 Vectorized th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- .01 for (i in 1:10000){ g &lt;- t(X) %*% (invlogit(X %*% th)-y) th &lt;- th - eta*g } scores &lt;- X %*% th pred &lt;- ifelse(scores&gt;0,1,0) plot(X,col=y+1,pch=19) plot(X,col=pred+1,pch=19) 13.8.2.3 Newtons th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 1 for (i in 1:5000){ p &lt;- invlogit(X %*% th) S &lt;- diag(c(p * (1-p)),N,N) H &lt;- t(X) %*% S %*% X g &lt;- t(X) %*% (p-y) th &lt;- th - eta * solve(H) %*% g } scores &lt;- X %*% th pred &lt;- ifelse(scores&gt;0,1,0) plot(X,col=y+1,pch=19) plot(X,col=pred+1,pch=19) 13.8.3 Softmax regression N &lt;- 1000 X &lt;- cbind(1,runif(N,0,100),runif(N,0,100)) theta &lt;- rbind(c(100,-1,-1),c(100,0,-2)) y &lt;- 1 + ifelse(X %*% theta[1,] + rnorm(N,0,10) &lt; 0, 0, ifelse(X %*% theta[2,] + rnorm(N,0,10) &lt; 0, 1, 2)) X[,-1] &lt;- scale(X[,-1]) K &lt;- ncol(X) J &lt;- length(unique(y)) 13.8.3.1 SGD th &lt;- matrix(rep(0,K*J),nrow=K,ncol=J) g &lt;- th eta &lt;- .5 for (i in 1:5000){ a &lt;- exp(X %*% th) # exp(thetak&#39; * xi) a_sum &lt;- rowSums(a) # SUMexp(thetaj&#39; * xi) p &lt;- t(sapply(1:N,function(n) a[n,]/a_sum[n])) # P(yi=k|xi,theta) = exp(thetak&#39; * xi)/SUMexp(thetaj&#39; * xi) # -SUM[xi * (1{yi=k} - P(yi=k|xi;theta))] = SUM[xi * (P(yi=k|xi;theta)) - 1{yi=k}] # therefore P(yi=k|xi;theta)) - 1{yi=k} is simply p-1 for all p where yi=k for (n in 1:N){ p[n,y[n]] &lt;- p[n,y[n]] - 1 # P(yi=k|xi;theta)) - 1{yi=k} } p &lt;- p/N # this is not necessary but adjusts the size of the estimates to avoid very large values g &lt;- t(X) %*% p # SUM[xi * (P(yi=k|xi;theta)) - 1{yi=k}] th &lt;- th - eta*g } scores &lt;- X %*% th pred &lt;- apply(scores,1,which.max) plot(X[,-1],col=y,pch=19) plot(X[,-1],col=pred,pch=19) 13.8.3.2 SGD with bias term N &lt;- 100 D &lt;- 2 K &lt;- 3 X &lt;- matrix(0,N*K,D) y &lt;- matrix(0,N*K) ind1 &lt;- 0 ind2 &lt;- 0 for (k in 1:K){ r &lt;- seq(0,1,length=N) t &lt;- seq((k-1)*4,(k)*4,length=N) + rnorm(N)*0.2 X[(1+ind1):(N+ind2),] &lt;- cbind(r*sin(t), r*cos(t)) y[(1+ind1):(N+ind2)] &lt;- k ind1 &lt;- ind1 + N ind2 &lt;- ind2 + N } W &lt;- matrix(0.01*rnorm(D*K),D,K) b &lt;- matrix(0,1,K) step &lt;- 1 reg &lt;- 1e-3 for (i in 1:5000){ a &lt;- exp(X %*% W + rep(b,N*K)) a_sum &lt;- rowSums(a) p &lt;- t(sapply(1:(N*K),function(n) a[n,]/a_sum[n])) for (n in 1:(N*K)){ p[n,y[n]] &lt;- p[n,y[n]] - 1 } p &lt;- p/(N*K) gW &lt;- t(X) %*% p gb &lt;- colSums(p) gW &lt;- gW + reg*W W &lt;- W + -step*gW b &lt;- b + -step*gb } scores &lt;- X %*% W + rep(b,N*K) pred &lt;- apply(scores,1,which.max) plot(X,col=y,pch=19) plot(X,col=pred,pch=19) 13.9 Nonparametric Bayesian Processes 13.9.1 Chinese Restaurant chinese_restaurant &lt;- function(N, alpha){ tables &lt;- vector(length=N) tables[1] &lt;- 1 open &lt;- 2 for (i in 2:N){ choice &lt;- rbinom(1,1,alpha/(i+alpha)) if (choice == 1){ tables[open] &lt;- tables[open] + 1 open &lt;- open + 1 }else{ occupied &lt;- which(tables != 0) prob &lt;- tables[occupied]/(i+alpha) seat &lt;- sample(occupied,1,FALSE,prob) tables[seat] &lt;- tables[seat] + 1 } } return(tables) } chinese_restaurant(30,10) ## [1] 8 3 3 2 1 2 1 4 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 13.9.2 Polyas Urn polyas_urn &lt;- function(N,alpha){ balls &lt;- NULL for (i in 1:N){ choice &lt;- rbinom(1,1,alpha/(length(balls)+alpha)) if (choice == 1){ ball &lt;- rnorm(1) balls &lt;- c(balls,ball) }else{ ball &lt;- balls[sample(1:length(balls),1)] balls &lt;- c(balls,ball) } } return(balls) } rep_polyas_urn &lt;- function(N,alpha,R){ out &lt;- data.frame(replicate(R,polyas_urn(N,alpha))) colnames(out) &lt;- 1:R out %&gt;% gather(r,sample) %&gt;% mutate(r=as.factor(r)) %&gt;% ggplot(aes(x=sample,y = ..scaled..)) + geom_density(colour=&quot;black&quot;,size=1,fill=&quot;darkgreen&quot;) + facet_wrap(~r) + xlim(-3,3) + ylab(&quot;&quot;) + xlab(&quot;&quot;) } rep_polyas_urn(25,500,12) 13.9.3 Stick Breaking stick_breaking &lt;- function(N,alpha){ p &lt;- rbeta(N,1,alpha) len &lt;- 1 w &lt;- p[1] for (i in 2:N){ len &lt;- len-w[i-1] w_new &lt;- p[i]*(len) w &lt;- c(w,w_new) } return(w) } rep_stick_breaking &lt;- function(N,alpha,R){ out &lt;- data.frame(replicate(R,stick_breaking(N,alpha)),1:N) colnames(out) &lt;- c(1:R,&quot;Breaks&quot;) out %&gt;% gather(r,Probability,-Breaks) %&gt;% mutate(r=as.factor(r),Breaks=as.factor(Breaks)) %&gt;% ggplot(aes(x=Breaks,y=Probability,ymin=0,ymax=Probability)) + geom_linerange(colour=&quot;Blue&quot;,size=1) + geom_point(colour=&quot;Blue&quot;,size=4) + scale_y_continuous(lim=c(0,1)) + facet_wrap(~r) + theme(panel.background = element_rect(), title=element_text(size=20), strip.text=element_text(size=13), axis.text=element_text(size=13), axis.title=element_text(size=18,face=&quot;bold&quot;)) + ggtitle(bquote(alpha == .(paste(alpha,collapse=&quot; &quot;)))) } rep_stick_breaking(10,1,12) 13.10 Iteratively Reweighted Least Squares inv_logit &lt;- function(x) return(1/(1+exp(-x))) N &lt;- 100 k &lt;- 1 X &lt;- cbind(1,matrix(runif(N*k,-1,1))) theta_true &lt;- matrix(c(.25,-.75),ncol=1) y &lt;- rbinom(N,1,inv_logit(X %*% theta_true)) summary(glm(y ~ X[,-1], family=binomial(link=&quot;logit&quot;))) ## ## Call: ## glm(formula = y ~ X[, -1], family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7842 -1.2277 0.7490 0.9695 1.2757 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.5825 0.2153 2.705 0.00683 ** ## X[, -1] -0.8247 0.3847 -2.144 0.03206 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.79 on 99 degrees of freedom ## Residual deviance: 126.96 on 98 degrees of freedom ## AIC: 130.96 ## ## Number of Fisher Scoring iterations: 4 irls &lt;- function(X,y,tol=1e-6){ k &lt;- ncol(X) N &lt;- nrow(X) theta &lt;- matrix(rep(0,k),ncol=1) theta_new &lt;- Inf while (max(abs(theta - theta_new)) &gt; tol){ a &lt;- X %*% theta p &lt;- inv_logit(a) s &lt;- diag(c(p*(1-p)),N,N) xsx &lt;- t(X) %*% s %*% X sxt &lt;- s %*% X %*% theta theta_new &lt;- theta theta &lt;- solve(xsx) %*% t(X) %*% (sxt + y - p) } return(theta) } 13.11 Neural Netork import numpy as np import random import cPickle import gzip import os import sys def load_data(): f = gzip.open(&#39;mnist.pkl.gz&#39;, &#39;rb&#39;) training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) def load_data_wrapper(): tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) def vectorized_result(j): e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): return sigmoid(z)*(1-sigmoid(z)) training_data, validation_data, test_data = load_data_wrapper() sizes = [784, 30, 10] num_layers = len(sizes) eta = 3.0 # must be real, not integer (so not 3) epochs = 30 n = len(training_data) n_test = len(test_data) mini_batch_size = 10 biases = [np.zeros((y, 1)) for y in sizes[1:]] weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] # n/mini_batch_size # mini_batches is the result of breaking the training_data into length 25 batches, so 2000 mini_batches for mini_batch in mini_batches: # start gradient at 0 nabla_b = [np.zeros(b.shape) for b in biases] nabla_w = [np.zeros(w.shape) for w in weights] for x, y in mini_batch: delta_nabla_b = [np.zeros(b.shape) for b in biases] delta_nabla_w = [np.zeros(w.shape) for w in weights] # forward pass activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(biases, weights): z = np.dot(w, activation) + b zs.append(z) activation = sigmoid(z) activations.append(activation) # dot product between w1 in layer 1-2 with activation=input, add b1 for layer 2, set output as activation1 # dot product between w2 in layer 2-3 with activation=activation1, add b2 for layer 3, set output as activation2 # backward pass delta = (activations[-1]-y) * sigmoid_prime(zs[-1]) # dC/dz_lj = (a - y) * o&#39;(z), cost wrt output layer delta_nabla_b[-1] = delta # dC/db_lj = delta_lj delta_nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # dC/dw_ljk = a_(l-1)k * delta_lj for l in xrange(2, num_layers): delta = np.dot(weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l]) # dC/dz_lj delta_nabla_b[-l] = delta # dC/db_lj delta_nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # dC/dw_ljk nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, nabla_b)] weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, nabla_w)] test_results = [(np.argmax(feedforward(x,biases,weights)), y) for (x, y) in test_data] test_results = sum(int(x == y) for (x, y) in test_results) print &quot;Epoch {0}: {1} / {2} | mean_w = {3} | mean_nabla_w = {4}&quot;.format(j, test_results, n_test,np.mean(weights[1]),np.mean(nabla_w[1])) "]
]
