[
["index.html", "Worked Bioninformatics, Statistics, and Machine Learning Examples Chapter 1 Introduction", " Worked Bioninformatics, Statistics, and Machine Learning Examples Stephen Woloszynek 2020-02-24 Chapter 1 Introduction This is a bunch of stuff that I either complete wrote, lectured, or adapted during my time completing my PhD. It mostly consists material I used to intro-level statistics classes for biomedical engineers, some machine learning code, and topic model derivations related to my thesis work. "],
["dynprog.html", "Chapter 2 Dynamic Programming 2.1 Introduction 2.2 Rod cutting 2.3 Fibonacci rabbits", " Chapter 2 Dynamic Programming 2.1 Introduction Dynamic programming makes computationally demanding problems manageable by dividing them into a set of subproblems. On its surface, this might sound like a divide-and-conquer approach, but it is in fact different. D&amp;C solves disjoint subproblems recursively. Each subproblem must be calculated from scratch, and their results are combined to reach a final solution. This results in more work than necessary. DP approaches, on the other hand, solve overlapping subproblems and save their results for later use; hence, each subproblem needs to be calculated just once, and overlapping subproblems can inform each other to lessen the computational burden. DP alogorithms are designed as follows (from Introduction to Algorithms, 3rd ed.): Characterize the structure of an optimal solution Recusively define the value of an optimal solution. Compute the value of an optimal solution, typically in a bottom-up fashion. Construct an optimal solution from computed information. 2.2 Rod cutting Say we have a silver rod, and a piece of length i would net us \\(p_i\\) dollars, such that we have the following price table: set.seed(12) N &lt;- 25 C &lt;- 50 l &lt;- seq_len(N) p &lt;- sort(sample(seq_len(C),N,replace=TRUE),decreasing=FALSE) matrix(p,nrow=1,dimnames=list(&#39;price&#39;,l)) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## price 2 4 5 5 8 8 13 16 18 18 24 26 26 27 27 28 34 38 39 42 43 44 46 46 48 We’d like to maximize our profit, so we need to figure out a way to cut our rod such that our pieces total to the largest value. We can do this recursively: cut_recursive &lt;- function(prices,length){ if (length == 0) return(0) q &lt;- -Inf for (i in seq_len(length)){ #cat(sprintf(&#39;len=%s,val=%s\\n&#39;,length-i,cut_recursive(prices,length-i))) q &lt;- max(q,prices[i] + cut_recursive(prices,length-i)) } return(q) } cut_recursive(p,5) ## [1] 10 cut_recursive(p,15) ## [1] 32 cut_recursive(p,20) ## [1] 42 cut_aux &lt;- function(prices,length){ if (R[length + 1] &gt;= 0) return(R[length + 1]) if (length == 0){ q &lt;- 0 }else{ q &lt;- -Inf for (i in seq_len(length)){ cat(sprintf(&#39;len=%s,val=%s\\n&#39;,length-i,cut_aux(prices,length-i))) q &lt;- max(q,prices[i] + cut_aux(prices,length-i)) } } R[length+1] &lt;&lt;- q return(q) } cut_memoized &lt;- function(prices,length){ R &lt;&lt;- rep(-Inf,length + 1) return(cut_aux(prices,length)) } cut_memoized(p,5) ## len=0,val=0 ## len=1,val=2 ## len=0,val=0 ## len=2,val=4 ## len=1,val=2 ## len=0,val=0 ## len=3,val=6 ## len=2,val=4 ## len=1,val=2 ## len=0,val=0 ## len=4,val=8 ## len=3,val=6 ## len=2,val=4 ## len=1,val=2 ## len=0,val=0 ## [1] 10 cut_bottomup &lt;- function(prices,length){ r &lt;- c(0,rep(-Inf,length)) for (i in seq_len(length)){ q &lt;- -Inf for (j in seq_len(i)) q &lt;- max(q,prices[j] + r[i-j+1]) r[i+1] &lt;- q } return(r[length+1]) } cut_bottomup(p,10) ## [1] 20 cut_bottomup(p,15) ## [1] 32 cut_bottomup(p,20) ## [1] 42 2.3 Fibonacci rabbits Let’s try and write a script that can solve a potentially computationally burdonsome problem – a problem involving reproducing rabbits. Say we start with 1 baby rabbit (age 0) at month 1. When the rabbit reaches 1 month of age, it can reproduce, producing a new baby rabbit the following month (month 3). On month 4, the baby rabbit can now reproduce, but our original rabbit will also reproduce, and so on. The rules are therefore Rabbits age each month Baby (age 0) rabbits cannot reproduce And here is a diagram showing the process: Now focus on the number of rabbits for each month: 1, 1, 2, 3, 5, 7. It’s the fibonacci sequence, where the current months total is a sum of the previous month’s total. We can therefore make a function that can recursively calculate the number of rabbits, using the finonacci sequence, only requiring the number of months the process will span across. A quick aside: recursive algorithms are hard. They take some work to get a hang of them. I would not worry about either trying to write recursive algorithms or completely understanding how the code below works. The point of showing them is that there’s often a natural way to tackle a programming problem, but it’s not necessarily always the best way. fib &lt;- function(n){ if (n==1 || n==2){ return(1) }else{ return(fib(n-1) + fib(n-2)) } } fib(5) ## [1] 5 for (n in seq_len(25)) cat(fib(n),&#39; &#39;) ## 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765 10946 17711 28657 46368 75025 Let’s change the problem a little bit. Let’s assume now that the rabbits can die after k months. For \\(k=3\\), we’d have the following process: We can write another recurssive algorithm to tackle this: lifespan_inner &lt;- function(n,y,Y){ if (n==1){ return(1) }else if (y==Y){ return(lifespan_inner(n-1,y-1,Y)) }else if(y==1){ lifespan_inner(n-1,Y,Y) }else{ return(lifespan_inner(n-1,y-1,Y) + lifespan_inner(n-1,Y,Y)) } } lifespan &lt;- function(n,y){ return(lifespan_inner(n,y,y)) } for (n in seq_len(25)) cat(lifespan(n,3),&#39; &#39;) ## 1 1 2 2 3 4 5 7 9 12 16 21 28 37 49 65 86 114 151 200 265 351 465 616 816 But look how much time it takes if we ramp up the number of months to 60: t1 &lt;- Sys.time() for (n in seq_len(60)) cat(lifespan(n,3),&#39; &#39;) t2 &lt;- Sys.time() cat(&#39;Time elapsed:&#39;,round(t2-t1,1),&#39;minutes.&#39;) ## Time elapsed: 5.3 minutes. It’s slow! Now we’ll use a dynamic programming approach. You’re going to need dynamic programming later in the course for genomic sequence alignment, so it’s worth exploring the type of speedup one can obtain with a quite intuitive method, particularly when aimed at computationally demanding tasks often seen in genomics. Again, dynamic programming saves a ton of time and resources by sweeping through a problem as a set of smaller subproblems, while storing the results of each subproblem as you go. Think about the rabbit flow chart. If we wanted to know the number of rabbits present on month 1000, we’d have to add months 999 and 998 together, which require information from months 996 through 998, and so on. A recursive algorithm would calculate the result of 999 independently of 998, and then add them together. Dynamic programming, on the other hand, would have the results from those previously months stored, simply requiring us to look them up. The game here involves the following: Make a \\(n \\times y\\) matrix M, where n is the number of months and y is the rabbit’s lifespan. Row 1 will represent month 1, column 1 will represent baby rabbits, and column y will represent the final month of life for an adult rabbit. Each subsequent row will be a running tally of the number of rabbits in each age group. Because each month is updated sequentially, you only need the information of a previous row (month) to update a current row (month). dynprog &lt;- function(m,y,p=FALSE){ mat &lt;- matrix(0,m,y) mat[1,1] &lt;- 1 for (i in 2:m){ y1 &lt;- mat[i-1,] y2 &lt;- mat[i,] y2[1] &lt;- sum(y1[-1]) y2[-1] &lt;- y1[-y] mat[i-1,] &lt;- y1 mat[i,] &lt;- y2 if (p){ cat(sprintf(&#39;y%s:\\t%s&#39;,i,paste0(mat[i,],collapse=&#39;\\t&#39;))) line &lt;- readline(&#39;&#39;) } } return(rowSums(mat)) } Here are some example answers to check. Note that there may be some variability given the max integer number in R, which is set in the options. If you get the right answer for smaller parameterizations, then your code is correct. dynprog(25,5,TRUE) ## y2: 0 1 0 0 0 ## y3: 1 0 1 0 0 ## y4: 1 1 0 1 0 ## y5: 2 1 1 0 1 ## y6: 3 2 1 1 0 ## y7: 4 3 2 1 1 ## y8: 7 4 3 2 1 ## y9: 10 7 4 3 2 ## y10: 16 10 7 4 3 ## y11: 24 16 10 7 4 ## y12: 37 24 16 10 7 ## y13: 57 37 24 16 10 ## y14: 87 57 37 24 16 ## y15: 134 87 57 37 24 ## y16: 205 134 87 57 37 ## y17: 315 205 134 87 57 ## y18: 483 315 205 134 87 ## y19: 741 483 315 205 134 ## y20: 1137 741 483 315 205 ## y21: 1744 1137 741 483 315 ## y22: 2676 1744 1137 741 483 ## y23: 4105 2676 1744 1137 741 ## y24: 6298 4105 2676 1744 1137 ## y25: 9662 6298 4105 2676 1744 ## [1] 1 1 2 3 5 7 11 17 26 40 61 94 ## [13] 144 221 339 520 798 1224 1878 2881 4420 6781 10403 15960 ## [25] 24485 dynprog(50,5)[50] ## [1] 1085554510 dynprog(70,6)[70] ## [1] 2.685139e+13 "],
["align.html", "Chapter 3 Alignment 3.1 Longest Common Subsequence 3.2 Global Alignment 3.3 Local Alignment 3.4 Local Alignment: Homework 3.5 Global Alignment Code (R) 3.6 Global Alignment Code (Python)", " Chapter 3 Alignment 3.1 Longest Common Subsequence We’re going to tackle alignment programatically in three steps: (1) finding the longest common subsequence, (2) performing a global alignment, and finally (3) performing a local alignment. These steps build upon one another, so the order should seem natural. A subsequence is simply an ordered set; it need not be consecutive. For example, if we had the sequence ABCDEFG, then ABC, ACF, and DFG would all be subsequences, whereas AGD would not. We aim to find the largest subsequence that two nucleotide sequences share. This boils down to essentially an alignment problem only involving insertions and deletions. The algorithm is as follows: DEFINE LCS: Given sequences x and y 1 Create empty an matrix S for scores and an empty matrix B for the backtracking path 2 For i in 2 to length x ..3 For j in 2 to length y ....4 Obtain the score of the upper S[i,j-1], upper-left diagonal S[i-1,j-1], .... and left S[i-1,j] cells. ...... if the current nucleotides in x and y are the same, such that x[i-1] == y[i-1], ...... set s[i,j] to the maximum score among S[i,j-1], S[i-1,j-1] + 1, and S[i-1,j], ...... where &quot;+1&quot; is the bonus for a match; ...... if x[i-1] != y[i-1], then set s[i,j] to the maximum score among S[i,j-1] and S[i-1,j] ....6 Record the position of the maximum score (upper, upper-left, or left) ..End loop End loop 7 Perform backtrack We’re going to create a function that computes the LCS based on the pseudocode above. It’ll take two sequences, x and y. We’ll call the function find_lcs, and it’ll wrap our code like so: find_lcs &lt;- function(x,y){ ## code will go here } We’ll start with the following two sequences: x &lt;- &#39;AGCAGACACGTGAT&#39; y &lt;- &#39;ATCACCGGTAT&#39; Now, the actual algorithm can be written a bunch of ways, but we’re first going to split the sequence strings into character vectors, for example: unlist(strsplit(x,&#39;&#39;)) ## [1] &quot;A&quot; &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;G&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;G&quot; &quot;T&quot; &quot;G&quot; &quot;A&quot; &quot;T&quot; so, x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) We also will create two variables, m and n, that will store the length of our sequences: m &lt;- length(x) n &lt;- length(y) Lastly, we need to preinitialize our score and backtrack matrices. Recall that each is padded by an additional row and column that is filled in with either zeros or some type of penalty: s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) This gives us matrices that look like ## A T C A C C G G T A T ## 0 0 0 0 0 0 0 0 0 0 0 0 ## A 0 0 0 0 0 0 0 0 0 0 0 0 ## G 0 0 0 0 0 0 0 0 0 0 0 0 ## C 0 0 0 0 0 0 0 0 0 0 0 0 ## A 0 0 0 0 0 0 0 0 0 0 0 0 ## G 0 0 0 0 0 0 0 0 0 0 0 0 ## A 0 0 0 0 0 0 0 0 0 0 0 0 ## C 0 0 0 0 0 0 0 0 0 0 0 0 ## A 0 0 0 0 0 0 0 0 0 0 0 0 ## C 0 0 0 0 0 0 0 0 0 0 0 0 ## G 0 0 0 0 0 0 0 0 0 0 0 0 ## T 0 0 0 0 0 0 0 0 0 0 0 0 ## G 0 0 0 0 0 0 0 0 0 0 0 0 ## A 0 0 0 0 0 0 0 0 0 0 0 0 ## T 0 0 0 0 0 0 0 0 0 0 0 0 ## A T C A C C G G T A T ## &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## A &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## G &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## C &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## A &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## G &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## A &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## C &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## A &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## C &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## G &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## T &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## G &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## A &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## T &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; Putting it all together, our function should so far look like: find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) ## more code will go here } Now, we have to loop through these matrices just like we would if we were doing this problem on paper. We start at the upper left corner and work our way down to the bottom right. As we move down, we pay close attention to the upper, left, and upper-left diagonal cells relative to our current position. Starting with the loops, our function should look like find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ ## more code will go here } } } Pay careful attention to the indexes we’re iterating over; this is critical. Position i=2, j=2, means different things with respect to the score and backtracking matrices compared to the actual sequences. This position (2,2), in the matrices, represents letters A and A for the sequences, respectively. But position 2 in either sequence, so x[2] and y[2], would actually index the second letter in either (C and T). Consequently, we can iterate with respect to the matrices’ indexes and adjust when we need to index the sequences or vice versa. Here, we’ll iterate with respect to the matrices and adjust the indexes for x and y, such that s[i,] corresponds to x[i-1] and s[,j] corresponds to y[j-1]: i &lt;- 5; j &lt;- 5 rownames(s)[i] == x[i] ## [1] FALSE rownames(s)[i] == x[i-1] ## [1] TRUE rownames(s)[j] == x[j] ## [1] FALSE rownames(s)[j] == x[j-1] ## [1] TRUE Next, we’ll add our scoring criteria, which pertains to the left, upper, and upper-left diagonal scores relative to our current position. Thus, if we are at position i, j, then we care about scores s[i-1,j], s[i,j-1], and s[i-1,j-1], respectively. We also want to add our match bonus, in which we’ll add 1 to the score of the upper-left diagonal, assuming there is an actual match in our sequence at this position (which we’ll get to next). find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 ## more code will go here } } } Now, we’ll deal with awarding the correct score for our current position. Recall the piecewise function is as follows, ....5 if the current nucleotides in x and y are the same, such that x[i-1] == y[i-1], .... set s[i,j] to the maximum score among S[i,j-1], S[i-1,j-1] + 1, and S[i-1,j], .... where &quot;+1&quot; is the bonus for a match; .... if x[i-1] != y[i-1], then set s[i,j] to the maximum score among S[i,j-1] and S[i-1,j] Thus, we’ll add the following to our function, which will create a vector that stores our 2 or 3 scores, depending on whether there is a match: find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 if (x[i-1]==y[j-1]) scores &lt;- c(s_up,s_left,s_diag) else scores &lt;- c(s_up,s_left) score_update &lt;- max(scores) ## more code will go here } } } We also need to know which of the 3 cells contributed to our current position’s score, so we’ll record the index of the max score in our scores vector. We’ll call it backtrack update (which will soon be obvious). find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 if (x[i-1]==y[j-1]) scores &lt;- c(s_up,s_left,s_diag) else scores &lt;- c(s_up,s_left) backtrack_update &lt;- which.max(scores) score_update &lt;- max(scores) ## more code will go here } } } Our backtrack_update variable records the position in the scores vector that had the max value, whereas the score_update variable contains the actual max score. We can use the position of the max value as a way of indexing a dictionary that can contain arrows that point to the cell that contributed to the current positions score. We’ll name this dictionary backtrack_key and place it outside of the loop: find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) backtrack_key &lt;- c(&#39;|&#39;,&#39;--&#39;,&#39;\\\\&#39;) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 if (x[i-1]==y[j-1]) scores &lt;- c(s_up,s_left,s_diag) else scores &lt;- c(s_up,s_left) backtrack_update &lt;- which.max(scores) score_update &lt;- max(scores) ## more code will go here } } } Finally, we can update our s and b matrices: find_lcs &lt;- function(x,y){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) backtrack_key &lt;- c(&#39;|&#39;,&#39;--&#39;,&#39;\\\\&#39;) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 if (x[i-1]==y[j-1]) scores &lt;- c(s_up,s_left,s_diag) else scores &lt;- c(s_up,s_left) backtrack_update &lt;- which.max(scores) score_update &lt;- max(scores) s[i,j] &lt;- score_update b[i,j] &lt;- backtrack_key[backtrack_update] } } } That’s the function. It will calculate both the path history and the scores for all nucleotides. Now the question is “what about the backtracking”? We’ll deal with that for global alignment, but for now, we’ll use the following recursive function to handle it. We also have to update an option to allow for very deep recursions (if that makes no sense, don’t worry about it). We’ll finish the function up with a list that contains all of the information we’d like returned. backtrack_lcs &lt;- function(b,x,m,n){ if (m==0 | n==0) return(NULL) if (b[m+1,n+1] == &#39;\\\\&#39;){ return(c(x[m],backtrack_lcs(b,x,m-1,n-1))) }else if(b[m+1,n+1] == &#39;|&#39;){ backtrack_lcs(b,x,m-1,n) }else{ backtrack_lcs(b,x,m,n-1) } } find_lcs &lt;- function(x,y){ options(expressions=10000) x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) backtrack_key &lt;- c(&#39;|&#39;,&#39;--&#39;,&#39;\\\\&#39;) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 if (x[i-1]==y[j-1]) scores &lt;- c(s_up,s_left,s_diag) else scores &lt;- c(s_up,s_left) backtrack_update &lt;- which.max(scores) score_update &lt;- max(scores) s[i,j] &lt;- score_update b[i,j] &lt;- backtrack_key[backtrack_update] } } lcs &lt;- backtrack_lcs(b,x,m,n) return(list(lcs=paste0(rev(lcs),collapse=&#39;&#39;), length=s[length(s)], score=s, backtrack=b)) } We can now test it out: find_lcs(&#39;AGCAGACACGTGAT&#39;,&#39;ATCACCGGTAT&#39;) ## $lcs ## [1] &quot;ACACCGTAT&quot; ## ## $length ## [1] 9 ## ## $score ## A T C A C C G G T A T ## 0 0 0 0 0 0 0 0 0 0 0 0 ## A 0 1 1 1 1 1 1 1 1 1 1 1 ## G 0 1 1 1 1 1 1 2 2 2 2 2 ## C 0 1 1 2 2 2 2 2 2 2 2 2 ## A 0 1 1 2 3 3 3 3 3 3 3 3 ## G 0 1 1 2 3 3 3 4 4 4 4 4 ## A 0 1 1 2 3 3 3 4 4 4 5 5 ## C 0 1 1 2 3 4 4 4 4 4 5 5 ## A 0 1 1 2 3 4 4 4 4 4 5 5 ## C 0 1 1 2 3 4 5 5 5 5 5 5 ## G 0 1 1 2 3 4 5 6 6 6 6 6 ## T 0 1 2 2 3 4 5 6 6 7 7 7 ## G 0 1 2 2 3 4 5 6 7 7 7 7 ## A 0 1 2 2 3 4 5 6 7 7 8 8 ## T 0 1 2 2 3 4 5 6 7 8 8 9 ## ## $backtrack ## A T C A C C G G T A T ## &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## A &quot;&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; ## G &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; ## C &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; ## A &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; ## G &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; ## A &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; ## C &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; ## A &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; ## C &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;|&quot; &quot;|&quot; ## G &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; &quot;--&quot; ## T &quot;&quot; &quot;|&quot; &quot;\\\\&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; &quot;--&quot; ## G &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; ## A &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;--&quot; ## T &quot;&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;|&quot; &quot;\\\\&quot; &quot;|&quot; &quot;\\\\&quot; And now with much longer sequences: xy &lt;- readr::read_lines(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/6617b2ceae8765ff7b91f4a600fc5460b335279d/lcs_sequences&#39;) find_lcs(xy[1],xy[2])$lcs ## [1] &quot;CTCTAAGCCAATGGCTCAGGGTGGGTGTAGCGATCCCGCGACAGTAAGCGTCTTGGTAGTTTCATCGCAGCTTCGACCTCGGGTTATCCCGCACCACCCCTTAGACTATTAAGAGCGAATACCCAATAAGTCTTTGCTCACATCCCCAGGGTTAAAACCGCGTGAGAGCGTCTCGACACGTTCTCGTTTATAGGGCGTAATTCATTCTGAGTGTGCGCCAGACTCAATACATCGTAGTCTTCTGCAAACGACTATGAGGGATGGACGCTCGGTACTACTGATTAGGTTGACAGTGATACAAACCAGGATGTATATATCGGTAAAGCCAGTTTACCGACTGCATCGCCGAGTGAAGTTCCACTCATGAAGTAAAATCTTAGTATATGTTATGCCCCCCCTTTTTTATCGAAAGTAGACTAGGTCACTTGTTGTACCGAGCCGCGCTGACATATTAACGCACCACCGACCGTTTTAGTCCTGATTGTTGGGGCATATCAGTGAATTGCGTGAGAAGTGGTCAGGTGGAGCCAGCCGAGAATCGGCTCTAGCCTCAAAACCTACTTCCACGTTCTGCCCCACTGCGAGAGAGAACTCCGGAGTCCTTCGTAGGACCAGGGC&quot; 3.2 Global Alignment Now we can extend the algorithm above for a global alignment problem. Unlike the LCS problem where we only accounted for insertions and deletions, we’re now going to penalize for mismatches. The score we give based on a particular nucleotide-nucleotide or amino acid-amino acid pair will be defined by a scoring matrix such as the amino acid scoring matrix BLOSSUM62: ## A C D E F G H I K L M N P Q R S T V W Y ## A 4 0 -2 -1 -2 0 -2 -1 -1 -1 -1 -2 -1 -1 -1 1 0 0 -3 -2 ## C 0 9 -3 -4 -2 -3 -3 -1 -3 -1 -1 -3 -3 -3 -3 -1 -1 -1 -2 -2 ## D -2 -3 6 2 -3 -1 -1 -3 -1 -4 -3 1 -1 0 -2 0 -1 -3 -4 -3 ## E -1 -4 2 5 -3 -2 0 -3 1 -3 -2 0 -1 2 0 0 -1 -2 -3 -2 ## F -2 -2 -3 -3 6 -3 -1 0 -3 0 0 -3 -4 -3 -3 -2 -2 -1 1 3 ## G 0 -3 -1 -2 -3 6 -2 -4 -2 -4 -3 0 -2 -2 -2 0 -2 -3 -2 -3 ## H -2 -3 -1 0 -1 -2 8 -3 -1 -3 -2 1 -2 0 0 -1 -2 -3 -2 2 ## I -1 -1 -3 -3 0 -4 -3 4 -3 2 1 -3 -3 -3 -3 -2 -1 3 -3 -1 ## K -1 -3 -1 1 -3 -2 -1 -3 5 -2 -1 0 -1 1 2 0 -1 -2 -3 -2 ## L -1 -1 -4 -3 0 -4 -3 2 -2 4 2 -3 -3 -2 -2 -2 -1 1 -2 -1 ## M -1 -1 -3 -2 0 -3 -2 1 -1 2 5 -2 -2 0 -1 -1 -1 1 -1 -1 ## N -2 -3 1 0 -3 0 1 -3 0 -3 -2 6 -2 0 0 1 0 -3 -4 -2 ## P -1 -3 -1 -1 -4 -2 -2 -3 -1 -3 -2 -2 7 -1 -2 -1 -1 -2 -4 -3 ## Q -1 -3 0 2 -3 -2 0 -3 1 -2 0 0 -1 5 1 0 -1 -2 -2 -1 ## R -1 -3 -2 0 -3 -2 0 -3 2 -2 -1 0 -2 1 5 -1 -1 -3 -3 -2 ## S 1 -1 0 0 -2 0 -1 -2 0 -2 -1 1 -1 0 -1 4 1 -2 -3 -2 ## T 0 -1 -1 -1 -2 -2 -2 -1 -1 -1 -1 0 -1 -1 -1 1 5 0 -2 -2 ## V 0 -1 -3 -2 -1 -3 -3 3 -2 1 1 -3 -2 -2 -3 -2 0 4 -3 -1 ## W -3 -2 -4 -3 1 -2 -2 -3 -3 -2 -1 -4 -4 -2 -3 -3 -2 -3 11 2 ## Y -2 -2 -3 -2 3 -3 2 -1 -2 -1 -1 -2 -3 -1 -2 -2 -2 -1 2 7 Note that this matrix scores for mismatches and matches, hence we need not develop separate match and mismatch scoring criteria; we can simply use this matrix as a lookup table. Also, unlike before, we’re no longer going to recursively backtrack through our algorithm; instead, we’ll use a while loop to backtrack in a fashion similar to doing the problem on paper. This allows us to take full advantage of the fact we have the longest path recorded in our backtrack matrix and also permits us to shift the position of NTs or AAs in our sequences when necessary by adding ‘-’, allowing us to create our correctly oriented alignment strings. The algorithm is as follows: DEFINE GLOBAL_ALIGNMENT: Given sequences x and y, penalty p, and scoring lookup table score 1 Create empty an matrix S for scores and an empty matrix B for the backtracking path 2 Preinitialize edges of S and B based on penalty 3 For i in 2 to length x ..4 For j in 2 to length y ....5 Obtain the score of the upper S[i,j-1], upper-left diagonal S[i-1,j-1], .... and left S[i-1,j] cells: ...... set s[i,j] to the maximum score among S[i,j-1] + p, S[i-1,j-1] + score[x[i-1],y[j-1]], ...... and S[i-1,j] + p ....6 Record the position of the maximum score (upper, upper-left, or left). ..End loop End loop 7 Perform backtrack The BLOSSUM62 scoring matrix can be found here: blosum62 &lt;- as.matrix(read.table(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/34c4a34ce49d58b595c3fb2dc77b89a5b6a9b0af/blosum62.dat&#39;)) And our backtracking algorithm: DEFINE GLOBAL_ALIGNMENT_BACKTRACK: Given sequences x and y, backtracking matrix b, penalty p, and scoring lookup table score 1 Create empty alignment vectors align_x and align_y and . preinitialze score s to 0, m to len(x), and n to len(y) 2 While m &gt; 0 OR n &gt; 0 ..3 if b[m+1,n+1] == UP .... update align_x .... update align_y .... update score .... m -= 1 ..4 if b[m+1,n+1] == LEFT .... update align_x .... update align_y .... update score .... n -= 1 ..5 else .... update align_x .... update align_y .... update score .... n -= 1 .... m -= 1 ..End loop End loop A sample sequence pair can be found here: xy &lt;- readr::read_lines(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/0488087071c0b27cd7a576913d5502bdf95db6e5/global_sequences&#39;) 3.3 Local Alignment Often the alignment between two subsequences between our sequences x and y is better than their global alignment. Performing a local alignment is not much different than the algorithms above except for an important change to the recurrence (piecewise function) that dictates the score for a given position. In the global alignment recurrence, we only factor in scores from the upper-left, left, and upper cells. Now, for a local alignment, we will also factor in the sink (cell position 1,1). Thus, the pseudocode will change to: DEFINE LOCAL_ALIGNMENT: ... ....5 Obtain the score of the upper S[i,j-1], upper-left diagonal S[i-1,j-1], .... and left S[i-1,j] cells: ...... set s[i,j] to the maximum score among S[i,j-1] + p, S[i-1,j-1] + score[x[i-1],y[j-1]], ...... S[i-1,j] + p, and 0. ... The backtrack algorithm will also change such that if we land on a zero, we set m and n to 0 to end the backtrack. Lastly, be sure to record the position of the max score in the matrix, which should act as a starting point during backtracking. 3.4 Local Alignment: Homework Edit the global alignment code with the information above to create a function that calculates the local alignment between strings x and y. Use a penalty of -5 and a PAM250 scoring matrix: pam250 &lt;- as.matrix(read.table(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/0b783786e3797d1bb55172e750776e26021224b0/PAM250.dat&#39;)) See below for the global alignment code. Fit your algorithm to these sequences: xy &lt;- readr::read_lines(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/6617b2ceae8765ff7b91f4a600fc5460b335279d/local_sequences&#39;) Report the following: The alignment in its own text file (.txt, .dat, etc.) The max score Your code (in a text file – specifically, not a .doc or .docx) It’s worth commenting your code to help indicate what you were thinking. Note that a correct forward pass will result in the correct max score; a correct forward and backward pass is required for the correct alignment. A few hints: You’ll need to update (1) the piecewise function and (2) the score vector. You no longer want to start your backtrack at the bottom right corner (the sink), so update accordingly. The backtrack will require another if statement. The backtrack may require some debugging to ensure that your indexes are correct. Also, the code may take as little as 35 seconds to run (and even faster on Python) for the sequences given above. That cmdfun command wrapping the function will speed up your function; it is not necessary. If you’re approaching 5 minutes, something is likely wrong. The correct score is around 3000. 3.5 Global Alignment Code (R) backtrack_global &lt;- function(x,y,b,score_matrix,penalty){ m &lt;- length(x) n &lt;- length(y) score &lt;- 0 align_x &lt;- NULL align_y &lt;- NULL while (m &gt; 0 | n &gt; 0){ if (b[m+1,n+1] == &#39;|&#39;){ align_x &lt;- c(align_x,x[m]) align_y &lt;- c(align_y,&#39;-&#39;) score &lt;- score + penalty m &lt;- m-1 }else if(b[m+1,n+1] == &#39;--&#39;){ align_x &lt;- c(align_x,&#39;-&#39;) align_y &lt;- c(align_y,y[n]) score &lt;- score + penalty n &lt;- n-1 }else{ align_x &lt;- c(align_x,x[m]) align_y &lt;- c(align_y,y[n]) score &lt;- score + score_matrix[x[m],y[n]] n &lt;- n-1 m &lt;- m-1 } } alignment &lt;- c(paste0(rev(align_x),collapse=&#39;&#39;),paste0(rev(align_y),collapse=&#39;&#39;)) return(list(score=score,alignment=alignment)) } library(compiler) global_alignment &lt;- cmpfun(function(x,y,score_matrix,penalty){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) backtrack_key &lt;- c(&#39;|&#39;,&#39;--&#39;,&#39;\\\\&#39;) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) s[1,] &lt;- cumsum(c(0,rep(penalty,ncol(s)-1))) s[,1] &lt;- cumsum(c(0,rep(penalty,nrow(s)-1))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b[1,] &lt;- &#39;--&#39; b[,1] &lt;- &#39;|&#39; b[1,1] &lt;- &#39;\\\\&#39; for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] + penalty s_left &lt;- s[i,j-1] + penalty s_diag &lt;- s[i-1,j-1] + score_matrix[x[i-1],y[j-1]] scores &lt;- c(s_up,s_left,s_diag) backtrack_update &lt;- which.max(scores) score_matrix_update &lt;- max(scores) s[i,j] &lt;- score_matrix_update b[i,j] &lt;- backtrack_key[backtrack_update] } } return(backtrack_global(x,y,b,score_matrix,penalty)) }) 3.6 Global Alignment Code (Python) import urllib.request def scoring_matrix(filename): scoring = urllib.request.urlopen(filename).readlines() scoring = [i.decode(&quot;utf-8&quot;).strip(&#39;\\n&#39;) for i in scoring[1:]] keys = [i[0] for i in scoring] scoring = [i.split()[1:] for i in scoring] scoring_dict = {} for ii,i in enumerate(keys): scoring_dict[i] = {} for ji,j in enumerate(keys): scoring_dict[i][j] = int(scoring[ii][ji]) return scoring_dict def global_alignment(v,w,penalty,matrix_file): score_matrix = scoring_matrix(matrix_file) s = [[0]*(len(w)+1) for i in range(len(v)+1)] s[0] = list(range(0,penalty*len(s[0]),penalty)) for i in range(1,len(s)): s[i][0] = penalty + s[i-1][0] path = [[&#39;||&#39;] + [&#39;--&#39;]*(len(w)) for i in range(len(v)+1)] path[0][0] = &#39;\\\\&#39; for i in range(1,len(v)+1): for j in range(1,len(w)+1): score = score_matrix[v[i-1]][w[j-1]] s[i][j] = max(s[i-1][j-1] + score, s[i][j-1] + penalty,s[i-1][j] + penalty) if s[i][j] == s[i-1][j] + penalty: path[i][j] = &#39;||&#39; if s[i][j] == s[i][j-1] + penalty: path[i][j] = &quot;--&quot; if s[i][j] == s[i-1][j-1] + score: path[i][j] = &quot;\\\\&quot; score = 0 align1 = &#39;&#39; align2 = &#39;&#39; while i &gt;= 1 or j &gt;= 1: if path[i][j] == &quot;||&quot;: align1 += v[i-1] align2 += &#39;-&#39; score += penalty i -= 1 elif path[i][j] == &quot;--&quot;: align1 += &#39;-&#39; align2 += w[j-1] score += penalty j -= 1 else: align1 += v[i-1] align2 += w[j-1] score += score_matrix[w[j-1]][v[i-1]] i -= 1 j -= 1 align1 = align1[::-1] align2 = align2[::-1] print(&#39;\\n&#39;.join([str(score),align1,align2])) return [score, align1,align2] "],
["alignalg.html", "Chapter 4 Alignment Algorithms 4.1 Longest Common Subsequence 4.2 Global Alignment (R) 4.3 Global Alignment (Python)", " Chapter 4 Alignment Algorithms 4.1 Longest Common Subsequence backtrack_lcs &lt;- function(b,x,m,n){ if (m==0 | n==0) return(NULL) if (b[m+1,n+1] == &#39;\\\\&#39;){ return(c(x[m],backtrack_lcs(b,x,m-1,n-1))) }else if(b[m+1,n+1] == &#39;|&#39;){ backtrack_lcs(b,x,m-1,n) }else{ backtrack_lcs(b,x,m,n-1) } } library(compiler) find_lcs &lt;- cmpfun(function(x,y){ options(expressions=10000) x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) backtrack_key &lt;- c(&#39;|&#39;,&#39;--&#39;,&#39;\\\\&#39;) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] s_left &lt;- s[i,j-1] s_diag &lt;- s[i-1,j-1] + 1 if (x[i-1]==y[j-1]) scores &lt;- c(s_up,s_left,s_diag) else scores &lt;- c(s_up,s_left) backtrack_update &lt;- which.max(scores) score_update &lt;- max(scores) s[i,j] &lt;- score_update b[i,j] &lt;- backtrack_key[backtrack_update] } } lcs &lt;- backtrack_lcs(b,x,m,n) return(list(lcs=paste0(rev(lcs),collapse=&#39;&#39;), length=s[length(s)], score=s, backtrack=b)) }) 4.2 Global Alignment (R) backtrack_global &lt;- function(x,y,b,score_matrix,penalty){ m &lt;- length(x) n &lt;- length(y) score &lt;- 0 align_x &lt;- NULL align_y &lt;- NULL while (m &gt; 0 | n &gt; 0){ if (b[m+1,n+1] == &#39;|&#39;){ align_x &lt;- c(align_x,x[m]) align_y &lt;- c(align_y,&#39;-&#39;) score &lt;- score + penalty m &lt;- m-1 }else if(b[m+1,n+1] == &#39;--&#39;){ align_x &lt;- c(align_x,&#39;-&#39;) align_y &lt;- c(align_y,y[n]) score &lt;- score + penalty n &lt;- n-1 }else{ align_x &lt;- c(align_x,x[m]) align_y &lt;- c(align_y,y[n]) score &lt;- score + score_matrix[x[m],y[n]] n &lt;- n-1 m &lt;- m-1 } } alignment &lt;- c(paste0(rev(align_x),collapse=&#39;&#39;),paste0(rev(align_y),collapse=&#39;&#39;)) return(list(score=score,alignment=alignment)) } library(compiler) global_alignment &lt;- cmpfun(function(x,y,score_matrix,penalty){ x &lt;- unlist(strsplit(x,&#39;&#39;)) y &lt;- unlist(strsplit(y,&#39;&#39;)) m &lt;- length(x) n &lt;- length(y) backtrack_key &lt;- c(&#39;|&#39;,&#39;--&#39;,&#39;\\\\&#39;) s &lt;- matrix(0,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) s[1,] &lt;- cumsum(c(0,rep(penalty,ncol(s)-1))) s[,1] &lt;- cumsum(c(0,rep(penalty,nrow(s)-1))) b &lt;- matrix(&#39;&#39;,length(x)+1,length(y)+1,dimnames=list(c(&#39;&#39;,x),c(&#39;&#39;,y))) b[1,] &lt;- &#39;--&#39; b[,1] &lt;- &#39;|&#39; b[1,1] &lt;- &#39;\\\\&#39; for (i in seq(2,m+1)){ for (j in seq(2,n+1)){ s_up &lt;- s[i-1,j] + penalty s_left &lt;- s[i,j-1] + penalty s_diag &lt;- s[i-1,j-1] + score_matrix[x[i-1],y[j-1]] scores &lt;- c(s_up,s_left,s_diag) backtrack_update &lt;- which.max(scores) score_matrix_update &lt;- max(scores) s[i,j] &lt;- score_matrix_update b[i,j] &lt;- backtrack_key[backtrack_update] } } return(backtrack_global(x,y,b,score_matrix,penalty)) }) blosum62 &lt;- as.matrix(read.table(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/34c4a34ce49d58b595c3fb2dc77b89a5b6a9b0af/blosum62.dat&#39;)) x &lt;- &#39;ISTHISALL&#39; y &lt;- &#39;ALIGNED&#39; penalty &lt;- -5 global_alignment(x,y,blosum62,-5) ## $score ## [1] -15 ## ## $alignment ## [1] &quot;ISTHISALL&quot; &quot;-AL-IGNED&quot; 4.3 Global Alignment (Python) import urllib.request def scoring_matrix(filename): scoring = urllib.request.urlopen(filename).readlines() scoring = [i.decode(&quot;utf-8&quot;).strip(&#39;\\n&#39;) for i in scoring[1:]] keys = [i[0] for i in scoring] scoring = [i.split()[1:] for i in scoring] scoring_dict = {} for ii,i in enumerate(keys): scoring_dict[i] = {} for ji,j in enumerate(keys): scoring_dict[i][j] = int(scoring[ii][ji]) return scoring_dict def global_alignment(v,w,penalty,matrix_file): score_matrix = scoring_matrix(matrix_file) s = [[0]*(len(w)+1) for i in range(len(v)+1)] s[0] = list(range(0,penalty*len(s[0]),penalty)) for i in range(1,len(s)): s[i][0] = penalty + s[i-1][0] path = [[&#39;||&#39;] + [&#39;--&#39;]*(len(w)) for i in range(len(v)+1)] path[0][0] = &#39;\\\\&#39; for i in range(1,len(v)+1): for j in range(1,len(w)+1): score = score_matrix[v[i-1]][w[j-1]] s[i][j] = max(s[i-1][j-1] + score, s[i][j-1] + penalty,s[i-1][j] + penalty) if s[i][j] == s[i-1][j] + penalty: path[i][j] = &#39;||&#39; if s[i][j] == s[i][j-1] + penalty: path[i][j] = &quot;--&quot; if s[i][j] == s[i-1][j-1] + score: path[i][j] = &quot;\\\\&quot; score = 0 align1 = &#39;&#39; align2 = &#39;&#39; while i &gt;= 1 or j &gt;= 1: if path[i][j] == &quot;||&quot;: align1 += v[i-1] align2 += &#39;-&#39; score += penalty i -= 1 elif path[i][j] == &quot;--&quot;: align1 += &#39;-&#39; align2 += w[j-1] score += penalty j -= 1 else: align1 += v[i-1] align2 += w[j-1] score += score_matrix[w[j-1]][v[i-1]] i -= 1 j -= 1 align1 = align1[::-1] align2 = align2[::-1] print(&#39;\\n&#39;.join([str(score),align1,align2])) return [score, align1,align2] blosum62 = &#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/34c4a34ce49d58b595c3fb2dc77b89a5b6a9b0af/blosum62.dat&#39; x = &#39;ISTHISALL&#39; y = &#39;ALIGNED&#39; penalty = -5 global_alignment(x,y,penalty,blosum62) "],
["biocond.html", "Chapter 5 Bioconductor 5.1 Creating GC Functions 5.2 NCBI ESearch 5.3 CDS 5.4 Whole Genomes", " Chapter 5 Bioconductor library(Biostrings) library(tidyverse) library(reutils) library(XML) library(BSgenome) library(BSgenome.Athaliana.TAIR.04232008) source(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/162b0c542482d481f79b0160071114eb38cb568e/r_bioinformatics_functions.R&#39;) 5.0.1 Loading FASTA Files A FASTA file is a file containing multiple nucleotide or amino acid sequences, each with their own identifier, formatted as a header that starts with ‘&gt;’. A file essentially looks like &gt;Sequence_1 GGCGAT &gt;Sequence_2 AAATCG and so on. The structure of the content of the file is important, not necessarily the file extension. You can have a FASTA file with a .txt extension, no extension, or the common .fna extension. The trick is to know how these files are formatted to identify them. (Note that wikipedia tends to have the best information on bioinformatics file types, quality scoring, etc.) The other file type worth noting is FASTQ, which, in addition to sequence information, also contains a quality score for each position in the sequence that measures how likely that nucleotide or protein is correct. FASTQ files look somewhat different than FASTA: @Sequence_1 GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !&#39;&#39;*((((***+))%%%++)(%%%%).1***-+*&#39;&#39;))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65 @Sequence_2 GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + 9C;=;=&lt;9@4868&gt;9:67AA&lt;9&gt;65&lt;=&gt;59-+*&#39;&#39;))**55CCFMNO&gt;&gt;&gt;&gt;&gt;&gt;FFFFC65 Again, for a given sequence, line 1 has the header, but unlike the FASTA file, FASTQ headers begin with '@'. Line 2 is the actual sequence, followed by ‘+’ on line 3. The quality score is then found on line 4 and will be the same length as the sequence on line 1. Let’s load a FASTA file. We’ll use a simple Bioconductor function. fasta &lt;- readDNAStringSet(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/f1fb586160d12c34f29532c731066fd8912a0e0c/example.fasta&#39;,format=&#39;fasta&#39;) fasta ## A DNAStringSet instance of length 8 ## width seq names ## [1] 893 TGGTAGAACGTGTGGGCTCGAGA...TCCCTAGTTAGCGAGGTCCATAA Sequence_9715 ## [2] 860 TACTGCTTGTACAAGCCTCATCT...TAATAGACGTTGTACCGGGGGAA Sequence_5667 ## [3] 815 TATGTGTTTCATTTAGGACCTCG...GTTCCATAAAACGGTCAAGCAGT Sequence_2989 ## [4] 912 TGGTGCCGAGGCTCGAGTGTACG...GGTATGGGAAATTCAACAAACAC Sequence_2049 ## [5] 806 CTAGCAATGGCAAATTAGATGTA...ACTTGAACAGAAAATCAACCGGA Sequence_5456 ## [6] 834 AAACGAGAACGTGGAGATTTGCC...CAGTCGGGAGTACTAACTGATTT Sequence_1118 ## [7] 818 CAGAAAGCATGAGTCTCGCCCTG...AGCTTACGCCTATTTTCCCCAGT Sequence_7043 ## [8] 878 CGCCGCACTATCCACGTTAAAAG...TCTAAGTAGTACCTAACAGAACA Sequence_0123 For FASTQ, it’s essentially the same except we change ‘fasta’ to ‘fastq’ for the format argument: fastq &lt;- readDNAStringSet(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/f1fb586160d12c34f29532c731066fd8912a0e0c/example.fastq&#39;,format=&#39;fastq&#39;) fastq ## A DNAStringSet instance of length 250 ## width seq names ## [1] 31 TTTCCGGGGCACATAATCTTCAGCCGGGCGC Sequence_2:UMI_AT... ## [2] 31 TATCCTTGCAATACTCTCCGAACGGGAGAGC Sequence_8:UMI_CT... ## [3] 31 GCAGTTTAAGATCATTTTATTGAAGAGCAAG Sequence_12:UMI_G... ## [4] 31 GGCATTGCAAAATTTATTACACCCCCAGATC Sequence_21:UMI_A... ## [5] 31 CCCCCTTAAATAGCTGTTTATTTGGCCCCAG Sequence_29:UMI_G... ## ... ... ... ## [246] 31 GCTGTAGGAACAGCAGTCTTGGTGGTTAGCA Sequence_819:UMI_... ## [247] 31 CCATTATAATAGCCATCTTTATTTGTAAAAA Sequence_823:UMI_... ## [248] 31 AGCTTTGCAACCATACTCCCCCCGGAACCCA Sequence_824:UMI_... ## [249] 31 GCCCCCCCCCAAATCGGAAAAACACACCCCC Sequence_828:UMI_... ## [250] 31 AGGGTGGGGGATCACATTTATTGTATTGAGG Sequence_834:UMI_... 5.0.2 Creating Sequence Sets Let’s approach a FASTA problem from a different direction now. We’ll create a DNA string set from a bunch of individual sequences, then write the set to a FASTA file. Run the following command to add three new variables to your environment: s1, s2, and s3. Each represents a different DNA sequence or ‘read.’ problem_createsequencesets() ## Added s1, s2, and s3 to environment. We’re going to create a DNAStringSet object, which can then be saved as a FASTA file. First, we have to combine the sequences into a vector and then pass this vector into DNAStringSet(). S &lt;- c(s1,s2,s3) SS &lt;- DNAStringSet(S) Recall that FASTA files have header names. Let’s create header names for these three sequences. We can manually do it like so names(SS) &lt;- c(&#39;sequence_1&#39;,&#39;sequence_2&#39;,&#39;sequence_3&#39;) but this will be far from ideal if we had, say, 100,000 sequences. Instead, we’re going to use a function called paste(), which basically pastes together vectors of text, element-wise: DOG &lt;- c(&#39;dog1&#39;,&#39;dog2&#39;,&#39;dog3&#39;) CAT &lt;- c(&#39;cat1&#39;,&#39;cat2&#39;,&#39;cat3&#39;) paste(DOG,CAT) ## [1] &quot;dog1 cat1&quot; &quot;dog2 cat2&quot; &quot;dog3 cat3&quot; paste(DOG,CAT,sep=&#39;-&#39;) ## [1] &quot;dog1-cat1&quot; &quot;dog2-cat2&quot; &quot;dog3-cat3&quot; paste(DOG,CAT,sep=&#39;_&#39;) ## [1] &quot;dog1_cat1&quot; &quot;dog2_cat2&quot; &quot;dog3_cat3&quot; paste(DOG,CAT,sep=&#39;&#39;) ## [1] &quot;dog1cat1&quot; &quot;dog2cat2&quot; &quot;dog3cat3&quot; paste(&#39;dog&#39;,&#39;cat&#39;,1:3,sep=&#39;&#39;) ## [1] &quot;dogcat1&quot; &quot;dogcat2&quot; &quot;dogcat3&quot; paste(&#39;dog&#39;,&#39;cat&#39;,1:3,sep=&#39;_&#39;) ## [1] &quot;dog_cat_1&quot; &quot;dog_cat_2&quot; &quot;dog_cat_3&quot; paste(&#39;dog_&#39;,&#39;cat&#39;,1:3,sep=&#39;&#39;) ## [1] &quot;dog_cat1&quot; &quot;dog_cat2&quot; &quot;dog_cat3&quot; This is how we’ll create our header names. We can grab the number of total sequences in our set using length(), which will let us create a vector to number our sequences. We’ll create a header name that includes each sequence number, the word sequence, along with a user name. We’ll also pass in the date using the date() function. seq_names &lt;- paste(&#39;sequence_&#39;,1:length(SS),&#39; | User_12 | &#39;,date(), sep=&#39;&#39;) seq_names ## [1] &quot;sequence_1 | User_12 | Mon Feb 24 14:24:24 2020&quot; ## [2] &quot;sequence_2 | User_12 | Mon Feb 24 14:24:24 2020&quot; ## [3] &quot;sequence_3 | User_12 | Mon Feb 24 14:24:24 2020&quot; Now, we’ll rename the sequences in the set with these names: names(SS) &lt;- seq_names Finally, we can save our sequence set as a FASTA file: output_name &lt;- &#39;seq_set_out.fasta&#39; writeXStringSet(SS,file=output_name,format=&quot;fasta&quot;) 5.0.3 Sample Metadata Often, the sequences we’re working with have corresponding metadata. These metadata can range from information about the specific sequence (e.g., the type of sequencer used) to information about the organism from which the sequence was acquired (e.g., species, treatment, age). The way in which we can link our sequence reads in the FASTA file to the metadata is via the header name. Load the following sequence set: FASTA &lt;- readDNAStringSet(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/10bc2f50d1c739827ea2ba4edb146b36a6a4c14a/problems_metadata.fasta&#39;,format=&#39;fasta&#39;) To place the metadata, named META, into your environement, run the following: problem_metadata(FASTA) ## Added META to environment. We’ll henceforth refer to the rows as ‘samples.’ If we want to know which samples were sequenced at the Philadelphia sequencing center, we can type which(META$Center == &#39;Philadelphia&#39;) ## [1] 86 88 89 91 92 99 100 If we wanted to find the sequence with the header name ‘Rosalind_6333’, we can do FASTA[&#39;Sequence_6333&#39;] ## A DNAStringSet instance of length 1 ## width seq names ## [1] 1000 TTCGCAGTATCCAGGTACAGGGG...TCGGACGATGACAGTGGACATGT Sequence_6333 And if we wanted to get the sequences corresponding to rows 12, 15, and 78 in the metadata file: header_names &lt;- META$ID[c(12,15,78)] FASTA[header_names] ## A DNAStringSet instance of length 3 ## width seq names ## [1] 1000 TTGCAGGGTGGGCATGGTGGTAG...GTCAGCGATTAACATGTTGGCTA Sequence_1385 ## [2] 1000 CACTGAGGCGAATGAATATAAAA...GCCGAATAGCTACAACAGACACT Sequence_7797 ## [3] 1000 ATTGGTTTTGAAGCGACAGCGTT...TTCAGATCCGGTCCATAGAAATT Sequence_8314 5.1 Creating GC Functions The goal here will be (a) to demonstrate how to write a function and (b) better understand some useful GC quantification techniques. We’ll create a function that can calcualte the GC content in a given sequence. We’ll also give this function an additional parameter that allows it to calculate the GC content at a specific codon position. gc_calc &lt;- function(x) (x[&#39;g&#39;]+x[&#39;c&#39;])/sum(x) gc &lt;- function(s,pos){ s &lt;- stringr::str_to_lower(s) s &lt;- unlist(strsplit(s,&#39;&#39;)) if (!missing(pos)) s &lt;- s[seq(pos,length(s),3)] counts &lt;- table(s) gc_calc(counts) } Now, we’ll create a function to calculate the GC skew. This function will calcuate the skew for the entire sequence or successive windows in the sequence of some given size. gc_skew_calc &lt;- function(x) {counts &lt;- table(x); (counts[&#39;g&#39;]-counts[&#39;c&#39;])/(counts[&#39;g&#39;]+counts[&#39;c&#39;])} gc_skew &lt;- function(s,win){ s &lt;- stringr::str_to_lower(s) s &lt;- unlist(strsplit(s,&#39;&#39;)) if (missing(win)) { gc &lt;- gc_skew_calc(s) }else{ start &lt;- seq(1,length(s),win) gc &lt;- NULL for (i in start){ gc &lt;- c(gc, gc_skew_calc(s[(i):(i+win-1)])) } } gc } First, we can look at the GC content in some random sequences: generate_random_dna_gc_s(len=1000,seed=5) ## Added s to environment (seed=5). gc(s) ## g ## 0.559 gc(s,1) ## g ## 0.5628742515 gc(s,2) ## g ## 0.5405405405 gc(s,3) ## g ## 0.5735735736 And then we can check the skew: generate_random_dna_skew_s(len=1000,w=1,seed=5) ## Added s to environment (seed=5). gc_skew(s) ## g ## 0.2573402418 gc_skew(s,100) ## g g g g g ## 0.04545454545 -0.01818181818 0.26315789474 0.37704918033 0.10714285714 ## g g g g g ## 0.35593220339 0.14754098361 0.36666666667 0.26315789474 0.53623188406 plot_skew(gc_skew(s,25)) 5.2 NCBI ESearch This will look very familiar to the Python tutorial from earlier, but with more of an “R flavor.” Let’s look for 3 cds entries. ids1 &lt;- esearch(&quot;CFTR AND human[Organism] AND complete&quot;,db=&#39;nucleotide&#39;,retmax=15,sort=&#39;relevance&#39;) ids2 &lt;- esearch(&quot;PKD1 AND human[Organism] AND complete&quot;,db=&#39;nucleotide&#39;,retmax=15,sort=&#39;relevance&#39;) ids3 &lt;- esearch(&quot;DMPK AND human[Organism] AND complete&quot;,db=&#39;nucleotide&#39;,retmax=15,sort=&#39;relevance&#39;) We can parse a particular entry into a dataframe: ids_df &lt;- reutils::content(esummary(ids1),&#39;parsed&#39;) ## Warning: HTTP error: Status 429; Too Many Requests ## Warning: Errors parsing DocumentSummary We can also look at the text entries for each gene: efetch(ids1[1], rettype = &quot;fasta&quot;, retmode = &quot;text&quot;) ## Warning: HTTP error: Status 429; Too Many Requests ## Object of class &#39;efetch&#39; ## [1] &quot;HTTP error: Status 429; Too Many Requests&quot; ## EFetch query using the &#39;nucleotide&#39; database. ## Query url: &#39;https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?=efe...&#39; ## Retrieval type: &#39;fasta&#39;, retrieval mode: &#39;text&#39; efetch(ids2[4], rettype = &quot;fasta&quot;, retmode = &quot;text&quot;) ## Warning: HTTP error: Status 429; Too Many Requests ## Object of class &#39;efetch&#39; ## [1] &quot;HTTP error: Status 429; Too Many Requests&quot; ## EFetch query using the &#39;nucleotide&#39; database. ## Query url: &#39;https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?=efe...&#39; ## Retrieval type: &#39;fasta&#39;, retrieval mode: &#39;text&#39; efetch(ids3[5], rettype = &quot;fasta&quot;, retmode = &quot;text&quot;) ## Warning: HTTP error: Status 429; Too Many Requests ## Object of class &#39;efetch&#39; ## [1] &quot;HTTP error: Status 429; Too Many Requests&quot; ## EFetch query using the &#39;nucleotide&#39; database. ## Query url: &#39;https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?=efe...&#39; ## Retrieval type: &#39;fasta&#39;, retrieval mode: &#39;text&#39; These look good, so let’s combine the UIDs into a vector: ids &lt;- c(ids1[1],ids2[4],ids3[5]) Now, we can extract important information, such as the sequence, by switching to XML mode: FASTA &lt;- efetch(ids,db=&#39;nucleotide&#39;, rettype = &quot;fasta&quot;, retmode = &quot;xml&quot;) ## Warning: HTTP error: Status 429; Too Many Requests SEQS &lt;- FASTA$xmlValue(&#39;//TSeq_sequence&#39;) But there is actually a much better way, consistent with the FASTA tutorial above: tmp &lt;- tempfile() FASTA &lt;- efetch(ids,db=&#39;nucleotide&#39;, rettype = &quot;fasta&quot;, retmode = &quot;text&quot;, outfile=tmp) FASTA &lt;- readDNAStringSet(tmp) Now, let’s calculate the GC content, which is easy using a bioconductor functoin (we’ll skip over our functoin from before): letterFrequency(FASTA,&#39;GC&#39;,as.prob=TRUE) ## G|C ## [1,] 0.3225755543 ## [2,] 0.6391795013 ## [3,] 0.5959700336 If we want the GC skew, we can use our function from before. That will give us the same GC skew result that we got from BioPython: skew &lt;- gc_skew(FASTA[[2]],500) plot_skew(skew) But, we can use a function in Bioconductor. The difference betweent his function and our implementation (and hence BioPython’s) is the way the window is defined. BioPython’s were not overlapping; here they are. skew &lt;- lapply(seq_along(FASTA), function(i,w) { numer &lt;- letterFrequencyInSlidingView(FASTA[[i]],&#39;G&#39;,view.width=w) - letterFrequencyInSlidingView(FASTA[[i]],&#39;C&#39;,view.width=w) denom &lt;- letterFrequencyInSlidingView(FASTA[[i]],&#39;GC&#39;,view.width=w) numer/denom },w=500) plot_skew(skew[[2]]) 5.3 CDS ID &lt;- esearch(&quot;Galdieria sulphuraria[Organism] AND whole genome&quot;,db=&#39;nucleotide&#39;,retmax=5,sort=&#39;relevance&#39;) rec &lt;- efetch(ID[1],db=&#39;nucleotide&#39;, rettype = &quot;gb&quot;, retmode = &quot;xml&quot;) prec &lt;- reutils::content(rec,as=&#39;text&#39;) prec &lt;- xmlParse(prec) prec &lt;- xmlToList(prec) features &lt;- prec$GBSeq$`GBSeq_feature-table` cds_idx &lt;- which(sapply(features,function(x) x[[1]]) == &#39;CDS&#39;) features &lt;- features[cds_idx] features &lt;- lapply(features,cleanup_feat_table) na.omit(sapply(features,function(x) ifelse(grepl(&#39;ATPase&#39;,x[&#39;product&#39;]),x[&#39;protein_id&#39;],NA))) ## named list() 5.4 Whole Genomes A nice thing about Bioconductor is how easy it is to access genomic information. Bioconductor has a pacakge called ‘BSgenome’ that contains complete genomes of a ton of organisms. Simply look: available.genomes() ## [1] &quot;BSgenome.Alyrata.JGI.v1&quot; ## [2] &quot;BSgenome.Amellifera.BeeBase.assembly4&quot; ## [3] &quot;BSgenome.Amellifera.UCSC.apiMel2&quot; ## [4] &quot;BSgenome.Amellifera.UCSC.apiMel2.masked&quot; ## [5] &quot;BSgenome.Aofficinalis.NCBI.V1&quot; ## [6] &quot;BSgenome.Athaliana.TAIR.04232008&quot; ## [7] &quot;BSgenome.Athaliana.TAIR.TAIR9&quot; ## [8] &quot;BSgenome.Btaurus.UCSC.bosTau3&quot; ## [9] &quot;BSgenome.Btaurus.UCSC.bosTau3.masked&quot; ## [10] &quot;BSgenome.Btaurus.UCSC.bosTau4&quot; ## [11] &quot;BSgenome.Btaurus.UCSC.bosTau4.masked&quot; ## [12] &quot;BSgenome.Btaurus.UCSC.bosTau6&quot; ## [13] &quot;BSgenome.Btaurus.UCSC.bosTau6.masked&quot; ## [14] &quot;BSgenome.Btaurus.UCSC.bosTau8&quot; ## [15] &quot;BSgenome.Btaurus.UCSC.bosTau9&quot; ## [16] &quot;BSgenome.Carietinum.NCBI.v1&quot; ## [17] &quot;BSgenome.Celegans.UCSC.ce10&quot; ## [18] &quot;BSgenome.Celegans.UCSC.ce11&quot; ## [19] &quot;BSgenome.Celegans.UCSC.ce2&quot; ## [20] &quot;BSgenome.Celegans.UCSC.ce6&quot; ## [21] &quot;BSgenome.Cfamiliaris.UCSC.canFam2&quot; ## [22] &quot;BSgenome.Cfamiliaris.UCSC.canFam2.masked&quot; ## [23] &quot;BSgenome.Cfamiliaris.UCSC.canFam3&quot; ## [24] &quot;BSgenome.Cfamiliaris.UCSC.canFam3.masked&quot; ## [25] &quot;BSgenome.Cjacchus.UCSC.calJac3&quot; ## [26] &quot;BSgenome.Dmelanogaster.UCSC.dm2&quot; ## [27] &quot;BSgenome.Dmelanogaster.UCSC.dm2.masked&quot; ## [28] &quot;BSgenome.Dmelanogaster.UCSC.dm3&quot; ## [29] &quot;BSgenome.Dmelanogaster.UCSC.dm3.masked&quot; ## [30] &quot;BSgenome.Dmelanogaster.UCSC.dm6&quot; ## [31] &quot;BSgenome.Drerio.UCSC.danRer10&quot; ## [32] &quot;BSgenome.Drerio.UCSC.danRer11&quot; ## [33] &quot;BSgenome.Drerio.UCSC.danRer5&quot; ## [34] &quot;BSgenome.Drerio.UCSC.danRer5.masked&quot; ## [35] &quot;BSgenome.Drerio.UCSC.danRer6&quot; ## [36] &quot;BSgenome.Drerio.UCSC.danRer6.masked&quot; ## [37] &quot;BSgenome.Drerio.UCSC.danRer7&quot; ## [38] &quot;BSgenome.Drerio.UCSC.danRer7.masked&quot; ## [39] &quot;BSgenome.Ecoli.NCBI.20080805&quot; ## [40] &quot;BSgenome.Gaculeatus.UCSC.gasAcu1&quot; ## [41] &quot;BSgenome.Gaculeatus.UCSC.gasAcu1.masked&quot; ## [42] &quot;BSgenome.Ggallus.UCSC.galGal3&quot; ## [43] &quot;BSgenome.Ggallus.UCSC.galGal3.masked&quot; ## [44] &quot;BSgenome.Ggallus.UCSC.galGal4&quot; ## [45] &quot;BSgenome.Ggallus.UCSC.galGal4.masked&quot; ## [46] &quot;BSgenome.Ggallus.UCSC.galGal5&quot; ## [47] &quot;BSgenome.Ggallus.UCSC.galGal6&quot; ## [48] &quot;BSgenome.Hsapiens.1000genomes.hs37d5&quot; ## [49] &quot;BSgenome.Hsapiens.NCBI.GRCh38&quot; ## [50] &quot;BSgenome.Hsapiens.UCSC.hg17&quot; ## [51] &quot;BSgenome.Hsapiens.UCSC.hg17.masked&quot; ## [52] &quot;BSgenome.Hsapiens.UCSC.hg18&quot; ## [53] &quot;BSgenome.Hsapiens.UCSC.hg18.masked&quot; ## [54] &quot;BSgenome.Hsapiens.UCSC.hg19&quot; ## [55] &quot;BSgenome.Hsapiens.UCSC.hg19.masked&quot; ## [56] &quot;BSgenome.Hsapiens.UCSC.hg38&quot; ## [57] &quot;BSgenome.Hsapiens.UCSC.hg38.masked&quot; ## [58] &quot;BSgenome.Mdomestica.UCSC.monDom5&quot; ## [59] &quot;BSgenome.Mfascicularis.NCBI.5.0&quot; ## [60] &quot;BSgenome.Mfuro.UCSC.musFur1&quot; ## [61] &quot;BSgenome.Mmulatta.UCSC.rheMac10&quot; ## [62] &quot;BSgenome.Mmulatta.UCSC.rheMac2&quot; ## [63] &quot;BSgenome.Mmulatta.UCSC.rheMac2.masked&quot; ## [64] &quot;BSgenome.Mmulatta.UCSC.rheMac3&quot; ## [65] &quot;BSgenome.Mmulatta.UCSC.rheMac3.masked&quot; ## [66] &quot;BSgenome.Mmulatta.UCSC.rheMac8&quot; ## [67] &quot;BSgenome.Mmusculus.UCSC.mm10&quot; ## [68] &quot;BSgenome.Mmusculus.UCSC.mm10.masked&quot; ## [69] &quot;BSgenome.Mmusculus.UCSC.mm8&quot; ## [70] &quot;BSgenome.Mmusculus.UCSC.mm8.masked&quot; ## [71] &quot;BSgenome.Mmusculus.UCSC.mm9&quot; ## [72] &quot;BSgenome.Mmusculus.UCSC.mm9.masked&quot; ## [73] &quot;BSgenome.Osativa.MSU.MSU7&quot; ## [74] &quot;BSgenome.Ptroglodytes.UCSC.panTro2&quot; ## [75] &quot;BSgenome.Ptroglodytes.UCSC.panTro2.masked&quot; ## [76] &quot;BSgenome.Ptroglodytes.UCSC.panTro3&quot; ## [77] &quot;BSgenome.Ptroglodytes.UCSC.panTro3.masked&quot; ## [78] &quot;BSgenome.Ptroglodytes.UCSC.panTro5&quot; ## [79] &quot;BSgenome.Ptroglodytes.UCSC.panTro6&quot; ## [80] &quot;BSgenome.Rnorvegicus.UCSC.rn4&quot; ## [81] &quot;BSgenome.Rnorvegicus.UCSC.rn4.masked&quot; ## [82] &quot;BSgenome.Rnorvegicus.UCSC.rn5&quot; ## [83] &quot;BSgenome.Rnorvegicus.UCSC.rn5.masked&quot; ## [84] &quot;BSgenome.Rnorvegicus.UCSC.rn6&quot; ## [85] &quot;BSgenome.Scerevisiae.UCSC.sacCer1&quot; ## [86] &quot;BSgenome.Scerevisiae.UCSC.sacCer2&quot; ## [87] &quot;BSgenome.Scerevisiae.UCSC.sacCer3&quot; ## [88] &quot;BSgenome.Sscrofa.UCSC.susScr11&quot; ## [89] &quot;BSgenome.Sscrofa.UCSC.susScr3&quot; ## [90] &quot;BSgenome.Sscrofa.UCSC.susScr3.masked&quot; ## [91] &quot;BSgenome.Tgondii.ToxoDB.7.0&quot; ## [92] &quot;BSgenome.Tguttata.UCSC.taeGut1&quot; ## [93] &quot;BSgenome.Tguttata.UCSC.taeGut1.masked&quot; ## [94] &quot;BSgenome.Tguttata.UCSC.taeGut2&quot; ## [95] &quot;BSgenome.Vvinifera.URGI.IGGP12Xv0&quot; ## [96] &quot;BSgenome.Vvinifera.URGI.IGGP12Xv2&quot; ## [97] &quot;BSgenome.Vvinifera.URGI.IGGP8X&quot; We can install a specific genomes and parse them quite easily: load_library(BSgenome.Athaliana.TAIR.04232008) For example, to calculate the GC content of each chromosome in either genome, we can do the following: params &lt;- new(&#39;BSParams&#39;, X=Athaliana, FUN = function(x) letterFrequency(x,&#39;GC&#39;,as.prob=TRUE), exclude=c(&#39;M&#39;,&#39;C&#39;)) unlist(bsapply(params)) ## chr1.G|C chr2.G|C chr3.G|C chr4.G|C chr5.G|C ## 0.3567394241 0.3584669531 0.3630477097 0.3619816947 0.3590229191 "],
["sra.html", "Chapter 6 Retrieving Projects", " Chapter 6 Retrieving Projects Many of your projects will involve working with publically available data from published research. The data itself is typicaly uploaded to a data repository like NCBI or EMBL/EBI. The files are deposited as Sequence Read Archive (SRA) files and are often searchable as bioprojects. Accessing these files can be accomplished using Bioconductor in R. library(stringr) library(SRAdb) library(Biostrings) First, download the database file to your working director via getSRAdbFile(destdir=getwd(),destfile=&#39;SRAmetadb.sqlite.gz&#39;,method) Then, we’ll make a connection with the database: sqlfile &lt;- &#39;~/SRAmetadb.sqlite&#39; sra_con &lt;- dbConnect(SQLite(),sqlfile) Let’s assume we want to download the data from the Gevers IBD study that was deposited with the following accession: PRJNA237362. We can see this project on NCBI here: https://www.ncbi.nlm.nih.gov/bioproject/PRJNA237362 . If we click the link next to ‘SRA Experiments’ and then click one of the sample links on the subsequent page, we’ll end up on a page that looks like this: https://www.ncbi.nlm.nih.gov/sra/SRX1418176[accn] . Here’ we can access the SRA project code: SRP040765. This is what we need. Before we continue, let me go over the SRA file types: SRA - Accession information that contains the 5 files below SRP - Project information and metadata SRS - Sample metadata SRX - Experiment metadata including library, platform selection, and processing parametes involved in a particular sequencing experiment SRR - Sequencing run information SRX - Sequence analysis BAM file information Now that we have the SRP, let’s acquire the files we need, specifically the SRA files: rs &lt;- listSRAfile(c(&#39;SRP040765&#39;), sra_con, fileType = &#39;sra&#39;) str(rs) ## &#39;data.frame&#39;: 3408 obs. of 5 variables: ## $ study : chr &quot;SRP040765&quot; &quot;SRP040765&quot; &quot;SRP040765&quot; &quot;SRP040765&quot; ... ## $ sample : chr &quot;SRS587325&quot; &quot;SRS587957&quot; &quot;SRS587956&quot; &quot;SRS695027&quot; ... ## $ experiment: chr &quot;SRX691644&quot; &quot;SRX692390&quot; &quot;SRX509287&quot; &quot;SRX693364&quot; ... ## $ run : chr &quot;SRR1564534&quot; &quot;SRR1565230&quot; &quot;SRR1215330&quot; &quot;SRR1566410&quot; ... ## $ ftp : chr &quot;ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR156/SRR1564534/SRR1564534.sra&quot; &quot;ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR156/SRR1565230/SRR1565230.sra&quot; &quot;ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR121/SRR1215330/SRR1215330.sra&quot; &quot;ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR156/SRR1566410/SRR1566410.sra&quot; ... rs is a dataframe containing the SRP, SRS, SRX, and SRR IDs for a given sequencing run, as well as the ftp link to the actual .sra file containing the sequencing information. These are the links we can use to download the entire set of data we need to perform some analysis. Now, we could export these links and then just iterate through, maybe using bash with wget, to download all of these files. Alternatively, we can do the following: We’ll get a run ID for a run we’ld like to download: run &lt;- rs$run[1] run ## [1] &quot;SRR1564534&quot; If we want the specific SRR (run) information, we do: run_info &lt;- getSRA(search_terms=&#39;SRP040765&#39;, out_types=c(&#39;run&#39;),sra_con) str(run_info) ## &#39;data.frame&#39;: 3408 obs. of 11 variables: ## $ run_alias : chr &quot;A21YN121012.1.Illumina_P7-Fexexoja.screened.bam&quot; &quot;A21YN121012.1.Illumina_P7-Wodejora.screened.bam&quot; &quot;A2WP7130403.1.Illumina_P7-Xakokoxe.screened.bam&quot; &quot;A1UEN121219.1.Illumina_P7-Birarane.screened.bam&quot; ... ## $ run : chr &quot;SRR1564534&quot; &quot;SRR1565230&quot; &quot;SRR1215330&quot; &quot;SRR1566410&quot; ... ## $ run_date : chr &quot;2012-10-12&quot; &quot;2012-10-12&quot; &quot;2013-04-03&quot; &quot;2012-12-19&quot; ... ## $ updated_date : chr &quot;2014-09-04&quot; &quot;2014-09-05&quot; &quot;2019-12-11&quot; &quot;2014-09-06&quot; ... ## $ spots : num 120 449 2102 9532 676152 ... ## $ bases : num 4.20e+04 1.57e+05 7.36e+05 3.34e+06 1.37e+08 ... ## $ run_center : chr &quot;BI&quot; &quot;BI&quot; &quot;BI&quot; &quot;BI&quot; ... ## $ experiment_name: logi NA NA NA NA NA NA ... ## $ run_url_link : logi NA NA NA NA NA NA ... ## $ run_entrez_link: logi NA NA NA NA NA NA ... ## $ run_attribute : chr &quot;analysis_type: AssemblyWithoutReference || flowcell_barcode: A21YN || gssr_id: 247611.0 || instrument_name: SL-&quot;| __truncated__ &quot;analysis_type: AssemblyWithoutReference || flowcell_barcode: A21YN || gssr_id: 247629.0 || instrument_name: SL-&quot;| __truncated__ &quot;analysis_type: AssemblyWithoutReference || flowcell_barcode: A2WP7 || gssr_id: 247671.0 || instrument_name: SL-&quot;| __truncated__ &quot;analysis_type: AssemblyWithoutReference || data_type: 16S || flowcell_barcode: A1UEN || gssr_id: 276389.0 || in&quot;| __truncated__ ... and for the SRS (sample) information: sample_info &lt;- getSRA(search_terms=&#39;SRP040765&#39;, out_types=c(&#39;sample&#39;),sra_con) str(sample_info) ## &#39;data.frame&#39;: 1572 obs. of 10 variables: ## $ sample_alias : chr &quot;SKBTI-0107&quot; &quot;SKBTI-0128&quot; &quot;SKBTI-0172&quot; &quot;SKBTI-0594&quot; ... ## $ sample : chr &quot;SRS587325&quot; &quot;SRS587957&quot; &quot;SRS587956&quot; &quot;SRS695027&quot; ... ## $ taxon_id : int 408170 408170 408170 408170 408170 408170 408170 408170 408170 408170 ... ## $ common_name : chr NA NA NA NA ... ## $ anonymized_name : chr NA NA NA NA ... ## $ individual_name : chr NA NA NA NA ... ## $ description : logi NA NA NA NA NA NA ... ## $ sample_url_link : logi NA NA NA NA NA NA ... ## $ sample_entrez_link: logi NA NA NA NA NA NA ... ## $ sample_attribute : chr &quot;strain: SKBTI-0107 || collection date: missing || geographic location (country and/or sea, region): USA || spec&quot;| __truncated__ &quot;strain: SKBTI-0128 || collection date: missing || geographic location (country and/or sea, region): USA || spec&quot;| __truncated__ &quot;strain: SKBTI-0172 || collection date: missing || geographic location (country and/or sea, region): USA || spec&quot;| __truncated__ &quot;strain: SKBTI-0594 || collection date: missing || geographic location (country and/or sea, region): USA || spec&quot;| __truncated__ ... and SRX (experiment) information: experiment_info &lt;- getSRA(search_terms=&#39;SRP040765&#39;, out_types=c(&#39;experiment&#39;),sra_con) str(experiment_info) ## &#39;data.frame&#39;: 2708 obs. of 27 variables: ## $ experiment_alias : chr &quot;2949006.WR32770.Solexa-122962.A21YN121012.P&quot; &quot;2949006.WR32770.Solexa-122980.A21YN121012.P&quot; &quot;2949006.WR32770.Solexa-123022.A2WP7130403.P&quot; &quot;2949006.WR33991.Solexa-133729.A1UEN121219.P&quot; ... ## $ experiment : chr &quot;SRX691644&quot; &quot;SRX692390&quot; &quot;SRX509287&quot; &quot;SRX693364&quot; ... ## $ experiment_title : chr &quot;Illumina amplicon sequencing of metagenomic paired-end library &#39;Solexa-122962&#39; containing sample &#39;SKBTI-0107&#39;&quot; &quot;Illumina amplicon sequencing of metagenomic paired-end library &#39;Solexa-122980&#39; containing sample &#39;SKBTI-0128&#39;&quot; &quot;Illumina amplicon sequencing of metagenomic paired-end library &#39;Solexa-123022&#39; containing sample &#39;SKBTI-0172&#39;&quot; &quot;Illumina amplicon sequencing of metagenomic paired-end library &#39;Solexa-133729&#39; containing sample &#39;SKBTI-0594&#39;&quot; ... ## $ study_name : logi NA NA NA NA NA NA ... ## $ sample_name : logi NA NA NA NA NA NA ... ## $ design_description : chr &quot;Illumina sequencing of human gut metagenome via polymerase chain reaction&quot; &quot;Illumina sequencing of human gut metagenome via polymerase chain reaction&quot; &quot;Illumina sequencing of human gut metagenome via polymerase chain reaction&quot; &quot;Illumina sequencing of human gut metagenome via polymerase chain reaction&quot; ... ## $ library_name : chr &quot;Solexa-122962&quot; &quot;Solexa-122980&quot; &quot;Solexa-123022&quot; &quot;Solexa-133729&quot; ... ## $ library_strategy : chr &quot;AMPLICON&quot; &quot;AMPLICON&quot; &quot;AMPLICON&quot; &quot;AMPLICON&quot; ... ## $ library_source : chr &quot;METAGENOMIC&quot; &quot;METAGENOMIC&quot; &quot;METAGENOMIC&quot; &quot;METAGENOMIC&quot; ... ## $ library_selection : chr &quot;PCR&quot; &quot;PCR&quot; &quot;PCR&quot; &quot;PCR&quot; ... ## $ library_layout : chr &quot;PAIRED - NOMINAL_SDEV: 0.0E0; NOMINAL_LENGTH: 390; &quot; &quot;PAIRED - NOMINAL_SDEV: 0.0E0; NOMINAL_LENGTH: 390; &quot; &quot;PAIRED - NOMINAL_SDEV: 0.0E0; NOMINAL_LENGTH: 393; &quot; &quot;PAIRED - NOMINAL_SDEV: 0.0E0; NOMINAL_LENGTH: 382; &quot; ... ## $ library_construction_protocol: logi NA NA NA NA NA NA ... ## $ adapter_spec : logi NA NA NA NA NA NA ... ## $ read_spec : chr &quot;READ_INDEX: 0; READ_LABEL: forward; READ_CLASS: Application Read; READ_TYPE: Forward; BASE_COORD: 1 || READ_IND&quot;| __truncated__ &quot;READ_INDEX: 0; READ_LABEL: forward; READ_CLASS: Application Read; READ_TYPE: Forward; BASE_COORD: 1 || READ_IND&quot;| __truncated__ &quot;READ_INDEX: 0; READ_LABEL: forward; READ_CLASS: Application Read; READ_TYPE: Forward; BASE_COORD: 1 || READ_IND&quot;| __truncated__ &quot;READ_INDEX: 0; READ_LABEL: forward; READ_CLASS: Application Read; READ_TYPE: Forward; BASE_COORD: 1 || READ_IND&quot;| __truncated__ ... ## $ platform : chr &quot;ILLUMINA&quot; &quot;ILLUMINA&quot; &quot;ILLUMINA&quot; &quot;ILLUMINA&quot; ... ## $ instrument_model : chr &quot;Illumina MiSeq&quot; &quot;Illumina MiSeq&quot; &quot;Illumina MiSeq&quot; &quot;Illumina MiSeq&quot; ... ## $ instrument_name : logi NA NA NA NA NA NA ... ## $ platform_parameters : chr &quot;INSTRUMENT_MODEL: Illumina MiSeq&quot; &quot;INSTRUMENT_MODEL: Illumina MiSeq&quot; &quot;INSTRUMENT_MODEL: Illumina MiSeq&quot; &quot;INSTRUMENT_MODEL: Illumina MiSeq&quot; ... ## $ sequence_space : logi NA NA NA NA NA NA ... ## $ base_caller : logi NA NA NA NA NA NA ... ## $ quality_scorer : logi NA NA NA NA NA NA ... ## $ number_of_levels : logi NA NA NA NA NA NA ... ## $ multiplier : logi NA NA NA NA NA NA ... ## $ qtype : logi NA NA NA NA NA NA ... ## $ experiment_url_link : logi NA NA NA NA NA NA ... ## $ experiment_entrez_link : logi NA NA NA NA NA NA ... ## $ experiment_attribute : chr &quot;analysis_type: AssemblyWithoutReference || gssr_id: 247611.0 || library_type: 16S || lsid: broadinstitute.org:b&quot;| __truncated__ &quot;analysis_type: AssemblyWithoutReference || gssr_id: 247629.0 || library_type: 16S || lsid: broadinstitute.org:b&quot;| __truncated__ &quot;analysis_type: AssemblyWithoutReference || gssr_id: 247671.0 || library_type: 16S || lsid: broadinstitute.org:b&quot;| __truncated__ &quot;analysis_type: AssemblyWithoutReference || data_type: 16S || gssr_id: 276389.0 || library_type: 16S || lsid: br&quot;| __truncated__ ... Using these commands, you should be able to download the .sra files you need along with all corresponding metadata to do analysis. Still, you might be wondering how you get the .fasta files from the .sra file. Well, the easiest way is to use something called the sra toolkit, which can be found here: https://www.ncbi.nlm.nih.gov/books/NBK158900/ . So let’s say we aimed to extract the sequences from the following sra files: SRR1635768 and SRR1566401. First, we’d download the files: sra_dir &lt;- tempdir() sra_fns &lt;- c(&quot;SRR1634425&quot;,&quot;SRR1634428&quot;) for (sra in sra_fns) getSRAfile(sra, sra_con, fileType = &#39;sra&#39;,destDir=sra_dir) ## Files are saved to: ## &#39;/tmp/RtmphpX6br&#39; ## ## Files are saved to: ## &#39;/tmp/RtmphpX6br&#39; Then, we’d use the sra toolkit to extract the sequences. Assuming you have downloaded and installed it, we can do the following sra_output &lt;- tempdir() sra_files &lt;- list.files(sra_dir,full.names=TRUE,pattern=&#39;\\\\.sra$&#39;) for (i in seq_along(sra_files)) system2(&#39;fastq-dump&#39;,args=c(sra_files[i], &#39;-O&#39;, sra_output, &#39;--gzip&#39;, &#39;--clip&#39;, &#39;--skip-technical&#39;, &#39;--dumpbase&#39;)) And now we can check: fqs &lt;- list.files(sra_output,full.names=TRUE) FASTQ &lt;- readDNAStringSet(fqs,format=&#39;fastq&#39;) FASTQ Note the header names; they’re simply the SRR ids. You’d have to the sra metadata (see above) to match these to specific samples. For example, sample_info &lt;- getSRA(search_terms=&#39;SRR1635768&#39;, out_types=c(&#39;sample&#39;),sra_con) str(sample_info) ## &#39;data.frame&#39;: 1 obs. of 10 variables: ## $ sample_alias : chr &quot;SKBTI-0325&quot; ## $ sample : chr &quot;SRS734393&quot; ## $ taxon_id : int 408170 ## $ common_name : logi NA ## $ anonymized_name : logi NA ## $ individual_name : logi NA ## $ description : logi NA ## $ sample_url_link : logi NA ## $ sample_entrez_link: logi NA ## $ sample_attribute : chr &quot;strain: SKBTI-0325 || collection date: missing || geographic location (country and/or sea, region): USA || spec&quot;| __truncated__ 6.0.1 Fastq Dump for Paired End Reads It should be noted that simply using the fastq-dump command as above works only if your data does not consist of paired end reads. If you happen to have paired end reads, then the following arguments must be added to the fastq-dump call: for (i in seq_along(sra_files)) system2(&#39;fastq-dump&#39;,args=c(sra_files[i], &#39;-O&#39;, sra_output, &#39;--gzip&#39;, &#39;--clip&#39;, &#39;--skip-technical&#39;, &#39;--dumpbase&#39;, &#39;--split-files&#39;, &#39;--readids&#39;)) For more information, see https://edwards.sdsu.edu/research/fastq-dump/ "],
["phyloseq.html", "Chapter 7 Phyloseq", " Chapter 7 Phyloseq For downstream metagenomic analysis, you cannot go wrong with Phyloseq. It’s an excellent tool for importing and analyzing metagenomic data, and acts as a wrapper for a considerable number of well known tools and packages ranging from vegan to DESeq2. Moreover, it’s well equipt for importing data you’d generate by using, say, QIIME, and works with its own set of structures, which really comes helps prevent potential indexing issues and the like. library(phyloseq) library(tidyverse) library(ape) library(DESeq2) Let’s begin by loading four pieces of data, an OTU table of taxonomic abundances (counts), taxonomy information with the taxa that a given OTU likely belongs to, sample metadata, and a phylogenetic tree. OTU &lt;- read.csv(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/1b05f24f189f14ea9902ac3867aca40c80ac6db3/otu_table.csv&#39;) TAX &lt;- read.csv(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/1b05f24f189f14ea9902ac3867aca40c80ac6db3/tax_table.csv&#39;) SAMP &lt;- read.csv(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/052dfdc3df97589f6405d79889c9b3b651eb1967/sample_metadata.csv&#39;) TREE &lt;- read.tree(&#39;https://gist.githubusercontent.com/sw1/8870a124624f31585d5c15be72fcfc21/raw/052dfdc3df97589f6405d79889c9b3b651eb1967/tree.tree&#39;) In most circumstances, we’d work on this as is, but the nice thing about phyloseq is that we can place these into a phyloseq container, allowing us to manipulate the four objects simultaneously. Imagine if we decided to filter some OTUs based on the leaves of our tree. We’d then may want to then remove these OTUs from our taxonomy. Also, removing a subset of OTUs may result in some samples with 0 total OTU counts, which justifies removing them as well. This can lead to indexing issues where we accidentally shuffle our tables. Using phyloseq, all of this is done in tandem, preventing said issues. Let’s create that container. We need to coerse each objects into phyloseq-friendly objects, so note the functions wrapping each object. Also note that our row and column names must be consistent throughout (i.e., named the same and in the same order). And lastly, the taxonomy table has to be a matrix. all(colnames(OTU) == SAMP$Sample_ID) ## [1] TRUE rownames(SAMP) &lt;- SAMP$Sample_ID TAX &lt;- as.matrix(TAX) rownames(TAX) &lt;- paste0(&#39;otu&#39;,1:nrow(TAX)) rownames(OTU) &lt;- rownames(TAX) taxa_names(TREE) &lt;- rownames(TAX) And now, create the phyloseq container: PS &lt;- phyloseq(otu_table(OTU,taxa_are_rows=TRUE),tax_table(TAX),sample_data(SAMP),phy_tree(TREE)) First, we’ll filter any samples without enterotype information and then conver enterotype to a factor: PS1 &lt;- prune_samples(!is.na(sample_data(PS)$Enterotype),PS) sample_data(PS1)$ENTEROTYPE &lt;- as.factor(sample_data(PS1)$Enterotype) Now, we’ll remove any OTUs with 0 counts across samples: PS1 &lt;- filter_taxa(PS1,function(x) sum(x) &gt; 0,prune = TRUE) This leaves us with the following objects: PS1 ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 342 taxa and 271 samples ] ## sample_data() Sample Data: [ 271 samples by 10 sample variables ] ## tax_table() Taxonomy Table: [ 342 taxa by 2 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 342 tips and 341 internal nodes ] From here, we can do quite a bit, so I’m not going to go through absolutely everything. But, we can start with some figures. We can plot some metagenomic summary statistics: plot_richness(PS1,x=&#39;ENTEROTYPE&#39;,color=&#39;ENTEROTYPE&#39;) ## Warning: Removed 1359 rows containing missing values (geom_errorbar). And then some figures to show abundance in different ways: plot_tree(PS1,color=&#39;ENTEROTYPE&#39;) plot_bar(PS1,fill=&#39;Group&#39;) And a heatmap, but using subsetted data: PS2 &lt;- prune_taxa(names(sort(taxa_sums(PS1),decreasing=TRUE))[1:50],PS1) plot_heatmap(PS2,sample.order=&#39;ENTEROTYPE&#39;,method=&#39;MDS&#39;,distance=&#39;bray&#39;) ## Warning: Transformation introduced infinite values in discrete y-axis Ordination is quite easy as well. ORD &lt;- ordinate(PS1,method=&#39;MDS&#39;,distance=&#39;bray&#39;) plot_ordination(PS1,ORD,color=&#39;ENTEROTYPE&#39;) + geom_point(size=5) And networks NET &lt;- make_network(PS1,max.dist=.3,distance=&#39;bray&#39;) plot_network(NET,PS1,color=&#39;ENTEROTYPE&#39;,label=NULL) ## Warning: attributes are not identical across measure variables; they will be ## dropped And lastly, let’s say we wanted to perform a differential abundance analysis between genders using DESeq2: PS3 &lt;- prune_samples(!is.na(sample_data(PS)$Gender),PS) PS3 &lt;- filter_taxa(PS3,function(x) sum(x) &gt; 0,prune = TRUE) diagdds &lt;- phyloseq_to_deseq2(PS3, ~ Gender) ## converting counts to integer mode diagdds &lt;- DESeq(diagdds, test=&#39;Wald&#39;, fitType=&#39;parametric&#39;) ## estimating size factors ## estimating dispersions ## gene-wise dispersion estimates ## mean-dispersion relationship ## final dispersion estimates ## fitting model and testing ## -- replacing outliers and refitting for 40 genes ## -- DESeq argument &#39;minReplicatesForReplace&#39; = 7 ## -- original counts are preserved in counts(dds) ## estimating dispersions ## fitting model and testing res &lt;- results(diagdds, cooksCutoff = FALSE) res ## log2 fold change (MLE): Gender M vs F ## Wald test p-value: Gender M vs F ## DataFrame with 135 rows and 6 columns ## baseMean log2FoldChange lfcSE ## &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; ## otu1 4828.4506120415 0.245290778934588 0.274237524136042 ## otu5 0.408423815044056 -0.763578551311355 1.79402732847231 ## otu8 0.0792034228989187 -0.0356169619939926 2.98069121886303 ## otu9 0.0320327532464582 -0.0814917834894285 2.98139846283725 ## otu29 0.264845130897552 0.300955738130655 1.94223006989263 ## ... ... ... ... ## otu245 157.499213859763 0.0699225946302586 0.771481998887212 ## otu246 0.0555641881033384 0.0556347017417495 2.98128159912465 ## otu247 8.66211938696038 0.21953394551718 0.554061558082688 ## otu248 5.17914838549527 0.97428692011149 0.580195818970534 ## otu249 0.370083654698921 -0.103761272249879 1.25671122049443 ## stat pvalue padj ## &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; ## otu1 0.894446446405729 0.371083071088348 0.999308335917726 ## otu5 -0.425622586229818 0.670382879571334 0.999308335917726 ## otu8 -0.011949229013926 0.990466121537832 0.999308335917726 ## otu9 -0.0273334089707274 0.978193810311412 0.999308335917726 ## otu29 0.154953701312683 0.876857817345056 0.999308335917726 ## ... ... ... ... ## otu245 0.0906341233251264 0.92778331701462 0.999308335917726 ## otu246 0.0186613373785572 0.985111271182231 0.999308335917726 ## otu247 0.396226632789451 0.691937845592669 0.999308335917726 ## otu248 1.67923809213277 0.0931056508804161 0.999308335917726 ## otu249 -0.0825657243746543 0.934196856171995 0.999308335917726 "],
["dada.html", "Chapter 8 Dada2 8.1 FASTQ Prep 8.2 OTU Picking 8.3 Running Dada2 on Proteus", " Chapter 8 Dada2 library(tidyverse) library(dada2) library(gridExtra) library(DECIPHER) library(ape) library(phangorn) library(phyloseq) First we need to use some qiime functions via the command line, so we’ll save them as variable names. validate_mapping_file &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/validate_mapping_file.py&#39; split_libraries_fastq &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/split_libraries_fastq.py&#39; split_sequence_file_on_sample_ids &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/split_sequence_file_on_sample_ids.py&#39; 8.1 FASTQ Prep We’re going to use the same data as we did with QIIME. Dada2 requires individual fastq files for each sample, so we’ll split our single fastq file using a few QIIME commands. data_dir &lt;- &#39;data/data_moving_pictures&#39; MAP &lt;- read_delim(file.path(data_dir,&#39;map.tsv&#39;),&#39;\\t&#39;) ## Parsed with column specification: ## cols( ## `#SampleID` = col_character(), ## BarcodeSequence = col_character(), ## LinkerPrimerSequence = col_character(), ## SampleType = col_character(), ## Year = col_double(), ## Month = col_double(), ## Day = col_double(), ## Subject = col_double(), ## ReportedAntibioticUsage = col_character(), ## DaysSinceExperimentStart = col_double(), ## Description = col_character() ## ) Note that now we are forcing the split libraries command to also return a demultiplexed fastq file. We going to also add a bunch of arguments to ensure that QIIME does no filtering. We want to deal with that using dada2. system2(split_libraries_fastq,args=c(&#39;-o&#39;,file.path(data_dir,&#39;fastq_out_3&#39;), &#39;-i&#39;,file.path(data_dir,&#39;forward_reads.fastq.gz&#39;), &#39;-b&#39;,file.path(data_dir,&#39;barcodes.fastq.gz&#39;), &#39;-m&#39;,file.path(data_dir,&#39;map.tsv&#39;), &#39;-r&#39;,&#39;999&#39;, &#39;-n&#39;,&#39;999&#39;, &#39;-q&#39;,&#39;0&#39;, &#39;-p&#39;,&#39;0.0001&#39;, &#39;--store_demultiplexed_fastq&#39;)) Next, we’ll split these fastq file into separate files for each sample: system2(split_sequence_file_on_sample_ids,args=c(&#39;-i&#39;,file.path(data_dir,&#39;fastq_out_3&#39;,&#39;seqs.fastq&#39;), &#39;-o&#39;,file.path(data_dir,&#39;fastq_out_3&#39;,&#39;sequences&#39;), &#39;--file_type&#39;,&#39;fastq&#39;)) 8.2 OTU Picking We’re now going to run through the dada2 workflow. Dada2 is a reference free method, so this is analogous to de novo OTU picking had we used QIIME. Still, we can cluster our resulting count table into OTUs using a reference database; hence, we can compare our results. Dada2 captures metagenomic variation by exploiting illumina sequencing errors. Briefly, everything is based on an error model (a Poisson distribution). A given read is defined as a sample sequence and is compared to all other reads. The model calculates the probability these reads were generated from the sample sequence given the error model – that is, the probability that these reads resulted from independent sequencing errors that were generated based on given transition probabilities and quality scores. If a set of reads are too abundant to be explained by this error model, then they are separated into their own partition. The game is to continuously partition the reads until each partition is consistent with the error model, allowing us to separate true biological variation from the variation solely due to sequencing error. At this point, the abundance of reads within a partition can be calculated, giving us an abundance table. We can then compare the sequences associated with a given partition to a reference database to assign taxonomy. fqs &lt;- list.files(file.path(data_dir,&#39;fastq_out_3&#39;,&#39;sequences&#39;),full.names=TRUE) The first thing we’ll do is plot the quality scores as a function of base position. If you recall the definition of Q from the QIIME tutorial, this should make sense to you, and it should also help you appriciate setting those parameters for quality filtering before. sample_idx &lt;- sample(length(fqs),5) for (i in sample_idx) print(plotQualityProfile(fqs[i])) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. Per the Holmes group, illumina datasets often have errors in the first 10 base positions. Also, given the plots above, there seems to be a drop in quality towards the end of each read. Hence, we’ll trim our reads such that we keep bases 10 through 130. fqs_filt &lt;- gsub(&#39;sequences&#39;,&#39;filtered&#39;,fqs) dir.create(gsub(&#39;(filtered).*&#39;,&#39;\\\\1&#39;,fqs_filt[1]),showWarnings=FALSE) for (i in seq_along(fqs)){ fastqFilter(fqs[i],fqs_filt[i], trimLeft=10, truncLen=130, maxN=0, maxEE=2, truncQ=2, compress=TRUE) } The following command performs dereplication, returning a set of unique sequences and abundances from our set of fastq files. derep &lt;- derepFastq(fqs_filt) names(derep) &lt;- sapply(strsplit(basename(fqs_filt), &quot;_&quot;), `[`, 1) Dada2’s error model depends on the fact that there are 16x41 transition probabilities, but if these values are unknown, we can simply estimate them from the data. However, estimating the error rates to parameterize the model is costly, so it’s recommended to do this on a subset of the data, and then use these parameter estimates for the complete dataset: dd_err &lt;- dada(derep[1:10], err=NULL, selfConsist=TRUE) ## Initializing error rates to maximum possible estimate. ## selfConsist step 1 .......... ## selfConsist step 2 ## selfConsist step 3 ## selfConsist step 4 ## selfConsist step 5 ## selfConsist step 6 ## selfConsist step 7 ## Convergence after 7 rounds. We can visualize the error estimates. This shows the frequency of each base transition as a function of quality score. plotErrors(dd_err,err_in=TRUE,nominalQ=TRUE) ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Transformation introduced infinite values in continuous y-axis We’ll now fit the full error model, using the estimated error rates from our subset of data. This step can be parallelized using the multihread argument. We’re also going to pool across samples, since this improves the detection of variants that are rare in a specific sample but less rare overall, but at a computational cost. Note that we can pass a lot of arguments into this function that control the error model. As briefly described above, the game is to partition these sequences until each partition is consistent with being generated soley by illumina and amplification error variation, and not due to biological variation. A p-value is calculated to test the sequences that form new partitions, with signififcant p-values at a given threshold leading to new parittions. Now, say we had a problem where rare sequence variants were of most interest. It would then make sense to use a less conservative significance threshold, leading to more significant p-values, more partitions, and hence more rare variants. We can do this by increasing the OMEGA_A value from its default value of \\(1\\times10^{-40}\\). We’ll fit a second model and change the threshold to \\(1\\times10^{-20}\\). dd &lt;- dada(derep, err=dd_err[[1]]$err_out, pool=TRUE) ## 34 samples were pooled: 109854 reads in 18034 unique sequences. dd_rare &lt;- dada(derep, err=dd_err[[1]]$err_out, pool=TRUE, OMEGA_A=1e-20) ## 34 samples were pooled: 109854 reads in 18034 unique sequences. Finally, we’ll make our sequence table. seqtab_all &lt;- makeSequenceTable(dd) seqtab_all_rare &lt;- makeSequenceTable(dd_rare) And then we’ll remove chimeras. This function compares sequences against one another, and removes sequences that can be generated by joining two abundant sequences. seqtab &lt;- removeBimeraDenovo(seqtab_all) seqtab_rare &lt;- removeBimeraDenovo(seqtab_all_rare) First, note the difference in dimensions; there are more sequences in the rare table: ncol(seqtab) ## [1] 662 ncol(seqtab_rare) ## [1] 817 Also note that despite now having a sequence abundance table that is similar in form to the OTU table we generated in QIIME, our ‘taxonomic variants’ are unique sequences and not OTUs: seqtab[1:5,1:5] ## CCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGATGGATGTTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGATATCTTGAGTGCAGTT ## L1S105.fastq 1769 ## L1S140.fastq 8 ## L1S208.fastq 11 ## L1S257.fastq 6 ## L1S281.fastq 4 ## GCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGAGCGCAGACGGTTACTTAAGCAGGATGTGAAATCCCCGGGCTCAACCTGGGAACTGCGTTCTGAACTGGGTGACTAGAGTGTGTCA ## L1S105.fastq 5 ## L1S140.fastq 1 ## L1S208.fastq 0 ## L1S257.fastq 0 ## L1S281.fastq 0 ## CCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGATGGATGTTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGATACTGGATGTCTTGAGTGCAGTT ## L1S105.fastq 8 ## L1S140.fastq 1531 ## L1S208.fastq 1552 ## L1S257.fastq 870 ## L1S281.fastq 1196 ## GCGAGCGTTAATCGGAATAACTGGGCGTAAAGGGCACGCAGGCGGTGACTTAAGTGAGGTGTGAAAGCCCCGGGCTTAACCTGGGAATTGCATTTCATACTGGGTCGCTAGAGTACTTTA ## L1S105.fastq 0 ## L1S140.fastq 0 ## L1S208.fastq 5 ## L1S257.fastq 0 ## L1S281.fastq 0 ## CCGAGCGTTGTCCGGATTTATTGGGCGTAAAGCGAGCGCAGGCGGTTAGATAAGTCTGAAGTTAAAGGCTGTGGCTTAACCATAGTACGCTTTGGAAACTGTTTAACTTGAGTGCAAGAG ## L1S105.fastq 1 ## L1S140.fastq 0 ## L1S208.fastq 2 ## L1S257.fastq 0 ## L1S281.fastq 0 If we want to assign taxonomy from a reference database to these sequences, we can use the following command that applies a naive Bayes classifer to compare our sequences to classified sequences in a training set. First, we’ll use a GreenGenes training set: ref_fasta &lt;- &#39;data/data_stability/references/gg_13_8_train_set_97.fa.gz&#39; taxtab_gg &lt;- assignTaxonomy(seqtab, refFasta=ref_fasta) colnames(taxtab_gg) &lt;- c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) Now, instead, we can try a Silva training set. Note that Silva does not give species level assignments ref_fasta &lt;- &#39;data/data_stability/references/silva_nr_v123_train_set.fa.gz&#39; taxtab_silva &lt;- assignTaxonomy(seqtab, refFasta=ref_fasta) colnames(taxtab_silva) &lt;- c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;) If we want species level assignments, we can do the following. ref_fasta &lt;- &#39;data/data_stability/references/rdp_species_assignment_14.fa.gz&#39; sptab_silva &lt;- assignSpecies(seqtab, refFasta=ref_fasta, allowMultiple=FALSE, verbose=TRUE) Next, we might want to build a phylogenetic tree. First, we perform a multiple sequence alignment: seqs &lt;- getSequences(seqtab) names(seqs) &lt;- seqs alignment &lt;- AlignSeqs(DNAStringSet(seqs), anchor=NA) Then, we’ll build a tree, specifically, a maximum likelihood tree from a NJ tree. phang_align &lt;- phyDat(as.matrix(alignment), type=&quot;DNA&quot;) dm &lt;- dist.ml(phang_align) treeNJ &lt;- NJ(dm) # Note, tip order != sequence order fit = pml(treeNJ, data=phang_align) fit_gtr &lt;- update(fit, k=4, inv=0.2) fit_gtr &lt;- optim.pml(fit_gtr, model=&quot;GTR&quot;, optInv=TRUE, optGamma=TRUE, rearrangement = &quot;stochastic&quot;, control = pml.control(trace = 0)) And finally, we can build our phyloseq object for our GreenGenes table: TREE &lt;- phy_tree(fit_gtr) META &lt;- as.data.frame(MAP) rownames(META) &lt;- META$`#SampleID` OTU &lt;- seqtab rownames(OTU) &lt;- gsub(&#39;\\\\.fastq&#39;,&#39;&#39;,rownames(OTU)) OTU &lt;- OTU[rownames(META),] OTU &lt;- otu_table(OTU,taxa_are_rows=FALSE) META &lt;- sample_data(META) TAXA &lt;- taxtab_gg[colnames(OTU),] TAXA &lt;- tax_table(TAXA) PS &lt;- phyloseq(OTU,TAXA,META,TREE) 8.3 Running Dada2 on Proteus Now let’s redo the analysis above, but on a cluster. I’ll explain below how to run dada2 via two ways. The first will involve simply using packages installed in a shared group folder. The second will cover installing dada2 in a local folder. If you lack access to the shared folder, or, if for some reason it no longer exists, go to method 2. 8.3.1 Method 1: Using nameGrp Shared R Library First, we’ll create a bash script that runs some QIIME commands to do the demultiplexing and splitting, and then runs the dada anlysis. We’ll assume that the moving pictures data is in your home directory. As before, the folder should contain three files: (1) the reads, (2) the barcodes, and (3) the mapping file. We’ll name the script prep_sub.sh: #!/bin/bash #$ -S /bin/bash #$ -cwd #$ -j y #$ -M user_name@email.edu #$ -l h_rt=01:00:00 #$ -P namePrj #$ -l mem_free=12G #$ -l h_vmem=16G #$ -q all.q . /etc/profile.d/modules.sh module load shared module load proteus module load sge/univa module load gcc/4.8.1 module load qiime/gcc/64/1.9.1 export ref_seqs=/mnt/HA/opt/qiime/gcc/64/1.9.1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/rep_set/97_otus.fasta export ref_tax=/mnt/HA/opt/qiime/gcc/64/1.9.1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt data_dir=/home/user_name/dirname/dada2/moving_pictures work_dir=/scratch/user_name/moving_pictures mkdir -p $work_dir cp $data_dir/* $work_dir out_dir=$work_dir/fastq_out seqs=$work_dir/forward_reads.fastq.gz bc=$work_dir/barcodes.fastq.gz map=$work_dir/map.tsv split_libraries_fastq.py -o $out_dir -i $seqs -b $bc -m $map -r 999 -n 999 -q 0 -p 0.0001 --store_demultiplexed_fastq split_sequence_file_on_sample_ids.py -i $out_dir/seqs.fastq -o $out_dir/sequences --file_type fastq mv $out_dir/sequences $data_dir rm -r $work_dir/* exit 0 This gives us our demultiplexed, split sequences. Now, we’ll make a dada2 R script that performs the actual analysis. We’ll call this dada.R. require(dada2) scratch_path &lt;- &#39;/scratch/user_name/moving_pictures&#39; ref_fasta &lt;- file.path(scratch_path,&#39;silva_nr_v123_train_set.fa.gz&#39;) fq_dir &lt;- file.path(scratch_path,&#39;sequences&#39;) fqs &lt;- list.files(fq_dir,full.names=TRUE) fqs_filt &lt;- gsub(&#39;sequences&#39;,&#39;filtered&#39;,fqs) dir.create(gsub(&#39;(filtered).*&#39;,&#39;\\\\1&#39;,fqs_filt[1]),showWarnings=FALSE) for (i in seq_along(fqs)){ fastqFilter(fqs[i],fqs_filt[i], trimLeft=10, truncLen=130, maxN=0, maxEE=2, truncQ=2, compress=TRUE) } derep &lt;- derepFastq(fqs_filt) names(derep) &lt;- sapply(strsplit(basename(fqs_filt), &quot;_&quot;), `[`, 1) dd_err &lt;- dada(derep[1:10],err=NULL,selfConsist=TRUE, multithread=TRUE,VERBOSE=TRUE) dd &lt;- dada(derep, err=dd_err[[1]]$err_out, pool=TRUE, multithread=TRUE,VERBOSE=TRUE) seqtab_all &lt;- makeSequenceTable(dd) seqtab &lt;- removeBimeraDenovo(seqtab_all,tableMethod=&#39;pooled&#39;,verbose=TRUE, multithread=TRUE) taxtab_silva &lt;- assignTaxonomy(seqtab,refFasta=ref_fasta,verbose=TRUE) colnames(taxtab_silva) &lt;- c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;) saveRDS(seqtab,file.path(scratch_path,&#39;seqtab.rds&#39;)) saveRDS(taxtab_silva,file.path(scratch_path,&#39;taxtab.rds&#39;)) Finally, we’ll create the submission script. Note the following change that allows you to use the packages in the shared folder. In this submission script, immediately after you load your modules, you must add the line: export R_LIBS=/mnt/HA/groups/nameGrp/r_libs This gives us the following submission script: #!/bin/bash #$ -S /bin/bash #$ -cwd #$ -j y #$ -M user_name@email.edu #$ -l h_rt=01:00:00 #$ -P namePrj #$ -pe shm 16 #$ -l mem_free=12G #$ -l h_vmem=16G #$ -q all.q . /etc/profile.d/modules.sh module load shared module load proteus module load sge/univa module load gcc/4.8.1 export R_LIBS=/mnt/HA/groups/nameGrp/r_libs data_dir=/home/user_name/dirname/dada2 work_dir=/scratch/user_name/moving_pictures mkdir -p $work_dir cp -r $data_dir/moving_pictures/sequences $work_dir cp $data_dir/dada.R $work_dir cp ~/references/silva_nr_v123_train_set.fa.gz $work_dir R CMD BATCH $work_dir/dada.R mv $work_dir/*.Rout $data_dir mv $work_dir/*.rds $data_dir rm -rf $work_dir exit 0 8.3.2 Method 2: Creating a Local Library We first need to make two scripts. One will be an R package script that installs our packages into a personal library folder. The other will run this script, but it will first make said folder and also unload any preloaded gcc modules that may cause conflicts during pacakge installation. First, make sure you’re in your home directory. We’ll now make the R package installer script. We’ll call it install_r_pkgs.R. MYLIB &lt;- Sys.getenv(&#39;R_LIBS_USER&#39;) source(&#39;https://bioconductor.org/biocLite.R&#39;) biocLite(&#39;dada2&#39;,lib=MYLIB) Next, we’ll make the bash script that runs this, which we’ll call run_install_r_pkgs.sh: #!/bin/bash module unload gcc Rscript -e &quot;dir.create(Sys.getenv(&#39;R_LIBS_USER&#39;),showWarnings=FALSE,recursive=TRUE)&quot; R CMD BATCH install_r_pkgs.R Finally, we’ll run the bash script by entering the following at the command line (this will take a few minutes to run): chmod +x run_install_r_pkgs.sh ./run_install_r_pkgs.sh You should now have a R folder in your home directory, and if you navigate through it, you should see a dada folder. To ensure your installation worked, in your home directly, type R to enter the R environment. Then, run library(dada2). Assuming everything loads correctly, we can proceed to submitting a job. Note that in the submission script, you must remove the line where we changed the R_LIBS path: export R_LIBS=/mnt/HA/groups/nameGrp/r_libs which gives us the following submission script: #!/bin/bash #$ -S /bin/bash #$ -cwd #$ -j y #$ -M user_name@email.edu #$ -l h_rt=01:00:00 #$ -P namePrj #$ -pe shm 16 #$ -l mem_free=12G #$ -l h_vmem=16G #$ -q all.q . /etc/profile.d/modules.sh module load shared module load proteus module load sge/univa module load gcc/4.8.1 data_dir=/home/user_name/dirname/dada2 work_dir=/scratch/user_name/moving_pictures mkdir -p $work_dir cp -r $data_dir/moving_pictures/sequences $work_dir cp $data_dir/dada.R $work_dir cp ~/references/silva_nr_v123_train_set.fa.gz $work_dir R CMD BATCH $work_dir/dada.R mv $work_dir/*.Rout $data_dir mv $work_dir/*.rds $data_dir rm -rf $work_dir exit 0 "],
["qiime.html", "Chapter 9 Qiime 9.1 OTU Picking 9.2 Summarizing Our Results 9.3 Loading QIIME Results into Phyloseq", " Chapter 9 Qiime library(tidyverse) library(Biostrings) First we need to use some qiime functions via the command line, so we’ll save them as variable names. validate_mapping_file &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/validate_mapping_file.py&#39; split_libraries_fastq &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/split_libraries_fastq.py&#39; count_seqs &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/count_seqs.py&#39; extract_barcodes &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/extract_barcodes.py&#39; pick_closed_reference_otus &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/pick_closed_reference_otus.py&#39; core_diversity_analyses &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/core_diversity_analyses.py&#39; make_emperor &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/make_emperor.py&#39; biom &lt;- &#39;/data/sw1/anaconda3/envs/qiime1/bin/biom&#39; 9.1 OTU Picking This will be an introduction to QIIME. We’ll start by going command by command, and then at the end will be a submission script for proteus. This should give you two flavors of running QIIME, one from within R, should you choose to install QIIME locally, and one via Proteus. We’re going to use the dataset from the QIIME illumina tutorial, moving pictures, simply because it’s small and reliable. If you choose to install QIIME and worth through this locally, you can download the dataset by running the following command in bash, or you can simply go to the link and download it manually as well. svn checkout https://github.com/sw1/Bioinformatics/trunk/Data/moving_pictures} Once it’s downloaded, set the directly to a variable: data_dir &lt;- &#39;data/data_moving_pictures&#39; Now, we have a large FASTQ file with a ton of sequences: FASTQ &lt;- readDNAStringSet(file.path(data_dir,&#39;forward_reads.fastq.gz&#39;),format=&#39;fastq&#39;) FASTQ ## A DNAStringSet instance of length 302581 ## width seq names ## [1] 152 TACGNAGGATCCGAGCGTTAT...GGCAGGGGGGGATTGGTGTG HWI-EAS440_0386:1... ## [2] 152 CCCCNCAGCGGCAAAAATTAA...TGATGATTCCACTGCAACAA HWI-EAS440_0386:1... ## [3] 152 TACGNAGGATCCGAGCGTTAT...GGCAGGGGGGGGGTTGGGGG HWI-EAS440_0386:1... ## [4] 152 TACGNAGGATCCGAGCGTTAT...GGCAGGGGGGAGTTTGGGGG HWI-EAS440_0386:1... ## [5] 152 TACGNAGGATCCGAGCGTTAT...GGCAGGCGGGATTCGTGGTG HWI-EAS440_0386:1... ## ... ... ... ## [302577] 152 NTGGCTGTTGGTTTCTCTGTG...TTCAGAATCAGAATGAGCCG HWI-EAS440_0386:6... ## [302578] 152 NACGTAGGTGGCAAGCGTTGT...GAAAGTGGAATTCCTAGTGA HWI-EAS440_0386:6... ## [302579] 152 NACGTAGGGTGCGAGCGTTAA...GGGAGGTAGAATTACACGTG HWI-EAS440_0386:6... ## [302580] 152 NACGTAGGGTGCGAGCGTTAA...GGGAGGTAGAACTCCACGTG HWI-EAS440_0386:6... ## [302581] 152 NACGTAGGTGGCAAGCGTTGT...GAGAGGTGGATTCATAGGAG HWI-EAS440_0386:6... Note the sequence header names; they have an interesting format, for examples HWI-EAS440_0386:1:23:17547:1423#0/1 If you look at the figure below, you can see a detailed explaination of what this all means if you are interested, but it is information from illumina sequencing. Basically, when we sequence samples using illumina, we sequence multiple samples in a single flowcell lane to save money. These headers have some of that lane information encoded in them. Seq id Because we are mixing samples within a lane, we need a way to keep track of the samples, such that we know which sequence belongs to Frank and which belongs to Emily, despite many of their sequences sharing a lane. The way we keep track is through barcoding, where we add a short nucleotide sequence to the adapters used in PCR amplification. We therefore create sequence libraries, with each library receiving its own barcode, perform sequencing, and then parse these libraries using our barcodes to reassign sequences to our original samples. Look at the metadata mapping file below. This contains all of the relevant sample information, as well as information associated with sequencing, particularly barcode information. MAP &lt;- read_delim(file.path(data_dir,&#39;map.tsv&#39;),&#39;\\t&#39;) ## Parsed with column specification: ## cols( ## `#SampleID` = col_character(), ## BarcodeSequence = col_character(), ## LinkerPrimerSequence = col_character(), ## SampleType = col_character(), ## Year = col_double(), ## Month = col_double(), ## Day = col_double(), ## Subject = col_double(), ## ReportedAntibioticUsage = col_character(), ## DaysSinceExperimentStart = col_double(), ## Description = col_character() ## ) MAP[1:5,1:5] ## # A tibble: 5 x 5 ## `#SampleID` BarcodeSequence LinkerPrimerSequence SampleType Year ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 L1S8 AGCTGACTAGTC GTGCCAGCMGCCGCGGTAA gut 2008 ## 2 L1S140 ATGGCAGCTCTA GTGCCAGCMGCCGCGGTAA gut 2008 ## 3 L1S57 ACACACTATGGC GTGCCAGCMGCCGCGGTAA gut 2009 ## 4 L1S208 CTGAGATACGCG GTGCCAGCMGCCGCGGTAA gut 2009 ## 5 L1S76 ACTACGTGTGGT GTGCCAGCMGCCGCGGTAA gut 2009 We can see that each sample receives its own unique barcode: nrow(MAP[,1:2]) == nrow(unique(MAP[,1:2])) ## [1] TRUE Now take a look at the barcode file: BARCODE &lt;- readDNAStringSet(file.path(data_dir,&#39;barcodes.fastq.gz&#39;),format=&#39;fastq&#39;) BARCODE ## A DNAStringSet instance of length 302581 ## width seq names ## [1] 12 ATGCAGCTCAGT HWI-EAS440_0386:1... ## [2] 12 CCCCTCAGCGGC HWI-EAS440_0386:1... ## [3] 12 GACGAGTCAGTC HWI-EAS440_0386:1... ## [4] 12 AGCAGTCGCGAT HWI-EAS440_0386:1... ## [5] 12 AGCACACCTACA HWI-EAS440_0386:1... ## ... ... ... ## [302577] 12 ATGGCTGTTGGT HWI-EAS440_0386:6... ## [302578] 12 ACACACTATGGC HWI-EAS440_0386:6... ## [302579] 12 CAGCGGTGACAT HWI-EAS440_0386:6... ## [302580] 12 GATCTTCAGTAC HWI-EAS440_0386:6... ## [302581] 12 GACAGCGTTGAC HWI-EAS440_0386:6... We can see that each barcode is assigned one of the header names we saw before. What we need to do now is break up this large fastq file that contains all of our sequence information into a new large file that has headers with unique sample IDs for each sequence, for example, Emily_1, Emily_2, and Emily_3 for 3 sequences belonging to Emily. We can easily do this in QIIME, but we first need to ensure that our mapping file is formatted correctly; otherwise, QIIME will not run. We can do that with the following command: system2(validate_mapping_file,args=c(&#39;-m&#39;,file.path(data_dir,&#39;map.tsv&#39;))) Now, let’s make some bad mapping files to see what happens. First, let’s simply remove the hashtag in the first column name: MAP_bad1 &lt;- MAP colnames(MAP_bad1)[1] &lt;- &#39;SampleID&#39; tmp &lt;- tempfile() write_delim(MAP_bad1,path=tmp,delim=&#39;\\t&#39;) system2(validate_mapping_file,args=c(&#39;-m&#39;,tmp)) Now, look what happens if we don’t have a description column as the last column: MAP_bad2 &lt;- MAP MAP_bad2 &lt;- MAP_bad2[,-ncol(MAP_bad2)] tmp &lt;- tempfile() write_delim(MAP_bad2,path=tmp,delim=&#39;\\t&#39;) system2(validate_mapping_file,args=c(&#39;-m&#39;,tmp)) And lastly, what if we used separate with commas instead of tabs: MAP_bad3 &lt;- MAP tmp &lt;- tempfile() write_delim(MAP_bad3,path=tmp,delim=&#39;,&#39;) system2(validate_mapping_file,args=c(&#39;-m&#39;,tmp)) Given that we have a good mapping file, let’s actually demultiplex our data. The following QIIME command performs demultiplexing and also the subsequent quality filtering. It also tosses out sequences that have poor matches to a given barcode. All of these parameters we can adjust. out &lt;- tempfile() system2(validate_mapping_file,args=c(&#39;-m&#39;,tmp, &#39;-o&#39;,file.path(data_dir,&#39;map_out&#39;))) system2(split_libraries_fastq,args=c(&#39;-o&#39;,file.path(data_dir,&#39;fastq_out_1&#39;), &#39;-i&#39;,file.path(data_dir,&#39;forward_reads.fastq.gz&#39;), &#39;-b&#39;,file.path(data_dir,&#39;barcodes.fastq.gz&#39;), &#39;-m&#39;,file.path(data_dir,&#39;map.tsv&#39;))) Now, there are a few noteable filtering arguments: phred_quality_threshold max_bad_run_length min_per_read_length_fraction Phred score is defined as follows: \\[ Q=-10\\log_{10}P \\] where P is the probability of a base-call error; hence, \\[ P=10^{-Q/10} \\] The default phred_quality_threshold Q is 3, implying \\(P=10^{-3/10}=.5\\), or the probability of an incorrectly called base is about 50%. Setting it at 3 will flag any base call if it has a probability of being an error above this threshold. For the entire sequence, it will then look at max_bad_run_length, which checks how many consecutive flagged base calls are present. If this number is above a given threshold (the default is 3), then the sequence is truncated at that position. Finally, it checks min_per_read_length_fraction. This tosses any sequences that are shorter than a given length after truncation (the default is a length shorter than 75% of the original, unaltered read). There is also a setting called phred offset, which you typically need not worry about (it’s automatically set), but is worth knowing what it represents: Phred offset I’d argue that something along the lines of Q=20 or greater makes more sense. Here, we’ll truncate at consecutive stretches of base calls where each call has 1% probability of being an error. system2(split_libraries_fastq,args=c(&#39;-o&#39;,file.path(data_dir,&#39;fastq_out_2&#39;), &#39;-i&#39;,file.path(data_dir,&#39;forward_reads.fastq.gz&#39;), &#39;-b&#39;,file.path(data_dir,&#39;barcodes.fastq.gz&#39;), &#39;-m&#39;,file.path(data_dir,&#39;map.tsv&#39;), &#39;-q&#39;,&#39;20&#39;)) We’ll now perform OTU picking on our filtered, demultiplexed sequences. There are a ton of parameters we can adjust: http://qiime.org/scripts/pick_otus.html. To keep it somewhat simple, we’ll perform a particular kind of OTU picking called ‘closed.’ There are 3 OTU picking strategies: closed, open, and de novo. Closed reference OTU picking is fast because it simply clusters sequences into OTUs based on reference sequences found in a database. If a sequence doesn’t match above a given similiary threshold to the database sequence, it is tossed away. It should probably be obvious if a given sequence isn’t found in the reference database, it too will be removed. De novo, on the other hand, can deal with novel sequences since it doesn’t use a lookup database. Instead, it clusters sequences by aligning them against one another, with sequences above a given similarity threshold clustered into OTUs. Lastly, open reference OTU picking first performed closed reference OTU picking, and then performs de novo on sequences that failed to match reference sequences in the lookup database. To perform closed reference OTU picking, we need to specify our reference database (we’ll use GreenGenes) by setting the location of our reference sequences, our reference taxonomy. We also need to specify our picking parameters, which in this example, will consist of the OTU picking method (sortmerna), the number of threads for paralellization, and the similarity threshold (.97) for our sequences when we compare them to the GreenGenes database. ref_seqs &lt;- &#39;/data/sw1/anaconda2/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/rep_set/97_otus.fasta&#39; ref_tax &lt;- &#39;/data/sw1/anaconda2/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt&#39; params &lt;- tempfile(fileext=&#39;.txt&#39;) write_lines(c(&#39;pick_otus:otu_picking_method sortmerna&#39;, &#39;pick_otus:threads 4&#39;, &#39;pick_otus:similarity: 0.97&#39;),path=params) And now we run the closed reference otu picking command: system2(pick_closed_reference_otus,args=c(&#39;-i&#39;,file.path(data_dir,&#39;fastq_out_2&#39;,&#39;seqs.fna&#39;), &#39;-o&#39;,file.path(data_dir,&#39;fastq_out_2&#39;,&#39;picked_otus_sortmerna&#39;), &#39;-r&#39;,ref_seqs, &#39;-t&#39;,ref_tax, &#39;-p&#39;,params)) This was all done in R, which is probably unnecessary for proteus. You can submit an Rscript, but since there is no real looping and need for immediate downstream analysis, it’s better off just submitting a bash script. We’ll remove the system2 functions and the like, add the submission parameters, and write the following script for proteus: #!/bin/bash #$ -S /bin/bash #$ -cwd #$ -j y #$ -M user_name@email.edu #$ -l h_rt=01:00:00 #$ -P namePrj #$ -pe shm 16 #$ -l mem_free=12G #$ -l h_vmem=16G #$ -q all.q . /etc/profile.d/modules.sh module load shared module load proteus module load sge/univa module load gcc/4.8.1 module load qiime/gcc/64/1.9.1 export ref_seqs=/mnt/HA/opt/qiime/gcc/64/1.9.1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/rep_set/97_otus.fasta export ref_tax=/mnt/HA/opt/qiime/gcc/64/1.9.1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt data_dir=/home/user_name/genStats/moving_pictures work_dir=/scratch/user_name/moving_pictures mkdir -p $work_dir cp $data_dir/* $work_dir out_dir=$work_dir/fastq_out seqs=$work_dir/forward_reads.fastq.gz bc=$work_dir/barcodes.fastq.gz map=$work_dir/map.tsv split_libraries_fastq.py -o $out_dir -i $seqs -b $bc -m $map -q 20 params=$work_dir/sortmerna_pick_params.txt printf &quot;pick_otus:otu_picking_method sortmerna\\npick_otus:threads 16\\npick_otus:similarity 0.97&quot; &gt; $params pick_closed_reference_otus.py -i $out_dir/seqs.fna -o $out_dir/picked_otus -r $ref_seqs -t $ref_tax -p $params mv $out_dir $data_dir rm -r $work_dir/* exit 0 9.2 Summarizing Our Results Now that we have our OTU table, in biom format, we can summarize our results using some QIIME commands. First, let’s look at the mapping file to see what sample features we’d like to focus on: View(MAP) Now, we’ll write a submission script to get analyses output. #!/bin/bash #$ -S /bin/bash #$ -cwd #$ -j y #$ -M user_name@email.edu #$ -l h_rt=01:00:00 #$ -P namePrj #$ -l mem_free=12G #$ -l h_vmem=16G #$ -q all.q . /etc/profile.d/modules.sh module load shared module load proteus module load sge/univa module load gcc/4.8.1 module load qiime/gcc/64/1.9.1 export ref_seqs=/mnt/HA/opt/qiime/gcc/64/1.9.1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/rep_set/97_otus.fasta export ref_tax=/mnt/HA/opt/qiime/gcc/64/1.9.1/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt data_dir=/home/user_name/genStats/moving_pictures work_dir=/scratch/user_name/moving_pictures mkdir -p $work_dir cp $data_dir/fastq_out/picked_otus/97_otus.tree $work_dir cp $data_dir/fastq_out/picked_otus/otu_table.biom $work_dir cp $data_dir/map.tsv $work_dir otus=$work_dir/otu_table.biom tree=$work_dir/97_otus.tree map=$work_dir/map.tsv analysis_dir=$work_dir/analysis mkdir -p $analysis_dir biom summarize-table -i $otus &gt; $analysis_dir/sampling_depth.dat core_diversity_analyses.py -o $analysis_dir/diversity -i $otus -m $map -t $tree -c SampleType -e 1000 --recover_from_failure mv $analysis_dir $data_dir rm -r $work_dir/* exit 0 Or we can calculate the metrics more directly: #!/bin/bash #$ -S /bin/bash #$ -cwd #$ -j y #$ -M user_name@email.edu #$ -l h_rt=01:00:00 #$ -P namePrj #$ -l mem_free=12G #$ -l h_vmem=16G #$ -q all.q . /etc/profile.d/modules.sh module load shared module load proteus module load sge/univa module load gcc/4.8.1 module load qiime/gcc/64/1.9.1 data_dir=/home/user_name/genStats/moving_pictures work_dir=/scratch/user_name/moving_pictures/ mkdir -p $work_dir cp $data_dir/fastq_out/picked_otus/97_otus.tree $work_dir cp $data_dir/fastq_out/picked_otus/otu_table.biom $work_dir cp $data_dir/map.tsv $work_dir otus=$work_dir/otu_table.biom tree=$work_dir/97_otus.tree map=$work_dir/map.tsv met_dir=$work_dir/metrics alpha_diversity.py -i $otus -t $tree -o $met_dir/alpha.txt beta_diversity.py -i $otus -o $met_dir/beta -t $tree -m unweighted_unifrac beta_diversity.py -i $otus -o $met_dir/beta -t $tree -m weighted_unifrac principal_coordinates.py -i $met_dir/beta -o $met_dir/pcoa make_2d_plots.py -i $met_dir/pcoa/pcoa_unweighted_unifrac_otu_table.txt -m $map -b SampleType -o $met_dir/pcoa/figures make_2d_plots.py -i $met_dir/pcoa/pcoa_weighted_unifrac_otu_table.txt -m $map -b SampleType -o $met_dir/pcoa/figures transform_coordinate_matrices.py -i $met_dir/pcoa/pcoa_unweighted_unifrac_otu_table.txt,$met_dir/pcoa/pcoa_weighted_unifrac_otu_table.txt -r 999 -o $met_dir/procrustes/ make_emperor.py -c -i $met_dir/procrustes -o $met_dir/procrustes/figures -m $map --custom_axes DaysSinceExperimentStart mv $met_dir $data_dir exit 0 9.3 Loading QIIME Results into Phyloseq We have a ton of output from QIIME, but we could have just as easily loaded the biom table into R, specifically phyloseq. We can do this like so: library(phyloseq) ## ## Attaching package: &#39;phyloseq&#39; ## The following object is masked from &#39;package:IRanges&#39;: ## ## distance biom_path &lt;- file.path(data_dir,&#39;fastq_out_2&#39;,&#39;picked_otus_sortmerna&#39;,&#39;otu_table.biom&#39;) tree_path &lt;- file.path(data_dir,&#39;fastq_out_2&#39;,&#39;picked_otus_sortmerna&#39;,&#39;97_otus.tree&#39;) BIOM &lt;- import_biom(biom_path) ## Warning in strsplit(conditionMessage(e), &quot;\\n&quot;): input string 1 is invalid in ## this locale META &lt;- as.data.frame(MAP) rownames(META) &lt;- META$`#SampleID` PS &lt;- merge_phyloseq(BIOM,sample_data(META)) PS ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 3409 taxa and 34 samples ] ## sample_data() Sample Data: [ 34 samples by 11 sample variables ] ## tax_table() Taxonomy Table: [ 3409 taxa by 7 taxonomic ranks ] And then perform similar analyses: plot_ordination(PS,ordinate(PS,method=&#39;MDS&#39;,distance=&#39;bray&#39;),type=&#39;samples&#39;,color=&#39;SampleType&#39;) "],
["multcomp.html", "Chapter 10 Multiple Comparisons 10.1 Hypothesis Testing and Power 10.2 Multiple Comparisons", " Chapter 10 Multiple Comparisons library(tidyverse) library(shiny) 10.1 Hypothesis Testing and Power When we perform a statistical hypothesis test, we measure a given effect, assess its probability of occuring given a null hypothesis \\(H_0\\) (i.e., the hypothesis that there is no relationship between our test groupings), and reject said null if the probability falls below a particular threshold. We “accept” \\(H_1\\) and consider this test to be statistically significant. We aren’t always right, however; hence, hypothesis testing is prone to false positives (type I errors) and false negatives (type II errors): Reject \\(H_0\\) Reject \\(H_1\\) False \\(H_0\\) True Positives (TP) False Negatives (FN) True \\(H_0\\) False Positives (FP) True Negatives (TN) You’ll often see the false positive error probability refered to as \\(\\alpha\\) (where \\(\\alpha=\\frac{\\text{FP}}{\\text{FP}+\\text{TN}}\\)), the false negative error probability referred to as \\(\\beta\\) (where \\(\\beta=\\frac{\\text{FN}}{\\text{FN}+\\text{TP}}\\)), and the probabilty of a false negative not occuring as \\(1-\\beta\\). Tests that have low \\(beta\\) are said to have high power, where \\(\\text{Power}=1-\\beta\\). One can interpret a powerful test as a test that is very capable of detecting when the null hypothesis is false – that is, \\(P(\\text{reject } H_0|\\text{true } H_1)\\). If we were comparing two groups, and our power was about 0.9, then we’d have about a 90% chance of detecting a difference between the groups and a 10% chance of missing it. 10.2 Multiple Comparisons Whether you continue to work with large datasets or transition into the types of data problems one often sees stemming from benchwork, you’ll come across the multiple comparisons problem. Let’s focus on a situation common when working with DNA or RNA sequencing counts. Say we were given some \\(N \\times M\\) normalized gene table, where there are \\(N=100\\) samples and \\(M=10000\\) genes. Of these samples, 50 will belong to group 1 and the other 50 to group 2: N_grp1 &lt;- 50 N_grp2 &lt;- 50 N &lt;- N_grp1+N_grp2 M &lt;- 10000 set.seed(453) TABLE_grp1 &lt;- matrix(rnorm(N_grp1*M,0,1),N_grp1,M,dimnames=list(paste0(&#39;sample&#39;,1:N_grp1),paste0(&#39;gene&#39;,1:M))) TABLE_grp2 &lt;- matrix(rnorm(N_grp2*M,0,1),N_grp2,M,dimnames=list(paste0(&#39;sample&#39;,1:N_grp2),paste0(&#39;gene&#39;,1:M))) TABLE &lt;- rbind(TABLE_grp1,TABLE_grp2) Now, let’s aim to identify important genes that are different between the two groups. We can do this by performing a t test between the groups for each gene sequentially. I’m going to hardcode the t test there in case you are unfamiliar. tstats &lt;- vector(mode=&#39;double&#39;,length=M) for (gene in seq_len(M)){ grp1 &lt;- TABLE[1:50,gene] grp2 &lt;- TABLE[51:100,gene] mu1 &lt;- mean(grp1) mu2 &lt;- mean(grp2) v1 &lt;- sum((grp1 - mean(grp1))^2) v2 &lt;- sum((grp2 - mean(grp2))^2) s2 &lt;- (1/(N-2))*(v1 + v2) se &lt;- sqrt(s2*(1/N_grp1 + 1/N_grp2)) tstats[gene] &lt;- (mu1-mu2)/se } And now we can plot it. qplot(tstats,geom=&#39;histogram&#39;,color=I(&#39;black&#39;),bins=50) Clearly the t statistics are normally distributed. The typical approach with t tests is to perform a hypothesis test at the 5% significance level, such that we reject any null hypothesis with t statistics more than 2 standard deviations from the mode in either direction of this distribution. We use 2 standard deviations because qnorm(0.975) is 1.96 and qnorm(0.025) is -1.96: qplot(tstats,geom=&#39;histogram&#39;,color=I(&#39;black&#39;),bins=50) + geom_vline(xintercept=qnorm(0.025),linetype=2,color=&#39;red&#39;) + geom_vline(xintercept=qnorm(0.975),linetype=2,color=&#39;red&#39;) It should be apparent that there are quite a few significant genes. If you were paying attention to how our groups were generated, this should be surprising to you. We generated both groups from the same normal distribution. Nevertheless, we still have a ton of significant genes: sum(tstats&lt;qnorm(0.025) | tstats&gt;qnorm(0.975)) ## [1] 530 So over 500 genes were significant despite the fact that we know there is no underlying effect. Well, this is actually quite consistent with the defiinition of a p value. Given a typical hypothesis test, we reject the null if the probability of the observation is less than the probability of the observation occuring assuming the null hypothesis is true. In other words, we reject the claim that there is no statistical effect if our p value is less than (in absolute value) \\(\\alpha\\), the probability of a statistical effect due to random sampling variation. Therefore, because there are 10,000 genes and hence we perform 10,000 t tests, givin an \\(\\alpha\\) of 0.05, we should expect about \\(0.05 \\times 10000 = 500\\) significant t statistics due to random sampling variation alone, which is the case here. So that’s all well and good. We now know that if we set \\(\\alpha\\) at a particular level, given our null and sample size, we should expect some false positives. But is that enough information? Let’s rerun the above model, but now with an actual difference between groups: N_grp1 &lt;- 50 N_grp2 &lt;- 50 N &lt;- N_grp1+N_grp2 M &lt;- 10000 set.seed(65) TABLE_grp1 &lt;- matrix(rnorm(N_grp1*M,.25,2),N_grp1,M,dimnames=list(paste0(&#39;sample&#39;,1:N_grp1),paste0(&#39;gene&#39;,1:M))) TABLE_grp2 &lt;- matrix(rnorm(N_grp2*M,-.25,2),N_grp2,M,dimnames=list(paste0(&#39;sample&#39;,1:N_grp2),paste0(&#39;gene&#39;,1:M))) TABLE &lt;- rbind(TABLE_grp1,TABLE_grp2) tstats &lt;- vector(mode=&#39;double&#39;,length=M) tstats2 &lt;- vector(mode=&#39;double&#39;,length=M) for (gene in seq_len(M)){ grp1 &lt;- TABLE[1:50,gene] grp2 &lt;- TABLE[51:100,gene] mu1 &lt;- mean(grp1) mu2 &lt;- mean(grp2) v1 &lt;- sum((grp1 - mean(grp1))^2) v2 &lt;- sum((grp2 - mean(grp2))^2) s2 &lt;- (1/(N-2))*(v1 + v2) se &lt;- sqrt(s2*(1/N_grp1 + 1/N_grp2)) tstats[gene] &lt;- (mu1-mu2)/se } sum(tstats&lt;qnorm(0.025) | tstats&gt;qnorm(0.975)) ## [1] 2386 Now the two groups have different normlized gene values, which leads to over 2000 significant genes. We know from before that about 500 should be due to random sampling variation, but how do we know which ones? This brings us to methods to adjust our results such that we can interpret them more easily. 10.2.1 Bonferroni We’ll start with Bonferroni correction. First thing we’ll do is calculate the p values for all of those t statistics: pvals &lt;- 2*(1-pt(tstats,N-2)) Let’s continue to assume \\(\\alpha=0.05\\). Now, we should have about the same number of significant genes as mentioned before: alpha &lt;- .05 sum(pvals &lt; alpha) ## [1] 2309 Bonferroni corrects for multiple comparisons by controlling for the family-wise error rate (FWER) – the probability of making at least one false positive – by simply dividing \\(\\alpha\\) by the number of tests performed, which gives a new, adjusted \\(\\alpha\\) to use as a significance threshold: alpha_bonf &lt;- alpha/M sum(pvals &lt; alpha_bonf) ## [1] 5 This gives us only 2 significant genes. We should expect more given the way we set the group means. The issue with using Bonferonni here is that it’s quite conservative, and tends to really decrease statistical power when there are a lot of tests, as in this case. Consequently, we are likely to see a ton of false negatives. In the cases where we perform only a few tests (say 10 or less), Bonferonni is quick and easy, and probably appropriate, but definitely not here. 10.2.2 False Discovery Rate Instead of controlling for the family-wise error rate, we can control for the false discovery rate (FDR), the expected proportion of false positives. Unlike the FWER, which provides stringent control over false positives, the FDR is far more lenient, thereby increasing power at the sacrifice for more false positives. If we rewrite our table from above as follows: Reject \\(H_0\\) Reject \\(H_1\\) Total False \\(H_0\\) True Positives (TP) False Negatives (FN) Actual Positives (AP) True \\(H_0\\) False Positives (FP) True Negatives (TN) Actual Negatives (AN) Total Called Positives (CP) Called Negatives (CN) Total (TOTAL) we can rewrite the FDR as \\[ \\text{FDR}=\\mathop{\\mathbb{E}}\\left[\\frac{\\text{FP}}{\\text{CP}}\\right] \\]. Assuming all of our tests are independent, and given a significance threshold \\(\\alpha\\), the FDR has the following property \\[ \\text{FDR}=\\mathop{\\mathbb{E}}\\left[\\frac{\\text{FP}}{\\text{CP}}\\right] \\le \\alpha\\frac{\\text{AN}}{\\text{TOTAL}} \\le \\alpha \\] Below is a figure to show what FDR correction will ultimately do to our dataset we generated above. alpha &lt;- .05 df &lt;- data.frame(gene=1:length(pvals), p=sort(pvals), fit=(1:length(pvals))*alpha/M) ggplot(df,aes(gene,p)) + geom_vline(xintercept=sum(pvals &lt; alpha),size=.5) + geom_vline(xintercept=alpha*M,size=.5) + geom_line(aes(gene,fit),color=&#39;red&#39;,linetype=3,size=.5) + geom_point(size=.3) + scale_x_log10() + scale_y_log10() + labs(x=&#39;Genes&#39;,y=&#39;Log10 p Value&#39;) + annotate(&#39;label&#39;,sum(pvals &lt; alpha),1e-06,label=&#39;alpha&#39;,parse=TRUE) + annotate(&#39;label&#39;,alpha*M,1e-06,label=&#39;alpha*M&#39;,parse=TRUE) The genes are ordered in terms of increasing p value, with everything on log scale. The vertical lines shows the number of genes with \\(p&lt;\\alpha\\) and the number of genes we should expect based on random sampling alone, \\(\\alpha*M\\), where M is our total number of genes. The red diagonal line is the gene index multiplied by \\(\\alpha/\\text{TOTAL}\\). This gives us the FDR threshold. Any genes that fall below this line are considered significant after FDR adjustment. To perform this adjustment, we simply choose an alpha, then rank our M genes in terms of increasing p value and identify the largest p value for genes where \\[ p_\\text{gene} &lt; \\alpha \\frac{\\text{rank}}{M} \\] This becomes our new significance threshold: alpha_fdr &lt;- max(df$p[df$p &lt; alpha*df$gene/M]) sum(pvals&lt;alpha_fdr) ## [1] 222 This results in far more significant genes than the Bonferroni methods, but still less than half of what we’d expect from random sampling variation. ggplot(df,aes(gene,p)) + geom_vline(xintercept=sum(pvals &lt; alpha),size=.5) + geom_vline(xintercept=alpha*M,size=.5) + geom_vline(xintercept=sum(pvals &lt; alpha_bonf),size=.5,color=&#39;blue&#39;) + geom_vline(xintercept=sum(pvals &lt; alpha_fdr),size=.5,color=&#39;darkgreen&#39;) + geom_line(aes(gene,fit),color=&#39;red&#39;,linetype=3,size=.5) + geom_point(size=.3) + scale_x_log10() + scale_y_log10() + labs(x=&#39;Genes&#39;,y=&#39;Log10 p Value&#39;) + annotate(&#39;label&#39;,sum(pvals &lt; alpha),1e-06,label=&#39;alpha&#39;,parse=TRUE) + annotate(&#39;label&#39;,alpha*M,1e-06,label=&#39;alpha*M&#39;,parse=TRUE) + annotate(&#39;label&#39;,sum(pvals &lt; alpha_bonf),1e-06,label=&#39;alpha[bonf]&#39;,parse=TRUE) + annotate(&#39;label&#39;,sum(pvals &lt; alpha_fdr),1e-06,label=&#39;alpha[fdr]&#39;,parse=TRUE) "],
["lasso.html", "Chapter 11 Lasso 11.1 Big Data and Feature Selection 11.2 Lasso Regression 11.3 Multivariate Lasso Regression 11.4 Parallel Coordinate Descent 11.5 Simulations 11.6 Conclusion 11.7 Code: Lasso 11.8 Code: Lasso via cyclic gradient descent 11.9 Code: Parallel lasso 11.10 References", " Chapter 11 Lasso 11.1 Big Data and Feature Selection A common hurdle found when dealing with big data is determining which covariates are relevant prior to model fitting. One could fit a full model, but when dimensionality of the potential features reaches the order of thousands, both the demand on computational resources and time often makes such a strategy impractical. Choosing relevant features ahead of time allows the user to simultaneously explore relevant models and features, while avoiding the unnecessary computational cost of iteratively performing a step-wise model fitting procedure (either forwards or backwards) to arrive at a good fit. Feature selection methods often employ information theory measures to determine the most explanatory set of features for a particular outcome (e.g., joint mutual information and minimum redundancy maximum relevance). Another strategy is to perform L1 regularized regression (i.e., lasso), where, like ordinary regression, the least squares loss function is minimized, but constraint is also applied to the absolute sum of regression coefficients. Using the L1 norm has important consequences in feature selection. If one uses the L2 norm (i.e., ridge regression), every feature will be conserved in the final solution; hence, if one starts with 100 covariates, then the final regression equation will too have 100 non-zero covariates. The L1 norm, on the other hand, shrinks less predictive covariates to zero as a function of the weighting parameter \\(\\lambda\\). The final regression equation is therefore a sparse solution in the case of the L1 norm. 11.2 Lasso Regression Lasso can be formulated as follows: \\[\\underset{\\beta}{\\mathrm{min}} \\sum_{i=1}^n (y_i - \\beta x_{i})^2 s.t. |\\beta| \\le s\\]. The constraint on \\(\\beta\\) can be interpreted as for any value s, there is a corresponding value \\(\\lambda\\) that places an upper bound on the absolute sum of the coefficients. A large s value – and hence small \\(\\lambda\\) value – results in a sum that is large, implying many coefficients were not set to zero. Small values of s yield a small sum and hence a sparse solution. Solving the lasso problem is more difficult than simple linear regression. The first half of the equation above is the same equation seen in OLS regression. Solving this is easy: simply take the derivative, set to zero, and solve for \\(\\beta\\). We can see this first hand in gradient descent implementations of regression. The right half of the equation is where things get complicated; because the derivative of \\(|\\beta|\\) is not defined at \\(\\beta=0\\), a subdifferential approach is needed, resulting in the following sub thresholding function: \\[ \\hat \\beta_j = \\left\\{\\def\\arraystretch{1.2}% \\begin{array}{@{}c@{\\quad}l@{}} y_j - \\lambda/2 &amp; \\text{if $y_j &gt; \\lambda/2$}\\\\ y_j + \\lambda/2 &amp; \\text{if $y_j &lt; - \\lambda/2$}\\\\ 0 &amp; \\text{if $|y_j| \\le \\lambda/2$}\\\\ \\end{array}\\right. \\] We can see the effect of varying \\(\\lambda\\) values in the figure below: The value of \\(y_j\\) is represented by the black lines, whereas the subthresholding transformation is represented by the blue lines. For values outside of the \\(|y_j|\\) interval, the function essentially drives \\(y_j\\) closer to 0. Once it’s within that interval, \\(y_j\\) is set to zero. The size of this interval is determined by \\(\\lambda\\), with larger values producing a larger interval, and consequently, a more sparse solution. 11.3 Multivariate Lasso Regression In the multivariate case, the lasso problem becomes \\[\\underset{\\beta}{\\mathrm{min}} \\frac{1}{2} ||y-X\\beta||_2^2 + \\lambda ||\\beta||_1\\], which can only be solved explicitly if \\(X^TX=I\\), a case that isn’t met when \\(p&gt;n\\). Coordinate descent provides a solution for the multivariate regression problem. Here, the subthresholding function becomes: \\[ \\hat \\beta_j = \\left\\{\\def\\arraystretch{1.2}% \\begin{array}{@{}c@{\\quad}l@{}} c_j - p\\lambda/a_j &amp; \\text{if $c_j &gt; p\\lambda$}\\\\ c_j + p\\lambda/a_j &amp; \\text{if $c_j &lt; - p\\lambda$}\\\\ 0 &amp; \\text{if $|c_j| \\le p\\lambda$}\\\\ \\end{array}\\right. \\] where p is the dimensionality of the feature vector. Because it’s easier to optimize each \\(jth\\) coefficient individually, instead of solving for the entire feature vector simultaneously, we solve for the \\(jth\\) coefficient while holding all others fixed. This results in \\(c_j = X_{\\cdot,j}^T(y - X_{\\cdot,-j}\\beta_{-j})\\) and \\(a_j = \\sum_{i=1}^n x_{i,-j}\\). This is often performed sequentially, i.e., for each iteration each \\(\\beta\\) is updated, one-by-one. This is called the shooting algorithm. A slight adjustment allows for easy parallel implementation. 11.4 Parallel Coordinate Descent Unlike before where each \\(\\beta_j\\) is updated during each iteration until convergence, an alternative strategy is to randomly sample index \\(j\\) to update during each iteration. While this method would require more iterations, each iteration will be faster, and it will still ultimately converge. Because the \\(jth\\) index is now being sampled, an obvious extension is to simply distribute a randomly sampled index to each of m processors on a cluster. Each processor can than update its respective \\(\\beta_j\\) independently, resulting in m simultaneous updates per iteration. This requires a rather simple implementation via MPI. Set vector B of length p to 0. Given m processors, m indexes are drawn from a uniform distribution ranging 1 to m. This index vector is then sent to each processor via MPI_Scatter. Each processor independently updates \\(a_j\\), \\(c_j\\), and ultimately \\(\\beta_j\\). The updated \\(\\beta_j\\)s are then returned to the head node using MPI_gather, where the original vector B is then updated and then distributed across all processes with MPI_bcast for future iterations. This is repeated until convergence – i.e., when two sequential iterations result in a change of the objective function less than a predetermined tolerance value. The objective function function is defined as \\[\\frac{1}{2m} ||y-X\\beta||_2^2 + \\lambda \\sum_{i=1}^n |\\beta|\\]. 11.5 Simulations All matrices were generated as follows: X, a \\(m \\times n\\) matrix, and k \\(\\beta\\) values were sampled from a normal distribution (0,1). The k \\(\\beta\\) values were set to k randomly sampled indexes sampled from a uniform distribution (1,n) in a \\(m \\times 1\\) vector . All other values were set to 0. Finally, y was calculated via \\(y = X^T b\\). 11.5.1 \\(50 \\times 1000\\) Matrix | 5 Target Coefficients Before any analysis of the results was performed, there seemed to be numerical issues when performing the algorithm on small \\(\\lambda\\) values (\\(&lt;.05\\)). The objective and all \\(\\beta\\) coefficients would increase exponentially after every update. Only with larger \\(\\lambda\\) values would the algorithm converge (and note it would always converge to the correct values). It turned out this was unrelated to the code; there were no errors. Instead, it was a function of the number of cores set. All initial runs were performed with 64 cores, resulting in the numerical issue for small \\(\\lambda\\) values. Note that this is neither an MPI problem nor a problem inherent to the cluster. The issue lies in selecting more simultaneous updates than there is data (rows in matrix X), which, in this simulation, there are 50. See the figure below: If we increase the sample size to 100, then the convergence problem is gone: And the ability to achieve convergence as a function of processors is also dictated by \\(\\lambda\\): Here are the lasso traces for 10 independent samples for the above parameter settings The text on the right hand side represents the indexes of the true coefficients used to generate the system for each sample. Their height represents their true value. We can easily see which traces correspond to which index, and moreover, we can see the \\(\\lambda\\) value in which the traces achieve the correct value. Every sample managed to both identify and estimate the correct index and its coefficient at some point throughout the trace. The figures seem to suggest that \\(\\lambda=-.005\\) did the best job at estimating the true values. Coefficients with larger values were more robust to smaller \\(\\lambda\\) values. For example, in sample 7, coefficients 154 and 895 were nonzero for \\(\\lambda\\) values well over 1. Coefficient 399 was driven to zero around \\(\\lambda=.5\\), whereas the remaining two coefficients were set to 0 much sooner. Here is the error at each \\(\\lambda\\) averaged across samples: The error term here is measured as the following: \\[\\mathrm{Error}=\\frac{||\\hat x - x||_2}{||x||_2}\\]. This figure confirms \\(\\lambda=.005\\) performing the best. It should be noted that there was an iteration cap, so the very small \\(\\lambda\\) values likely just failed to reach a reasonable approximation. While they clearly were converging and hence approaching a reasonable solution, it simply would have taken too long and these values were the result of 20,000 iterations using 48 cores. The error began to rise somewhat around \\(\\lambda=.779\\) The hump in this region and the subsequent decline can be attributed to the variability regarding when certain coefficients are driven to 0. Samples 1 and 2 have a few relatively robust coefficients that help decrease the error, whereas the coefficients in sample 4 are probably key contributors in the hump around \\(\\lambda=.779\\). 11.5.2 \\(50 \\times 10000\\) Matrix | 5 Target Coefficients Now we are dealing with 10 times more unwanted coefficients, and the figures make it quite obvious that lasso is having more trouble than the \\(50 \\times 1000\\) case. Sample 10 managed to capture only 1 coefficient for a prolonged stretch, and it also had quite a few false positives for moderate \\(\\lambda\\) values. Sample 3 also had quite a few false positives, but farther along than sample 10. Samples 7 and 8 detected only 2 of the 5 coefficients. Like the previous simulation, larger true coefficients were more robust at being detected across \\(\\lambda\\) values, exemplified quite well by sample 2. Unlike the last simulation, \\(\\lambda=.005\\) performed poorly. The best value seemed to be around \\(\\lambda=.058\\), although each value ranging from .058 to .371 did quite well. Still, even for .058, the error was larger (approximately .300) than .005 (.009) for the last simulation 11.5.3 \\(200 \\times 10000\\) Matrix | 5 Target Coefficients By increasing the number of data points from 50 to 200, there seems to be a return to the behavior we saw in the \\(50 \\times 1000\\) simulation. Notwithstanding sample 6, all of the samples managed to estimate at least 4 of the 5 coefficients. Again, if the coefficient was truly large, then it behaved robustly across all \\(\\lambda\\) values. The error too seems similar to the \\(50 \\times 1000\\) simulation. The minimum ended up being at \\(\\lambda=0.007\\), analogous to the minimum of .005 we saw in the smaller system. The error managed to stay low even for a slightly larger \\(\\lambda\\) (.01). Like all of the simulations so far, there is a hump at the moderate \\(\\lambda\\) values. Based on the last 3 simulations, there seems to be a trade-off between the amount of data (i.e., the number of rows in \\(X\\)) and the number of coefficients (i.e., the columns in \\(X\\)). More coefficients gives lasso problems, but given more data, then lasso returns to form. Let’s see how lasso does at estimating more coefficients. 11.5.4 \\(50 \\times 10000\\) Matrix | 15 Target Coefficients Now lasso seems to be struggling to find the true coefficients. Other than sample 4, which managed to only capture about 6 of the 15 coefficients, all of the samples performed poorly. Other than a few large coefficients, most traces were lost in false positives. Also, unlike the systems with only 5 true coefficients, this simulation resulted in many incorrect traces of similar magnitude to the true coefficients, represented by the black lines. It’s surprising that lasso even failed to capture the larger coefficients (see the labels far from the x-axis). The fact that there are quite a few coefficients not much larger than 0 in terms of magnitude (recall they are sampled from a normal distribution (0,1)) gives lasso problems setting any potentially non-zero coefficient to zero. This may be remedied by a smaller tolerance, but most likely, it won’t make a difference. For example, with \\(\\lambda=2.\\) (sample 4), the tolerance level was reached after only 1999 iterations, managing only to capture 4 coefficients, all of which are far smaller in magnitude than their true counterparts. A smaller \\(\\lambda\\) of 0.01 (sample 10) reached the iteration cap of 20,000, but ended up with 317 coefficients, far more than the target 15. A more moderate value, \\(\\lambda=1.05\\) (sample 1), broke after 5499 iterations, ending with 21 coefficients, which is closer to 15, but nevertheless failed to capture any of the true estimates of the larger coefficients since no trace reaches the required height. Note the gap in the plot was due to the aforementioned convergence/core issue; hence, those \\(\\lambda\\) values were omitted. The error here essentially reaches a plateau for the majority of the \\(\\lambda\\) values. The error drops at the end, but this is not due to improved estimation, but instead a result of driving many of the incorrect estimates to zero. Considering this situation, if one has a dataset with many potentially informative coefficients, unless some yield significantly large estimates relative to the majority, then lasso will likely have problems. Since this simulation dealt with true coefficients near 0, many false positives resulted. A noisy dataset probably requires the use of larger \\(\\lambda\\) values and the hope that the some coefficients are significantly more informative than the others. 11.5.5 \\(50 \\times 10000\\) Matrix | 30 Target Coefficients And here’s an even noisier system, with 30 true coefficients. Only sample 8 was able to separate a coefficient from the crown. Driving \\(\\lambda\\) up further for this sample would likely isolate it. Sample 5 also had a large robust estimate, but it was a false positive. Clearly the conclusion reached in the last simulation can only be confirmed here. The trend here is similar to the system with 15 true values. If we increase \\(\\lambda\\) further, we would likely see the same drop in error we see before since we’d be driving the many incorrect estimates to zero. 11.5.6 Scaling the coeficients It seems plausible that as we increase the number of false coefficients, we decrease the chance of identifying true coefficients unless those coefficients are much different than the majority – i.e., more informative. Instead of increasing the sampling size this time, let’s increase the magnitude of the true coefficients by scaling them 10-fold. Using a very large \\(50 \\times 100000\\) matrix, we get the following: Even with 100,000 potential coefficients to work with, lasso managed to cleanly identify one of the five targets. Moreover, while there are quite a few false positives around \\(\\lambda=.211\\) (30), there are far less than 100,000, and 3 total true coefficients were still detectable. At \\(\\lambda=.525\\), 4 coefficients were returned, albeit only 1 being correct. Had we used larger \\(\\lambda\\) values, we likely would have ended with at most 1 true and 1 false coefficient. Note that the 5 starting coefficients had values of -4.79, 9.88, 3.12, 1.22, and 7.02. That robust coefficient was, to no one’s surprise, the coefficient with the largest value in magnitude of the 5, 9.88. 11.5.7 Comparison Here is a figure showing all of the error traces. The small system (\\(50 \\times 1000\\)) and the large system with more data (\\(200 \\times 10000\\)) performed the best assuming a small \\(\\lambda\\) was used. Interestingly, if only uses a larger \\(\\lambda\\), then that small system tends to perform the worst. Still, it’s quite clear that identifying informative features is a function of the amount of data, the number of features, the number of true coefficients, and their size. 11.6 Conclusion A parallelized implementation of the coordinate descent algorithm for lasso clearly works well for large datasets. The issue with cores and the number of rows is an interesting quirk, but nevertheless easily avoidable. Once that was accounted for, no numerical issues resulted, and estimates were consistent with the predetermined values designed for the simulations. y &lt;- Boston$medv # median housing value X &lt;- as.matrix(Boston[,-14]) # all other predictors y &lt;- scale(y,center=TRUE,scale=FALSE) X &lt;- scale(X,center = TRUE,scale = FALSE) B &lt;- rep(0,ncol(X)) lambda &lt;- 2 tol &lt;- 1e-8 m &lt;- length(y) iter &lt;- 1000 obj &lt;- numeric(iter + 1) B.list &lt;- lapply(1:(iter + 1), function(x) x) B.list[[1]] &lt;- B for (j in 1:iter) { k &lt;- sample(1:length(B),1) # 0 for B0 but centered so 0 # calculate Bj for for all j not in k cj &lt;- (X[,k] %*% (y - X[,-k] %*% B[-k])) # *2 aj &lt;- sum(X[,k]^2) # *2 # shrink Bj &lt;- 0 if (cj &lt; -lambda*m) Bj &lt;- (cj + lambda*m)/aj if (cj &gt; lambda*m) Bj &lt;- (cj - lambda*m)/aj B[k] &lt;- Bj B.list[[(j + 1)]] &lt;- B obj[j] &lt;- (1/2)*(1/m)*norm(y - X %*% B,&quot;F&quot;)^2 + lambda*sum(abs(B)) #if (sqrt(sum((B.list[[j]] - B.list[[j+1]])^2)) &lt; tol) break } g &lt;- (1/m)*t(X) %*% (y - X %*% B) if (any(abs(g[which(B == 0)]) &gt; lambda + tol)) print(&quot;No minimum&quot;) if (any(g[which(B != 0)] &gt; tol + lambda*sign(B[which(B != 0)]))) print(&quot;No minimum&quot;) ## [1] &quot;No minimum&quot; B ## [1] -0.021552320294 0.035523120761 0.000000000000 0.000000000000 ## [5] 0.000000000000 0.000000000000 0.043564050481 -0.067585897623 ## [9] 0.173365654135 -0.011672463466 -0.557109230058 0.007065846342 ## [13] -0.821549115759 11.7 Code: Lasso y &lt;- Boston$medv # median housing value X &lt;- as.matrix(Boston[,-14]) # all other predictors y &lt;- scale(y,center=TRUE,scale=FALSE) X &lt;- scale(X,center = TRUE,scale = FALSE) B &lt;- rep(0,ncol(X)) lambda &lt;- 2 tol &lt;- 1e-8 m &lt;- length(y) iter &lt;- 1000 obj &lt;- numeric(iter + 1) B.list &lt;- lapply(1:(iter + 1), function(x) x) B.list[[1]] &lt;- B for (j in 1:iter) { k &lt;- sample(1:length(B),1) # 0 for B0 but centered so 0 # calculate Bj for for all j not in k cj &lt;- (X[,k] %*% (y - X[,-k] %*% B[-k])) # *2 aj &lt;- sum(X[,k]^2) # *2 # shrink Bj &lt;- 0 if (cj &lt; -lambda*m) Bj &lt;- (cj + lambda*m)/aj if (cj &gt; lambda*m) Bj &lt;- (cj - lambda*m)/aj B[k] &lt;- Bj B.list[[(j + 1)]] &lt;- B obj[j] &lt;- (1/2)*(1/m)*norm(y - X %*% B,&quot;F&quot;)^2 + lambda*sum(abs(B)) #if (sqrt(sum((B.list[[j]] - B.list[[j+1]])^2)) &lt; tol) break } g &lt;- (1/m)*t(X) %*% (y - X %*% B) if (any(abs(g[which(B == 0)]) &gt; lambda + tol)) print(&quot;No min&quot;) if (any(g[which(B != 0)] &gt; tol + lambda*sign(B[which(B != 0)]))) print(&quot;No min&quot;) ## [1] &quot;No min&quot; B ## [1] -0.02157812903 0.03552762001 0.00000000000 0.00000000000 0.00000000000 ## [6] 0.00000000000 0.04356367353 -0.06769672154 0.17352502255 -0.01168198944 ## [11] -0.55710594088 0.00706537907 -0.82151479590 11.8 Code: Lasso via cyclic gradient descent y &lt;- Boston$medv # median housing value X &lt;- as.matrix(Boston[,-14]) # all other predictors y &lt;- scale(y,center=TRUE,scale=FALSE) X &lt;- scale(X,center = TRUE,scale = FALSE) B &lt;- rep(0,ncol(X)) lambda &lt;- 2 tol &lt;- 1e-8 m &lt;- length(y) iter &lt;- 500 obj &lt;- numeric(iter + 1) B.list &lt;- lapply(1:(iter + 1), function(x) x) B.list[[1]] &lt;- B # cyclic gradient descent for (j in 1:iter) { k &lt;- sample(1:length(B),2) kk &lt;- k[1] k &lt;- k[2] # 0 for B0 but centered so 0 # calculate Bj for for all j not in k cj &lt;- (X[,k] %*% (y - X[,-k] %*% B[-k])) # *2 aj &lt;- sum(X[,k]^2) # *2 Bj &lt;- 0 if (cj &lt; -lambda*m) Bj &lt;- (cj + lambda*m)/aj if (cj &gt; lambda*m) Bj &lt;- (cj - lambda*m)/aj B[k] &lt;- Bj cj &lt;- (X[,kk] %*% (y - X[,-kk] %*% B[-kk])) # *2 aj &lt;- sum(X[,kk]^2) # *2 Bj &lt;- 0 if (cj &lt; -lambda*m) Bj &lt;- (cj + lambda*m)/aj if (cj &gt; lambda*m) Bj &lt;- (cj - lambda*m)/aj B[kk] &lt;- Bj #B.list[[(j + 1)]] &lt;- B #obj[j] &lt;- (1/2)*(1/m)*norm(y - X %*% B,&quot;F&quot;)^2 + lambda*sum(abs(B)) #if (sqrt(sum((B.list[[j]] - B.list[[j+1]])^2)) &lt; tol) break } g &lt;- (1/m)*t(X) %*% (y - X %*% B) if (any(abs(g[which(B == 0)]) &gt; lambda + tol)) print(&quot;No min&quot;) if (any(g[which(B != 0)] &gt; tol + lambda*sign(B[which(B != 0)]))) print(&quot;No min&quot;) ## [1] &quot;No min&quot; B ## [1] -0.021551619080 0.035518538258 0.000000000000 0.000000000000 ## [5] 0.000000000000 0.000000000000 0.043562505958 -0.067622063355 ## [9] 0.173361916541 -0.011674045311 -0.557132361153 0.007066159619 ## [13] -0.821537236860 11.9 Code: Parallel lasso #include &lt;petscsys.h&gt; #include &lt;petscis.h&gt; #include &lt;petscvec.h&gt; #include &lt;petscmat.h&gt; #include &lt;petscmath.h&gt; #include &lt;stdio.h&gt; float coord_descent(PetscScalar *y_array, float *beta_array, int rend, int cend, Mat matrix, int idx, float lambda){ int col,row,ncols; const PetscInt *cols; const PetscScalar *vals; float cj = 0, aj = 0,rsum = 0; for (row=0; row&lt;rend; row++){ MatGetRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals); for (col=0; col&lt;cend; col++){ if (col != idx){ rsum += vals[col]*beta_array[col]; }else{ aj += vals[idx]*vals[idx]; } } cj += vals[idx]*(y_array[row] - rsum); rsum = 0; MatRestoreRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals); } float Bj = 0.0; if (cj &lt; -lambda*rend) Bj = (cj + lambda*rend)/aj; if (cj &gt; lambda*rend) Bj = (cj - lambda*rend)/aj; return Bj; } float objective(PetscScalar *y_array, float *beta_array, int rend, int cend, Mat matrix, float lambda){ int col,row,ncols; const PetscInt *cols; const PetscScalar *vals; float cj = 0, Bj = 0,rsum = 0; for (row=0; row&lt;rend; row++){ MatGetRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals); for (col=0; col&lt;cend; col++){ rsum += vals[col]*beta_array[col]; } cj += (y_array[row] - rsum) * (y_array[row] - rsum); rsum = 0; MatRestoreRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals); } for (col=0;col&lt;cend;col++){ Bj += PetscAbsReal(beta_array[col]); } return .5 * (1/(float)rend) * PetscSqrtReal(cj) * PetscSqrtReal(cj) + lambda * Bj; } int main(int argc,char **argv){ Mat X; Vec y; PetscInt i,j,k,it,rstart,rend,row,cstart,cend,col,N,save_step=100,iter=10000; PetscScalar rcounter,ind,tol=1e-6,L=.15,obj_last=1e10; PetscRandom rng; PetscViewer viewer; PetscScalar *y_array; PetscInt rank,size; PetscInitialize(&amp;argc,&amp;argv,NULL,NULL); MPI_Comm_rank(PETSC_COMM_WORLD,&amp;rank); MPI_Comm_size(PETSC_COMM_WORLD,&amp;size); int sendind[size]; int getind[size]; PetscRandomCreate(PETSC_COMM_WORLD, &amp;rng); #if defined(PETSC_HAVE_DRAND48) PetscRandomSetType(rng, PETSCRAND48); #elif defined(PETSC_HAVE_RAND) PetscRandomSetType(rng, PETSCRAND); #endif PetscRandomSetFromOptions(rng); PetscOptionsGetInt(NULL,&quot;-i&quot;,&amp;iter,NULL); PetscOptionsGetInt(NULL,&quot;-s&quot;,&amp;save_step,NULL); PetscOptionsGetReal(NULL,&quot;-l&quot;,&amp;L,NULL); PetscViewerBinaryOpen(PETSC_COMM_SELF,&quot;/home/sw424/NumComp/lasso/datafinal/lasso_50_10000_5_1/samp_1/X.bin&quot;,FILE_MODE_READ,&amp;viewer); //PetscViewerBinaryOpen(PETSC_COMM_SELF,&quot;data/X.bin&quot;,FILE_MODE_READ,&amp;viewer); MatCreate(PETSC_COMM_SELF,&amp;X); MatSetType(X,MATSEQDENSE); MatLoad(X,viewer); PetscViewerDestroy(&amp;viewer); MatGetOwnershipRange(X,&amp;rstart,&amp;rend); MatGetOwnershipRangeColumn(X,&amp;cstart,&amp;cend); PetscViewerBinaryOpen(PETSC_COMM_SELF,&quot;/home/sw424/NumComp/lasso/datafinal/lasso_50_10000_5_1/samp_1/y.bin&quot;,FILE_MODE_READ,&amp;viewer); //PetscViewerBinaryOpen(PETSC_COMM_SELF,&quot;data/y.bin&quot;,FILE_MODE_READ,&amp;viewer); VecCreate(PETSC_COMM_SELF,&amp;y); VecSetType(y,VECSEQ); // new //VecSetSizes(y,PETSC_DECIDE,rend); VecLoad(y,viewer); //VecView(y,PETSC_VIEWER_STDOUT_WORLD); PetscViewerDestroy(&amp;viewer); float B[cend]; memset(B,0,sizeof B); PetscPrintf(PETSC_COMM_WORLD,&quot;Lambda = %f\\n\\n&quot;,L); PetscPrintf(PETSC_COMM_WORLD,&quot;Iter\\tObj\\n&quot;); for (it=0;it&lt;iter;it++){ for (i=0;i&lt;size;i++){ PetscRandomSetInterval(rng,0,cend); PetscRandomGetValueReal(rng,&amp;ind); sendind[i] = PetscFloorReal(ind); //PetscPrintf(PETSC_COMM_WORLD,&quot;Iter %i | Rank %i | Rand %i: %i\\n&quot;,it,rank,i,sendind[i]); } MPI_Scatter(sendind,1,MPI_INT, getind,1,MPI_INT, 0,PETSC_COMM_WORLD); VecGetArray(y,&amp;y_array); float Bj = coord_descent(y_array,B,rend,cend,X,getind[0],L); double obj = objective(y_array,B,rend,cend,X,L); // maybe better precision //float obj = objective(y_array,B,rend,cend,X,L); VecRestoreArray(y,&amp;y_array); //PetscPrintf(PETSC_COMM_SELF,&quot;Iter %i | Rank: %i | B: %f\\n&quot;,it,rank,Bj); float *Bnew = (float *)malloc(sizeof(float)*size); MPI_Gather(&amp;Bj,1,MPI_FLOAT,Bnew,1,MPI_FLOAT,0,PETSC_COMM_WORLD); for (i=0;i&lt;size;i++){ B[sendind[i]] = Bnew[i]; //PetscPrintf(PETSC_COMM_WORLD,&quot;Iter %i | Rank %i | Rand %i: %i | B: %f\\n&quot;,it,rank,i,sendind[i],Bnew[i]); } MPI_Bcast(B,cend,MPI_FLOAT,0,PETSC_COMM_WORLD); if((it+1) % save_step == 0){ PetscPrintf(PETSC_COMM_WORLD,&quot;%i\\t%f\\t&quot;,it,obj); //for (i=0;i&lt;cend;i++){ // PetscPrintf(PETSC_COMM_WORLD,&quot;%f\\t&quot;,B[i]); //} PetscPrintf(PETSC_COMM_WORLD,&quot;\\n&quot;); //if(obj_last-obj &lt; tol){ if(PetscAbsReal(obj_last-obj) &lt; tol){ break; } obj_last = obj; } } PetscPrintf(PETSC_COMM_WORLD,&quot;\\n\\nBroke after %i iterations\\n&quot;,it); PetscPrintf(PETSC_COMM_WORLD,&quot;\\n\\nFinal Beta Coefficients:\\n&quot;); for (i=0;i&lt;cend;i++){ if (B[i] != 0){ PetscPrintf(PETSC_COMM_WORLD,&quot;%i: %f\\n&quot;,i,B[i]); } } PetscPrintf(PETSC_COMM_WORLD,&quot;\\n&quot;); PetscRandomDestroy(&amp;rng); MatDestroy(&amp;X); PetscFinalize(); return 0; } 11.10 References Parallel Coordinate Descent for L1-Regularized Loss Minimization. Bradley J. K., Kyrola, A., Bickson, D., and Guestrin, C. 2011. Proceedings of the 28th International Conference on Machine Learning. Machine Learning: A Probabilistic Perspective. Murphy, K. P., 2012. MIT Press, 1st ed. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie, T., Tibshirani, R., and Friedman, J. 2003. Springer. 3rd ed. "],
["tm.html", "Chapter 12 Topic Models 12.1 Variational Inference 12.2 LDA 12.3 Supervised LDA 12.4 The Correlated Topic Model 12.5 Dirichlet Distribution", " Chapter 12 Topic Models 12.1 Variational Inference 12.1.1 Evidence Lower Bound (ELBO) We first want to create a variational distribution, which is a distribution over all latent variables parameterized by variational parameters: \\(q(z_{1:m}|\\nu)\\). We want to choose \\(\\nu\\) that makes q as cloase as possible to our posterior \\(p\\). If \\(q=p\\), then we have typical expectation maximization. The whole put of variational inference, however, is to choose a \\(q\\) that is easier (or possible) to compute. One thing that we’re going to exploit is Jensen’s Inequality: We can apply this inequality to concave functions. If we have two points (say \\(v_1\\) and \\(v_2\\)), and we average the function \\(f\\) that is concave at those two points; it will be less than the function \\(f\\) applied to the average of those two points – that is, \\(f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(X)]\\). Let’s assume that \\(f(v)=\\log{v}\\). The weighted average between two points would therefore be \\(\\log{[\\alpha v_1 + (1-\\alpha)v_2]}\\), and the average of the function at those two points would be \\(\\alpha \\log{v_1} + (1-\\alpha)\\log{v_2}\\). This gives us the following inequality: \\(\\log{[\\alpha v_1 + (1-\\alpha)v_2]} \\ge \\alpha \\log{v_1} + (1-\\alpha)\\log{v_2}\\). We’re going to exploit the use of this inequality to calculate the evidence lower bound (ELBO) for our variational inference procedure. Given the log probability of our data \\(\\log{p(x)}\\), let’s also consider all possible latent variables \\(z\\). To do this, we’ll marginalize out \\(z\\): \\[ \\log{p(x)} = \\log{\\int_z p(x,z)} \\] To introduce our distribution \\(q\\), we’ll multiply by 1: \\[ \\begin{aligned} \\log{p(x)} &amp;= \\log{\\int_z p(x,z) \\frac{q(z)}{q(z)}}\\\\ &amp;= \\log{\\int_z \\frac{p(x,z)}{q(z)} q(z)}\\\\ &amp;= \\log{\\mathbb{E}_q [\\frac{p(x,z)}{q(z)}]} \\end{aligned} \\] Now we can apply Jensen’s Inequality. We just derived the function applied to the expectation, which was the left term of the inequality. Now, the right term is the expectation of function, giving us: \\[ \\begin{aligned} \\log{p(x)} = \\log{\\mathbb{E}_q [\\frac{p(x,z)}{q(z)}]} &amp;\\ge \\mathbb{E}_q [\\log{\\frac{p(x,z)}{q(z)}}]\\\\ &amp;= \\mathbb{E}_q [\\log p(x,z)] - \\mathbb{E}_q [\\log q(z)] \\end{aligned} \\] Turning the log of a quotient into a difference has a useful side effect: \\(\\mathbb{E}_q [\\log q(z)]\\) is simply the entropy of the variational distribution \\(q\\). We cannot optimize this part of the equation; however, we can maximize \\(\\mathbb{E}_q [\\log p(x,z)]\\). By doing so, we will drive the equation closer and closer to \\(\\log {p(x)}\\), and ideally as far as the entropy term will allow. This term we aim to maximize is the ELBO, and it will give us a tight lower bound of \\(\\log{p(x)}\\). 12.1.2 ELBO and KL Divergence Maximizing the ELBO is equivalent to minimizing the KL divergence. To see this, let’s rewrite out joint probability distribution of our data \\(x\\) and latent parameters \\(z\\) as \\[ p(z|x) = \\frac{p(x,x)}{p(x)}\\\\ \\] Now, let’s take the KL divergence between this distribution and the variational distribution \\(q\\): \\[ \\begin{aligned} \\text{KL}(q(z) || p(z|x)) &amp;= \\mathbb{E}_q [\\log{\\frac{q(z)}{p(z|x)}}]\\\\ &amp;= \\mathbb{E}_q [\\log{q(z)} - \\log{q(z)}]\\\\ &amp;= \\mathbb{E}_q [\\log{q(z)}] - \\mathbb{E}_q [\\log{p(z|x)}]\\\\ &amp;= \\mathbb{E}_q [\\log{q(z)}] - \\mathbb{E}_q [\\log{\\frac{p(x,z)}{p(x)}}]\\\\ &amp;= \\mathbb{E}_q [\\log{q(z)}] - \\mathbb{E}_q [\\log{p(x,z)} - \\log{p(x)}]\\\\ &amp;= \\mathbb{E}_q [\\log{q(z)}] - \\mathbb{E}_q [\\log{p(x,z)}] + \\log{p(x)} \\end{aligned} \\] When we optimize, the log probability of our data \\(\\log{p(x)}\\) will vanish because it’s a constant, so we can rewrite this as \\[ \\begin{aligned} \\text{KL}(q(z) || p(z|x)) &amp;= \\mathbb{E}_q [\\log{q(z)}] - \\mathbb{E}_q [\\log{p(x,z)}]\\\\ &amp;= -(\\mathbb{E}_q [\\log{p(x,z)}] - \\mathbb{E}_q [\\log{q(z)}]) \\end{aligned} \\] Thus, minimizing the KL divergence is the same as maximizing the ELBO. 12.1.3 Mean Field Method One way of writing our variational distribution is via the mean field method where we fully factorize our latent variables such that we assume that they are completely independent of one another. For our variational distribution \\(q\\) over latent parameters \\(z\\) we have \\[ q(z_1,\\dots,z_N) = \\prod_{i=1}^N q(z_i) \\] It should be obvious that, given this assumption, \\(p \\neq q\\) because in \\(p\\), the latent variables are dependent upon one another. This mean field method is a good starting point for deriving a variational distribution, but sometimes it doesn’t work, so other strategies to forming the variational distribution would be required. 12.2 LDA LDA follows the following generative process: The joint distribution over all latent variables \\(\\theta\\) and \\(z\\) and observed data \\(w\\) is then \\[ p(\\theta,z,w|\\alpha,\\beta) = \\prod_d p(\\theta_d|\\alpha) \\prod_n p(w_{d,n}|z_{d,n},\\beta) p(z_{d,n}|\\theta_d) \\] where \\[ \\begin{aligned} &amp;p(\\theta_d|\\alpha) = \\frac{\\Gamma (\\sum_i \\alpha_i)}{\\prod_i \\Gamma (\\alpha_i)} \\prod_i \\theta_{d,k}^{\\alpha_i -1} \\text{, (Dirichlet)}\\\\ &amp;p(z_{d,n}|\\theta_d) = \\prod_n \\prod_i \\theta_i^{1[z_n=i]} \\text{, (Multinomial)}\\\\ &amp;p(w_{d,n}|z_{d,n},\\beta) = \\prod_v \\prod_i \\beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]} \\text{, (Multinomial)} \\end{aligned} \\] Now, our variational distribution for LDA will be a mean field distribution where we assume complete independence between each \\(\\theta\\) and between all topic assignments \\(z\\), i.e., a fully factored form: \\[ q(\\theta,z|\\gamma,\\phi) = \\prod_d q(\\theta_d|\\gamma_d) \\prod_n q(z_{d,n}|\\phi_{d,n}) \\] where \\(\\gamma \\thicksim \\text{Dirichlet}\\) and \\(\\phi \\thicksim \\text{Multinomial}\\) are our variational parameters. \\(\\theta_d\\) will be a length \\(K\\) non-negative vector representing the distribution over topics \\(\\gamma_d\\) for document \\(d\\). Note that each vector does not sum to 1. \\(z_{d,n}\\) will also be a length \\(K\\) vector, but instead is a distribution over topic assignments \\(\\phi_{d,n}\\) for each token \\(z_{d,n}\\). Each vector sum to 1. Recall that the ELBO is \\(\\mathbb{E}_q [\\log p(x,z)] - \\mathbb{E}_q [\\log q(z)]\\), where \\(x\\) is our observed data and \\(z\\) are our latent variables. In LDA, \\(w\\) is our observed data, and \\(\\theta\\) and \\(z\\) are our latent variables. Using the joint distribution over latent variables shown above, we have \\[ \\begin{aligned} \\log p&amp;(w|\\alpha,\\beta) \\geq L(\\gamma,\\phi|\\alpha,\\beta) \\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta,z,w|\\alpha,\\beta)] - \\mathbb{E}_q [\\log q(\\theta,z)] \\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)p(z|\\theta)p(w|z,\\beta)] - \\mathbb{E}_q [\\log q(\\theta)q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha) + \\log p(z|\\theta) + \\log p(w|z,\\beta)] - \\mathbb{E}_q [\\log q(\\theta) + \\log q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)] + \\mathbb{E}_q [\\log p(z|\\theta)] + \\mathbb{E}_q [\\log p(w|z,\\beta)] - \\mathbb{E}_q [\\log q(\\theta)] - \\mathbb{E}_q[\\log q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)] + \\mathbb{E}_q [\\log p(z|\\theta)] + \\mathbb{E}_q [\\log p(w|z,\\beta)] + \\mathbb{H}_q [\\gamma] + \\mathbb{H}_q[\\phi] \\end{aligned} \\] Before we continue, note that \\(\\mathbb{E}_q\\) means the expectation with respect to all parameters in our variational distribution \\(q\\), so \\(\\theta\\) and \\(z\\). 12.2.1 Expectation of p(\\(\\theta|\\alpha)\\) To calculate \\(\\mathbb{E}[\\log p(\\theta | \\alpha)]\\), where \\(\\theta\\) is our variational parameter, let’s first recall that it is a Dirichlet distribution; therefore, we have \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(\\theta | \\alpha)] &amp;= \\mathbb{E}_q \\log{[\\frac{\\Gamma (\\sum_i \\alpha_i)}{\\prod_i \\Gamma (\\alpha_i)} \\prod_i \\theta_{d,k}^{\\alpha_i -1}]}\\\\ &amp;= \\mathbb{E}_q \\log \\Gamma (\\sum_i^k \\alpha_i) - \\mathbb{E}_q \\sum_i^k \\log \\Gamma (\\alpha_i) + \\mathbb{E}_q \\sum_i^k (\\alpha_i -1) \\log \\theta \\\\ &amp;= \\log \\Gamma (\\sum_i^k \\alpha_i) - \\sum_i^k \\log \\Gamma (\\alpha_i) + \\sum_i^k (\\alpha_i -1) \\mathbb{E}_q \\log \\theta \\\\ \\end{aligned} \\] Note that \\(\\mathbb{E}_q [f(\\alpha)] = f(\\alpha)\\) because \\(\\alpha\\) is not one of the latent parameters in our variational distribution \\(q\\). Now we have to calculate \\(\\mathbb{E}_q \\log \\theta\\) where \\(\\theta \\thicksim \\text{Dirichlet}\\). We must first convert this Dirichlet to its exponential form (given the fact that it belongs to the exponential family). First, we exponentiate the log of \\(p(\\theta | \\alpha)\\): \\[ \\exp{[\\log \\Gamma (\\sum_i^k \\alpha_i) - \\sum_i^k \\log \\Gamma (\\alpha_i) + (\\sum_i^k (\\alpha_i -1) \\log \\theta)]} \\] A pdf belonging to the exponential family has the form \\[ p(x|\\theta) = h(x) \\exp{[\\theta^T \\phi (x) - A(\\theta)]} \\] where \\(\\theta\\) are the natural or canonical parameters, \\(\\phi (x)\\) is a vector of sufficient statistics, \\(h (x)\\) is a scaling constant that is often equal to 1, and \\(A (\\theta)\\) is the log partition function. If we make one slight adjustment to the Dirichlet in exponential family form, we can these components more easily: \\[ \\exp{[(\\sum_i^k (\\alpha_i -1) \\log \\theta) - (\\sum_i^k \\log \\Gamma (\\alpha_i) + \\log \\Gamma (\\sum_i^k \\alpha_i))]} \\] where \\(\\sum_i^k (\\alpha_i -1)\\) are the natural parameters, \\(\\log \\theta\\) is the vector of sufficient statistics, and \\(\\sum_i^k \\log \\Gamma (\\alpha_i) + \\log \\Gamma (\\sum_i^k \\alpha_i)\\) is the log partition function. Taking the derivative of the log partition function with respect to the natural parameters results in the expectation of the sufficient statistic. Recall that we are working with \\(q(\\theta_d|\\gamma_d)\\) there, our variational distribution, and not \\(p(\\theta_d|\\alpha)\\): \\[ \\begin{aligned} \\mathbb{E}_q[\\log \\theta] &amp;= \\frac{\\partial}{\\partial \\gamma_i} (\\log \\Gamma (\\gamma_i) + \\log \\Gamma (\\sum_j^k \\gamma_j)) \\\\ &amp;= \\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j) \\end{aligned} \\] And so we have \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(\\theta | \\alpha)] &amp;= \\log \\Gamma (\\sum_i^k \\alpha_i) - \\sum_i^k \\log \\Gamma (\\alpha_i) + \\sum_i^k (\\alpha_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ \\end{aligned} \\] 12.2.2 Expectation of p(\\(z|\\theta)\\) For this expectation, we do the following: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z|\\theta)] &amp;= \\mathbb{E}_q \\log [\\prod_n \\prod_i \\theta_i^{1[z_n=i]}]\\\\ &amp;= \\mathbb{E}_q [\\sum_n \\sum_i \\log \\theta_i^{1[z_n=i]}]\\\\ &amp;= \\sum_n \\sum_i \\mathbb{E}_q [\\log \\theta_i^{1[z_n=i]}]\\\\ &amp;= \\sum_n \\sum_i \\mathbb{E}_q [1[z_n=i] \\log \\theta_i] \\end{aligned} \\] Recall that \\(1[z_n=i]\\) is an indicator of whether a particular word’s topic assignment \\(z_n\\) is equal to topic \\(i\\). If so, then we add the probability \\(\\theta_{d,i}\\) of this topic in document \\(d\\) to the summation. Because in our variational distribution \\(q\\) we assume that \\(\\theta\\) and \\(z\\) are independent, we can do the following: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z|\\theta)] &amp;= \\sum_n \\sum_i \\mathbb{E}_q [1[z_n=i] \\log \\theta_i]\\\\ &amp;= \\sum_n \\sum_i \\mathbb{E}_q 1[z_n=i] \\mathbb{E}_q \\log \\theta_i\\\\ \\end{aligned} \\] The expectation of this indicator function is just a measure of how much a particular token \\(n\\) takes on a topic assignment \\(i\\), which we’ll call \\(\\phi_{n,i}\\): \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z|\\theta)] &amp;= \\sum_n \\sum_i \\phi_{n,i} \\mathbb{E}_q \\log \\theta_i\\\\ &amp;= \\sum_n \\sum_i \\phi_{n,i} (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\end{aligned} \\] 12.2.3 Expectation of p(\\(w|z,\\beta)\\) Here we are simply looking up the probability \\(\\beta\\) of a topic assignment \\(z_{d,n}\\) for the word \\(w_{d,n}\\). \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(w|z,\\beta)] &amp;= \\mathbb{E}_q [\\log \\prod_v \\prod_i \\beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]}] \\\\ &amp;= \\mathbb{E}_q [\\sum_v \\sum_i \\log \\beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]}] \\\\ &amp;= \\sum_v \\sum_i \\mathbb{E}_q [\\log \\beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]}] \\\\ &amp;= \\sum_v \\sum_i \\mathbb{E}_q [1[w_{d,n}=v,z_{d,n}=i] \\log \\beta_{i,v}] \\\\ &amp;= \\sum_v \\sum_i \\mathbb{E}_q [1[w_{d,n}=v,z_{d,n}=i]] \\log \\beta_{i,v} \\\\ &amp;= \\sum_v \\sum_i \\mathbb{E}_q [1[w_{d,n}=v]1[z_{d,n}=i]] \\log \\beta_{i,v} \\\\ &amp;= \\sum_v \\sum_i 1[w_{d,n}=v] \\mathbb{E}_q [1[z_{d,n}=i]] \\log \\beta_{i,v} \\\\ \\end{aligned} \\] Recall that we already defined \\(\\mathbb{E}_q 1[z_n=i] = \\phi_{n,i}\\), leaving us with \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(w|z,\\beta)] &amp;= \\sum_v \\sum_i 1[w_{d,n}=v] \\phi_{n,i} \\log \\beta_{i,v} \\end{aligned} \\] 12.2.4 Entropy of \\(\\gamma\\) and \\(\\phi\\) We can simply look up the entropy of a Dirichlet for \\(\\gamma\\) and the entropy of a multinomial for \\(\\phi_{d,n}\\), giving us \\[ \\begin{aligned} \\mathbb{H}_q [\\gamma] &amp;= -\\log \\Gamma (\\sum_j \\gamma_j) + \\sum_i \\log \\Gamma(\\gamma_i) - \\sum_i (\\gamma_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j))\\\\ \\mathbb{H}_q [\\phi_{d,n}] &amp;= -\\sum_i \\phi_{d,n,i} \\log \\phi_{d,n,i} \\end{aligned} \\] 12.2.5 Complete Objective Function Now that we calculated the expectations and entropies we need, we can fill in our original equation: \\[ \\begin{aligned} \\mathbb{E}_q &amp;[\\log p(\\theta,z,w|\\alpha,\\beta)] - \\mathbb{E}_q [\\log q(\\theta,z)] \\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)] + \\mathbb{E}_q [\\log p(z|\\theta)] + \\mathbb{E}_q [\\log p(w|z,\\beta)] + \\mathbb{H}_q [\\gamma] + \\mathbb{H}_q[\\phi_{d,n}] \\\\ \\end{aligned} \\] Giving us \\[ \\begin{aligned} \\mathbb{E}_q &amp;[\\log p(\\theta,z,w|\\alpha,\\beta)] - \\mathbb{E}_q [\\log q(\\theta,z)] = \\\\ &amp;\\log \\Gamma (\\sum_i \\alpha_j) - \\sum_i \\log \\Gamma (\\alpha_i) + \\sum_i^k (\\alpha_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ &amp;+ \\sum_n \\sum_i \\phi_{n,i} (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ &amp;+ \\sum_n \\sum_i \\sum_v 1[w_n=v] \\phi_{n,i} \\log \\beta_{i,v} \\\\ &amp;- \\log \\Gamma (\\sum_j \\gamma_j) + \\sum_i \\log \\Gamma(\\gamma_i) - \\sum_i (\\gamma_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ &amp;- \\sum_n \\sum_i \\phi_{n,i} \\log \\phi_{n,i} \\end{aligned} \\] 12.2.6 Parameter Optimization To derive our parameter updates for variational EM, we need to maximize the objective function above with respect to our target parameter. 12.2.6.1 Optimization for \\(\\phi\\) Using the objective function above, we’ll isolate every term that is a function of \\(phi\\), and using the constraint \\(\\sum_i \\phi_{n,i}=1\\), we’ll perform the following optimization: \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\phi_{n,i}} &amp;= \\frac{\\partial}{\\partial \\phi_{n,i}}[\\phi_{n,i} (\\Psi(\\gamma_i) - \\Psi(\\sum_j \\gamma_j)) + \\sum_j 1[w_n = v] \\phi_{n,i} \\log \\beta_{i,v} \\\\ &amp;\\qquad- \\phi_{n,i} \\log \\phi_{n,i} + \\lambda_n (\\phi_{n,i} - 1)]\\\\ &amp;=\\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\lambda \\end{aligned} \\] Then, setting to this to zero, we get \\[ \\begin{aligned} 0 &amp;=\\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\lambda\\\\ \\log \\phi_{n,i} &amp;= \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - 1 + \\lambda \\\\ \\phi_{n,i} &amp;= \\exp [\\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - 1 + \\lambda]\\\\ &amp;= \\beta_{i,v}^{1[w_n = v]} \\exp [\\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j)] \\exp [- 1 + \\lambda]\\\\ &amp;= c_{n,i} \\beta_{i,v}^{1[w_n = v]} \\exp [\\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j)] \\end{aligned} \\] where \\(c_{n,i}=\\exp [\\lambda - 1]\\). Because we don’t know \\(\\lambda\\), we will simply calculate the unnormalized \\(\\phi_{n,i}\\) and then normalize to enforce the constraint that \\(\\sum_i \\phi_{n,i}=1\\). Therefore, we have \\[ \\begin{aligned} \\phi_{n,i} &amp;\\propto \\beta_{i,v}^{1[w_n = v]} \\exp [\\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j)] \\end{aligned} \\] 12.2.6.2 Optimization for \\(\\gamma_i\\) Now we’ll take all terms that are a function of \\(\\gamma_i\\): \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\gamma_i} &amp;= \\frac{\\partial}{\\partial \\gamma_i}[(\\alpha_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j \\gamma_j)) \\\\ &amp;\\qquad + \\sum_n \\phi_{n,i} (\\Psi (\\gamma_i) - \\Psi (\\sum_j \\gamma_j)) \\\\ &amp;\\qquad - \\log \\Gamma (\\sum_j \\gamma_j) + \\log \\Gamma(\\gamma_i) \\\\ &amp;\\qquad - (\\gamma_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j \\gamma_j))] \\\\ &amp;= \\frac{\\partial}{\\partial \\gamma_i} [\\alpha_i \\Psi (\\gamma_i) - \\alpha_i \\Psi (\\sum_j \\gamma_j) - \\Psi (\\gamma_i) + \\Psi (\\sum_j \\gamma_j)\\\\ &amp;\\qquad + \\sum_n \\phi_{n,i} \\Psi (\\gamma_i) - \\sum_n \\phi_{n,i} \\Psi (\\sum_j \\gamma_j)\\\\ &amp;\\qquad - \\log \\Gamma (\\sum_j \\gamma_j) + \\log \\Gamma (\\gamma_i) \\\\ &amp;\\qquad - \\gamma_i \\Psi (\\gamma_i) + \\gamma_i \\Psi (\\sum_j \\gamma_j) + \\Psi (\\gamma_i) - \\Psi(\\sum_j \\gamma_j)]\\\\ &amp;= \\alpha_i \\Psi&#39; (\\gamma_i) - \\alpha_i \\Psi&#39; (\\sum_j \\gamma_j) - \\Psi&#39; (\\gamma_i) + \\Psi&#39; (\\sum_j \\gamma_j)\\\\ &amp;\\qquad + \\sum_n \\phi_{n,i} \\Psi&#39; (\\gamma_i) - \\sum_n \\phi_{n,i} \\Psi&#39; (\\sum_j \\gamma_j)\\\\ &amp;\\qquad - \\Psi (\\sum_j \\gamma_j) + \\Psi (\\gamma_i) \\\\ &amp;\\qquad - \\gamma_i \\Psi&#39; (\\gamma_i) - \\Psi (\\gamma_i) + \\gamma_i \\Psi&#39; (\\sum_j \\gamma_j) + \\Psi (\\sum_j \\gamma_j) + \\Psi&#39; (\\gamma_i) - \\Psi&#39; (\\sum_j \\gamma_j)\\\\ &amp;= \\alpha_i \\Psi&#39; (\\gamma_i) - \\alpha_i \\Psi&#39; (\\sum_j \\gamma_j) - \\Psi&#39; (\\gamma_i) + \\Psi&#39; (\\sum_j \\gamma_j)\\\\ &amp;\\qquad + \\sum_n \\phi_{n,i} \\Psi&#39; (\\gamma_i) - \\sum_n \\phi_{n,i} \\Psi&#39; (\\sum_j \\gamma_j)\\\\ &amp;\\qquad - \\gamma_i \\Psi&#39; (\\gamma_i) + \\gamma_i \\Psi&#39; (\\sum_j \\gamma_j) + \\Psi&#39; (\\gamma_i) - \\Psi&#39; (\\sum_j \\gamma_j) \\end{aligned} \\] Now combine terms: \\[ \\begin{aligned} \\Psi&#39; (\\gamma_i)(\\alpha_i - 1 + \\sum_n \\phi_{n,i} - \\gamma_i + 1) &amp;= \\Psi&#39; (\\sum_j \\gamma_j)(\\alpha_i + \\sum_n \\phi_{n,i} - \\gamma_j)\\\\ \\Psi&#39; (\\gamma_i)(\\alpha_i + \\sum_n \\phi_{n,i} - \\gamma_i) &amp;= \\Psi&#39; (\\sum_j \\gamma_j)(\\alpha_i + \\sum_n \\phi_{n,i} - \\gamma_j)\\\\ \\end{aligned} \\] Then set this to zero: \\[ \\begin{aligned} 0 &amp;= \\Psi&#39; (\\gamma_i)(\\alpha_i + \\sum_n \\phi_{n,i} - \\gamma_i) - \\Psi&#39; (\\sum_j \\gamma_j)(\\alpha_i + \\sum_n \\phi_{n,i} - \\gamma_j)\\\\ \\end{aligned} \\] Leaving us with \\[ \\begin{aligned} \\gamma_i &amp;= \\alpha_i + \\sum_n \\phi_{n,i} \\end{aligned} \\] 12.2.6.3 Optimization for \\(\\beta\\) Now, we’ll extract all terms that are a function of \\(\\beta\\) and add the constrain \\(\\sum_v \\beta_{i,v} = 1\\): \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\beta} &amp;= \\frac{\\partial}{\\partial \\beta}[\\sum_n \\sum_i \\sum_v 1[w_n=v]\\phi_{n,i} \\log \\beta_{i,v} + \\sum_i \\lambda_i (\\sum_v \\beta_{i,v} - 1)]\\\\ &amp;=\\sum_n \\sum_i \\sum_v \\frac{1[w_n=v]\\phi_{n,i}}{\\beta_{i,v}} + \\sum_i \\lambda_i\\\\ \\end{aligned} \\] Setting this to 0 results in \\[ \\begin{aligned} 0 &amp;=\\sum_n \\sum_i \\sum_v \\frac{1[w_n=v]\\phi_{n,i}}{\\beta_{i,v}} + \\sum_i \\lambda_i\\\\ - \\beta_{i,v} \\sum_i \\lambda_i &amp;=\\sum_n \\sum_i \\sum_v 1[w_n=v]\\phi_{n,i}\\\\ \\beta_{i,v} &amp;= c_{i,v} \\sum_n \\sum_i \\sum_v 1[w_n=v]\\phi_{n,i}^{(k)} \\end{aligned} \\] where \\(c_{i,v} = -\\frac{1}{\\sum_i \\lambda_i}\\). Like what we did for \\(\\phi\\), we will calculate unnormalized \\(\\beta\\) and then normalize to satisfy the constraint. Our final update is the following: \\[ \\begin{aligned} \\beta_{i,v} &amp;\\propto \\sum_n \\sum_i \\sum_v 1[w_n=v]\\phi_{n,i} \\end{aligned} \\] library(tidyverse) vocab &lt;- c(&quot;river&quot;,&quot;stream&quot;,&quot;bank&quot;,&quot;money&quot;,&quot;loan&quot;) topic1 &lt;- c(.333,.333,.333,0,0) topic2 &lt;- c(0,0,.333,.333,.333) topics &lt;- topic1 + topic2 K &lt;- 2 V &lt;- length(vocab) N &lt;- 16 corpus &lt;- rbind(matrix(rep(sample(vocab,N,topic1,replace=T),7),7,N), matrix(rep(sample(vocab,N,topics,replace=T),10),10,N), matrix(rep(sample(vocab,N,topic2,replace=T),5),5,N)) M &lt;- nrow(corpus) start &lt;- t(apply(corpus, 1, function(x) c(sum(x==vocab[1]),sum(x==vocab[2]),sum(x==vocab[3]),sum(x==vocab[4]),sum(x==vocab[5])))) docnames &lt;- paste(&quot;D&quot;,1:M,sep=&quot;&quot;) rownames(start) &lt;- docnames colnames(start) &lt;- vocab topicassign &lt;- matrix(sample(c(0,1),M*N,replace=T),M,N,byrow=T) df &lt;- NULL for(i in 1:M){ df &lt;- rbind(df,cbind(corpus[i,],rep(docnames[i]),topicassign[i,])) } df &lt;- data.frame(df) names(df) &lt;- c(&#39;word&#39;,&#39;doc&#39;,&#39;topic&#39;) df_original &lt;- df nu &lt;- .25 alpha &lt;- .5 for (iter in 1:100){ cat(&quot;i=&quot;,iter,&quot;\\n&quot;,sep=&quot;&quot;) print(xtabs(~ word+topic,df)) cat(&quot;\\n&quot;,sep=&quot;&quot;) for (i in 1:nrow(df)){ w &lt;- as.vector(df[i,1]) d &lt;- as.vector(df[i,2]) civk &lt;- df[-i,] %&gt;% group_by(word,doc,topic) %&gt;% summarise(civk=n()) cik &lt;- df[-i,] %&gt;% group_by(doc,topic) %&gt;% summarise(cik=n()) cvk &lt;- df[-i,] %&gt;% group_by(word,topic) %&gt;% summarise(cvk=n()) niv &lt;- df[-i,] %&gt;% group_by(word,doc) %&gt;% summarise(niv=n()) ck &lt;- df[-i,] %&gt;% group_by(topic) %&gt;% summarise(words=n()) Li &lt;- df[-i,] %&gt;% group_by(doc) %&gt;% summarise(words=n()) a &lt;- unlist((cvk[cvk$word==w,3] + nu))/unlist((ck[,2] + V*nu)) b &lt;- unlist((cik[cik$doc==d,3] + alpha))/unlist((Li[Li$doc==d,2] + K*alpha)) pqilk &lt;- unlist((a*b)/sum(a*b)) df[i,3] &lt;- sample(c(0,1),size=1,prob=pqilk) } } est_alpha &lt;- TRUE alpha &lt;- rep(50/K,K) beta &lt;- rdirichlet(K,rep(1,V)) for (m_step in 1:50){ gamma &lt;- matrix(alpha + N/K,M,K,byrow=TRUE) phi &lt;- array(1/K,c(M,N,K)) ## E step for (d in 1:M){ conv &lt;- Inf tol &lt;- 1e-6 while (tol &lt; conv){ phi0 &lt;- phi gamma0 &lt;- gamma for (n in 1:N){ for (i in 1:K){ phi[d,n,i] &lt;- beta[i,corpus[d,n]==vocab] * exp(digamma(gamma[d,i]) - digamma(sum(gamma[d,]))) } phi[d,n,] &lt;- phi[d,n,]/sum(phi[d,n,]) } gamma[d,] &lt;- alpha + colSums(phi[d,,]) conv &lt;- max(c(max(abs(phi-phi0)),max(abs(gamma[d,]-gamma0[d,])))) } } ## M step for (i in 1:K){ for (j in 1:V){ w_dnj &lt;- corpus == vocab[j] beta[i,j] &lt;- ifelse(any(w_dnj),sum(phi[,,i][w_dnj]),1e-20) } beta[i,] &lt;- beta[i,]/sum(beta[i,]) } ## alpha if (est_alpha == TRUE){ conv &lt;- Inf tol &lt;- 1e-3 iter &lt;- 1 alpha_init &lt;- 100 alpha &lt;- alpha_init while (tol &lt; conv | iter &lt; 100){ if (any(is.na(alpha))){ alpha_init &lt;- alpha_init*10 alpha &lt;- alpha_init } alpha0 &lt;- alpha d1alpha &lt;- M*(K*digamma(K*alpha) - K*digamma(alpha)) + colSums(digamma(gamma) - K*digamma(rowSums(gamma))) d2alpha &lt;- M*(K * K * trigamma(K*alpha) - K*trigamma(alpha)) # note sure how to use the Kronecker-delta function here log_alpha &lt;- log(alpha) - d1alpha/(d2alpha*alpha + d1alpha) alpha &lt;- exp(log_alpha) conv &lt;- max(abs(alpha-alpha0)) iter &lt;- iter + 1 } } # Check word_counter &lt;- matrix(0,V,K) for (d in 1:M){ for (n in 1:N){ word &lt;- corpus[d,n] == vocab topic &lt;- sample(c(1,2),1,prob=beta[,word]) word_counter[word,topic] &lt;- word_counter[word,topic] + 1 } } print(word_counter) } 12.3 Supervised LDA For sLDA, we have the following generative model This has a similar joint distribution to LDA, but note \\(\\eta\\), \\(\\sigma^2\\), and another observed variable \\(y\\). These are the regression coefficients, model noise, and document labels, respectively: \\[ p(\\theta,z,w,y|\\eta,\\alpha,\\beta,\\sigma^2) = \\prod_d p(\\theta_d|\\alpha) p(y_d|z_{d,n},\\eta,\\sigma^2) \\prod_n p(z_{d,n}|\\theta_d)p(w_{d,n}|z_{d,n},\\beta) \\] where \\[ \\begin{aligned} &amp;p(\\theta_d|\\alpha) = \\frac{\\Gamma (\\sum_i \\alpha_i)}{\\prod_i \\Gamma (\\alpha_i)} \\prod_i \\theta_{d,k}^{\\alpha_i -1} \\text{, (Dirichlet)}\\\\ &amp;p(z_{d,n}|\\theta_d) = \\prod_n \\prod_i \\theta_i^{1[z_n=i]} \\text{, (Multinomial)}\\\\ &amp;p(w_{d,n}|z_{d,n},\\beta) = \\prod_v \\prod_i \\beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]} \\text{, (Multinomial)}\\\\ &amp;p(y_d|z_{d,n},\\eta,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp [-\\frac{(y-\\bar Z \\eta)^2}{2\\sigma^2}] \\text{, (Normal)} \\end{aligned} \\] where \\(\\bar Z\\) is an \\(K \\times D\\) matrix of topic proportions such that \\(\\bar Z_{i,d}\\) is the proportion topic \\(i\\) is represented out of all topics for document \\(d\\). Note that \\(\\sum_i \\bar Z_{i,d}=1\\). We’ll again use the mean field method, giving us a fully factored form of the latent variables \\(\\theta\\) and \\(z\\): \\[ q(\\theta,z|\\gamma,\\phi) = \\prod_d q(\\theta_d | \\gamma_d) \\prod_n q(z_{d,n}|\\phi_{d,n}) \\] which allows us to derive our variational objective function \\[ \\begin{aligned} \\log p&amp;(w|\\alpha,\\beta,\\eta,\\sigma^2) \\geq L(\\gamma,\\phi|\\alpha,\\beta,\\eta,\\sigma^2) \\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta,z,w,y|\\eta,\\alpha,\\beta,\\sigma^2)] - \\mathbb{E}_q [\\log q(\\theta,z)] \\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)p(y|z,\\eta,\\sigma^2)p(z|\\theta)p(w|z,\\beta)] - \\mathbb{E}_q [\\log q(\\theta)q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha) + \\log p(y|z,\\eta,\\sigma^2) + \\log p(z|\\theta) + \\log p(w|z,\\beta)] \\\\ &amp;\\qquad - \\mathbb{E}_q [\\log q(\\theta) + \\log q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)] + \\mathbb{E}_q[\\log p(y|z,\\eta,\\sigma^2)] + \\mathbb{E}_q [\\log p(z|\\theta)] + \\mathbb{E}_q[ \\log p(w|z,\\beta)] \\\\ &amp;\\qquad - \\mathbb{E}_q [\\log q(\\theta)] - \\mathbb{E}_q [\\log q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)] + \\mathbb{E}_q[\\log p(y|z,\\eta,\\sigma^2)] + \\mathbb{E}_q [\\log p(z|\\theta)] + \\mathbb{E}_q[ \\log p(w|z,\\beta)] \\\\ &amp;\\qquad + \\mathbb{H}_q [\\gamma] + \\mathbb{H}_q [\\phi] \\end{aligned} \\] 12.3.1 Expectations of \\(p(\\theta|\\alpha)\\), \\(p(z|\\theta)\\), and \\(p(w|z,\\beta)\\) These are identical to what we ended up with for LDA: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(\\theta | \\alpha)] &amp;= \\log \\Gamma (\\sum_i^k \\alpha_i) - \\sum_i^k \\log \\Gamma (\\alpha_i) + \\sum_i^k (\\alpha_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ \\mathbb{E}_q [\\log p(z|\\theta)] &amp;= \\sum_n \\sum_i \\phi_{n,i} (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j))\\\\ \\mathbb{E}_q [\\log p(w|z,\\beta)] &amp;= \\sum_v \\sum_i 1[w_{d,n}=v] \\phi_{n,i} \\log \\beta_{i,v} \\end{aligned} \\] 12.3.2 Expectation of \\(p(y|z,\\eta,\\sigma^2)\\) First, keep in mind that we are tackling this expectation for a single document \\(d\\). \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(y|z,\\eta,\\sigma^2)] &amp;= \\mathbb{E}_q [\\log 1 + \\log (2 \\pi \\sigma^2)^{-\\frac{1}{2}} - \\frac{(y-\\bar Z \\eta)^2}{2\\sigma^2}]\\\\ &amp;= \\mathbb{E}_q [-\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{y^2 - 2y\\bar Z \\eta + \\bar Z \\eta \\bar Z^T \\eta}{2\\sigma^2}]\\\\ &amp;= -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{y^2 - 2y\\eta^T \\mathbb{E}_q [\\bar Z] + \\eta^T \\mathbb{E}_q [\\bar Z \\bar Z^T] \\eta}{2\\sigma^2} \\end{aligned} \\] To determine \\(\\mathbb{E}_q [\\bar Z\\)], recall that it is a \\(K \\times D\\) matrix representing the topic frequencies across documents. Because we are focusing on only one document, we are dealing with a K-vector. Recall \\(z_n\\). If we wrote this in vector form, then for a document \\(d\\) and word \\(n\\), we’d have a K-vector of indicator values where 1 corresponds to the topic assignment, with all other topics receiving a 0. (If we were to store z, it would be a 3-dimensional array.) Now, we already defined \\(\\mathbb{E}_q 1[z_n=i] = \\phi_{n,i}\\), where \\(\\phi\\) is an \\(N \\times K\\) matrix of word occurrences across topics and it accounts for every document. The mean frequency of topic assignments would be column means of this matrix \\(\\phi\\), which would be a K-vector we’ll call \\(\\bar \\phi\\): \\[ \\mathbb{E}_q [\\bar Z] = \\bar \\phi := \\frac{1}{N}\\sum_n \\phi_{n} \\] For \\(\\mathbb{E}_q [\\bar Z \\bar Z^T]\\), first we’ll exploit the fact that our variational distribution is fully factorized, allowing us to assume independence between latent variables. For the case where \\(n \\neq m\\), we get \\(\\mathbb{E}_q [ z_n z_m^T] = \\mathbb{E}_q [z_n] \\mathbb{E}_q [ z_m^T] = \\phi_n \\phi_m^T\\), thanks to the full factorization. When \\(n = m\\), we have \\(\\mathbb{E}_q [z_n z_n^T]\\), but recall that \\(z_n\\) is just an indicator vector of length \\(K\\), consisting of zeros or ones, where \\(z_n z_n^T\\) results in a \\(K \\times K\\) matrix, where there will be a \\(1\\) somewhere on the diagonal corresponding to the topic assignment for word \\(n\\). Therefore, \\(z_n z_n^T = \\text{diag}(z_n)\\). Now we have \\(\\mathbb{E}_q [z_n z_n^T] = \\text{diag}(\\mathbb{E}_q[z_n])\\). The expectation \\(\\mathbb{E}_q [z_n]\\) is simply the topic frequencies for word \\(n\\); that is, \\(\\text{diag}(\\mathbb{E}_q[z_n]) = \\text{diag}(\\phi_n)\\). Now that we have \\(\\mathbb{E}_q [ z_n z_m^T]\\) when \\(n \\neq m\\) and \\(\\mathbb{E}_q [ z_n z_n^T]\\) when \\(n = m\\), we can add them together and divide by \\(N^2\\): \\[ \\mathbb{E}_q [\\bar Z \\bar Z^T] = \\frac{1}{N^2} (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\] Now, we can write the complete expectation: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(&amp;y|z,\\eta,\\sigma^2)] \\\\ &amp;= -\\frac{1}{2} \\log (2 \\pi \\sigma^2) \\\\ &amp;\\qquad - (\\frac{y^2 - \\frac{2}{N}y\\eta^T \\sum_n \\phi_{n} + \\frac{1}{N^2} \\eta^T (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\eta}{2\\sigma^2}) \\end{aligned} \\] 12.3.3 Entropy of \\(\\gamma\\) and \\(\\phi\\) These will be the same as LDA: \\[ \\begin{aligned} \\mathbb{H}_q [\\gamma] &amp;= -\\log \\Gamma (\\sum_j \\gamma_j) + \\sum_i \\log \\Gamma(\\gamma_i) - \\sum_i (\\gamma_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j))\\\\ \\mathbb{H}_q [\\phi_{d,n}] &amp;= -\\sum_i \\phi_{d,n,i} \\log \\phi_{d,n,i} \\end{aligned} \\] 12.3.4 Complete Objective Function Let’s fill in the ELBO: \\[ \\begin{aligned} \\mathbb{E}_q &amp;[\\log p(\\theta,z,w,y|\\eta,\\alpha,\\beta,\\sigma^2)] - \\mathbb{E}_q [\\log q(\\theta,z)] \\\\ &amp;= \\mathbb{E}_q [\\log p(\\theta|\\alpha)] + \\mathbb{E}_q[\\log p(y|z,\\eta,\\sigma^2)] + \\mathbb{E}_q [\\log p(z|\\theta)] + \\mathbb{E}_q[ \\log p(w|z,\\beta)] \\\\ &amp;\\qquad + \\mathbb{H}_q [\\gamma] + \\mathbb{H}_q [\\phi]\\\\ &amp;= \\log \\Gamma (\\sum_i^k \\alpha_i) - \\sum_i^k \\log \\Gamma (\\alpha_i) + \\sum_i^k (\\alpha_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ &amp;\\qquad -\\frac{1}{2} \\log (2 \\pi \\sigma^2) \\\\ &amp;\\qquad - \\frac{y^2}{2\\sigma^2} + \\frac{y\\eta^T \\sum_n \\phi_{n}}{N\\sigma^2} - \\frac{\\eta^T (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\eta}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\sum_n \\sum_i \\phi_{n,i} (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ &amp;\\qquad + \\sum_n \\sum_v \\sum_i 1[w_{d,n}=v] \\phi_{n,i} \\log \\beta_{i,v} \\\\ &amp;\\qquad -\\log \\Gamma (\\sum_j \\gamma_j) + \\sum_i \\log \\Gamma(\\gamma_i) - \\sum_i (\\gamma_i -1) (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) \\\\ &amp;\\qquad - \\sum_n \\sum_i \\phi_{d,n,i} \\log \\phi_{d,n,i} \\end{aligned} \\] 12.3.5 Parameter Optimization 12.3.5.1 Optimization for \\(\\gamma_i\\) and \\(\\beta\\) These are the same as LDA since they don’t involve \\(y\\): \\[ \\begin{aligned} \\gamma_i &amp;= \\alpha_i + \\sum_n \\phi_{n,i}\\\\ \\beta_{i,v} &amp;\\propto \\sum_n \\sum_i \\sum_v 1[w_n=v]\\phi_{n,i} \\end{aligned} \\] 12.3.5.2 Optimization for \\(\\phi\\) This will be somewhat similar to LDA, but we have to account for all of the terms associated with \\(y\\). First, let’s take all of the terms in the objective functions associated with \\(\\phi\\): \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\phi_{n,i}} &amp;= \\frac{\\partial}{\\partial \\phi_{n,i}}[\\frac{y\\eta^T \\sum_n \\phi_{n}}{N\\sigma^2} - \\frac{\\eta^T (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\eta}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\sum_n \\sum_i \\phi_{n,i} (\\Psi (\\gamma_i) - \\Psi (\\sum_j^k \\gamma_j)) + \\sum_n \\sum_v \\sum_i 1[w_{d,n}=v] \\phi_{n,i} \\log \\beta_{i,v} \\\\ &amp;\\qquad - \\sum_n \\sum_i \\phi_{d,n,i} \\log \\phi_{d,n,i}]\\\\ &amp;= \\frac{\\partial}{\\partial \\phi_{n,i}}[\\frac{y\\eta^T \\sum_n \\phi_{n}}{N\\sigma^2} - \\frac{\\eta^T (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\eta}{2N^2 \\sigma^2}] \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\lambda \\end{aligned} \\] So we easily optimized the terms we had to deal with for LDA, leaving us with our new terms that are associated with \\(y\\). The first terms is easy, so we’ll get it out of the way: \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\phi_{n,i}} &amp;= \\frac{y\\eta}{N\\sigma^2} - \\frac{1}{2N^2 \\sigma^2} \\frac{\\partial}{\\partial \\phi_{n,i}}[\\eta^T (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\eta] \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\lambda \\end{aligned} \\] We should stop here and talk about this remaining unoptimized term. If we are dealing with \\(y \\thicksim \\text{Normal}(\\mu,\\sigma^2)\\) or \\(y \\thicksim \\text{Poisson}(\\lambda)\\), then we can solve this and obtain and exact update for coordinate descent. Other types of labels will require a gradient optimization procedure, which would be the more generalized version of this variational EM strategy. To calculate this partial derivative, we’ll focus on \\(\\phi_n\\) that corresponds to a single word \\(n\\), This is a vector of length \\(K\\). We’ll call this \\(\\phi_j\\). This allows us to rewrite \\(\\eta^T (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)) \\eta\\) as \\[ \\begin{aligned} f(\\phi_j) &amp;= \\eta^T [\\phi_j \\phi_{-j}^T + \\phi_{-j} \\phi_j^T + \\text{diag}(\\phi_j)] \\eta + \\text{const}\\\\ &amp;= \\eta^T \\phi_j \\phi_{-j}^T \\eta + \\eta^T \\phi_{-j} \\phi_j^T \\eta + \\eta^T \\text{diag}(\\phi_j) \\eta + \\text{const} \\end{aligned} \\] First, notice that \\(\\eta^T \\phi_j = \\phi_j^T \\eta\\) because they are both scalars (i.e., \\(1 \\times 1\\)). The same is true for \\(\\phi_{-j}^T \\eta = \\eta^T \\phi_{-j}\\). Therefore, we can rewrite \\(f\\) as \\[ \\begin{aligned} f(\\phi_j) &amp;= 2 \\eta^T \\phi_{-j} \\eta^T \\phi_j + \\eta^T \\text{diag}(\\phi_j) \\eta + \\text{const} \\end{aligned} \\] Second, \\(\\eta^T \\text{diag}(\\phi_j) \\eta\\) is also a scalar. It’s worth making some vectors in R and testing this, but \\(\\eta^T \\text{diag}(\\phi_j)\\) simply multiplies each element in \\(\\eta\\) with each element in \\(\\phi\\) and then returns a row vector of length \\(K\\). Then we calculate its dot product with \\(\\eta\\), giving us a scalar. This is exactly the same as doing \\((\\eta \\circ \\eta)^T \\phi\\); therefore, we have the following: \\[ \\begin{aligned} f(\\phi_j) &amp;= 2 \\eta^T \\phi_{-j} \\eta^T \\phi_j + (\\eta \\circ \\eta)^T \\phi_j + \\text{const} \\end{aligned} \\] Now, in this form, we can easily compute the gradient: \\[ \\begin{aligned} \\frac{\\partial f}{\\partial \\phi_j} &amp;= \\frac{\\partial}{\\partial \\phi_l} [2\\eta^T \\phi_{-j} \\eta^T \\phi_j + (\\eta \\circ \\eta)^T \\phi_j + \\text{const}]\\\\ &amp;= 2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta) \\end{aligned} \\] Substituting this into our original equation, we get \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\phi_{n,i}} &amp;= \\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\lambda \\end{aligned} \\] And then we set this equal to \\(0\\) and solve: \\[ \\begin{aligned} 0 &amp;= \\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\lambda\\\\ \\log \\phi_{n,i} &amp;= \\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - 1 + \\lambda\\\\ \\phi_{n,i} &amp;= \\exp[\\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v} - 1 + \\lambda]\\\\ &amp;= \\exp[- 1 + \\lambda]\\exp[\\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v}]\\\\ &amp;= c_{n,i}\\exp[\\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} \\\\ &amp;\\qquad + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v}]\\\\ &amp;\\propto \\exp[\\frac{y\\eta}{N\\sigma^2} - \\frac{2\\eta^T \\phi_{-j} \\eta + (\\eta \\circ \\eta)}{2N^2 \\sigma^2} + \\Psi &#39; (\\gamma) - \\Psi &#39; (\\sum_j \\gamma_j) + 1[w_n = v] \\log \\beta_{i,v}] \\end{aligned} \\] 12.3.5.3 Optimization for \\(\\eta\\) Again, we’ll isolate the terms involving \\(\\eta\\), but let’s first recall that the only expectation that had \\(\\eta\\) terms was \\(\\mathbb{E}_q [\\log p(y|z,\\eta,\\sigma^2]\\). Also recall that \\(\\eta\\) are regression coefficients where \\(p(y_d | z_{d,n},\\eta,\\sigma^2) \\thicksim \\text{Normal}(\\bar Z \\eta, \\sigma^2)\\), which is the familiar regression model. This should make our lives easier, so let’s use this, which we’ll call \\(g\\) for now: \\[ \\begin{aligned} g(y) &amp;= \\mathbb{E}_q [-\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{(y-\\bar Z \\eta)^2}{2\\sigma^2}] \\end{aligned} \\] Now we’ll rewrite it for all documents (remember that for our previous derivations, we assumed only one document): \\[ \\begin{aligned} g(y_{1:D}) &amp;= \\mathbb{E}_q [\\sum_d [-\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{(y_d-\\bar Z \\eta)^2}{2\\sigma^2}]]\\\\ &amp;= \\mathbb{E}_q [\\sum_d [-\\frac{1}{2} \\log (2 \\pi \\sigma^2)] - \\sum_d [\\frac{(y_d-\\bar Z \\eta)^2}{2\\sigma^2}]]\\\\ &amp;= \\mathbb{E}_q [-\\frac{D}{2} \\log (2 \\pi \\sigma^2) - \\frac{\\sum_d(y_d-\\bar Z \\eta)^2}{2\\sigma^2}]\\\\ &amp;= \\mathbb{E}_q [-\\frac{D}{2} \\log (2 \\pi \\sigma^2) - \\frac{\\sum_d(y_d-\\bar Z \\eta)(y_d-\\bar Z \\eta)}{2\\sigma^2}]\\\\ &amp;= \\mathbb{E}_q [-\\frac{D}{2} \\log (2 \\pi \\sigma^2) - \\frac{\\sum_d(y_d-\\bar Z \\eta)(y_d-\\bar Z \\eta)}{2\\sigma^2}] \\end{aligned} \\] Let’s rewrite this in matrix form where \\(y_{1:D}= y\\): \\[ \\begin{aligned} g( y) &amp;= \\mathbb{E}_q [-\\frac{D}{2} \\log (2 \\pi \\sigma^2) - \\frac{( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)}{2\\sigma^2}]\\\\ &amp;= -\\frac{D}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2}\\mathbb{E}_q [( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)] \\end{aligned} \\] Finally, using \\(g\\), let’s take the partial with respect to \\(\\eta\\) and maximize. This is completely analogous to finding the MLE in a linear regression model: \\[ \\begin{aligned} \\frac{\\partial g}{\\partial \\eta} &amp;= \\frac{\\partial}{\\partial \\eta} [-\\frac{D}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)]\\\\ &amp;= \\frac{\\partial}{\\partial \\eta} [ - \\frac{1}{2\\sigma^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)]\\\\ &amp;= -\\frac{1}{2\\sigma^2}\\mathbb{E}_q[\\frac{\\partial}{\\partial \\eta} ( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)]\\\\ &amp;= -\\frac{1}{2\\sigma^2}\\mathbb{E}_q[( y - \\bar Z \\eta)^T (-\\bar Z) + ( y - \\bar Z \\eta)^T (-\\bar Z)]\\\\ &amp;= -\\frac{1}{2\\sigma^2}\\mathbb{E}_q[-2( y - \\bar Z \\eta)^T \\bar Z]\\\\ &amp;= \\frac{1}{\\sigma^2}\\mathbb{E}_q[( y - \\bar Z \\eta)^T \\bar Z]\\\\ &amp;= \\frac{1}{\\sigma^2}\\mathbb{E}_q[( y^T - (\\bar Z \\eta)^T) \\bar Z]\\\\ &amp;= \\frac{1}{\\sigma^2}\\mathbb{E}_q[( y^T - \\eta^T \\bar Z^T) \\bar Z]\\\\ &amp;= \\frac{1}{\\sigma^2}\\mathbb{E}_q[( y^T \\bar Z - \\eta^T \\bar Z^T \\bar Z)]\\\\ &amp;= \\frac{1}{\\sigma^2}( y^T \\mathbb{E}_q[\\bar Z] - \\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z]) \\end{aligned} \\] Setting this equal to zero, we have \\[ \\begin{aligned} 0 &amp;= \\frac{1}{\\sigma^2}( y^T \\mathbb{E}_q[\\bar Z] - \\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z])\\\\ 0 &amp;= y^T \\mathbb{E}_q[\\bar Z] - \\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z]\\\\ \\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z] &amp;= y^T \\mathbb{E}_q[\\bar Z]\\\\ \\eta^T &amp;= y^T \\mathbb{E}_q[\\bar Z] (\\mathbb{E}_q[\\bar Z^T \\bar Z])^{-1}\\\\ \\eta &amp;= ( y^T \\mathbb{E}_q[\\bar Z] (\\mathbb{E}_q[\\bar Z^T \\bar Z])^{-1})^T\\\\ \\eta &amp;= ( y^T \\mathbb{E}_q[\\bar Z])^T ((\\mathbb{E}_q[\\bar Z^T \\bar Z])^{-1})^T\\\\ \\eta &amp;= \\mathbb{E}_q[\\bar Z^T \\bar Z]^{-1} \\mathbb{E}_q[\\bar Z]^T y\\\\ &amp;= (\\frac{1}{N^2} (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)))^{-1} (\\frac{1}{N} \\sum_n \\phi_n)^T y \\end{aligned} \\] 12.3.5.4 Optimization for \\(\\sigma^2\\) For the dispersion parameter, since it’s only related to the regression part of our model, we’ll reuse our \\(g( y)\\) function, but we’ll define \\(\\delta := \\sigma^2\\) just so notation is easier to follow. \\[ \\begin{aligned} \\frac{\\partial g}{\\partial \\delta} &amp;= \\frac{\\partial}{\\partial \\delta} [-\\frac{D}{2} \\log (2 \\pi \\delta) - \\frac{1}{2\\delta}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)]\\\\ &amp;= -\\frac{D}{2\\delta} + \\frac{1}{2\\delta^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta) \\end{aligned} \\] Setting this equal to zero, we get \\[ \\begin{aligned} 0 &amp;= -\\frac{D}{2\\delta} + \\frac{1}{2\\delta^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)\\\\ &amp;= \\frac{1}{2}[-\\frac{D}{\\delta} + \\frac{1}{\\delta^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)]\\\\ &amp;= -\\frac{D}{\\delta} + \\frac{1}{\\delta^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)\\\\ \\frac{D}{\\delta} &amp;= \\frac{1}{\\delta^2}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)\\\\ D &amp;= \\frac{1}{\\delta}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)\\\\ \\delta &amp;= \\frac{1}{D}\\mathbb{E}_q( y-\\bar Z \\eta)^T( y-\\bar Z \\eta) \\end{aligned} \\] And now we’ll expand. \\[ \\begin{aligned} \\delta &amp;= \\frac{1}{D}\\mathbb{E}_q[( y-\\bar Z \\eta)^T( y-\\bar Z \\eta)]\\\\ &amp;= \\frac{1}{D}\\mathbb{E}_q[ y^T y - y^T \\bar Z \\eta - \\eta^T \\bar Z^T y + \\eta^T \\bar Z^T \\bar Z \\eta]\\\\ &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\eta - \\eta^T \\mathbb{E}_q[\\bar Z^T] y + \\eta^T \\mathbb{E}_q[\\bar Z^T \\bar Z] \\eta) \\end{aligned} \\] Now, in our optimization for \\(\\eta\\), we saw: \\[ \\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z] = y^T \\mathbb{E}_q[\\bar Z] \\] Which can be rewritten as \\[ \\begin{aligned} \\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z] &amp;= y^T \\mathbb{E}_q[\\bar Z]\\\\ (\\eta^T\\mathbb{E}_q[\\bar Z^T \\bar Z])^T &amp;= ( y^T \\mathbb{E}_q[\\bar Z])^T\\\\ \\mathbb{E}_q[\\bar Z^T \\bar Z] \\eta &amp;= \\mathbb{E}_q[\\bar Z]^T y \\end{aligned} \\] Now let’s substitute this into our equation for \\(\\delta\\) \\[ \\begin{aligned} \\delta &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\eta - \\eta^T \\mathbb{E}_q[\\bar Z^T] y + \\eta^T \\mathbb{E}_q[\\bar Z^T \\bar Z] \\eta)\\\\ &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\eta - \\eta^T \\mathbb{E}_q[\\bar Z^T] y + \\eta^T \\mathbb{E}_q[\\bar Z]^T y)\\\\ &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\eta) \\end{aligned} \\] We know that \\(\\eta = \\mathbb{E}_q[\\bar Z^T \\bar Z]^{-1} \\mathbb{E}_q[\\bar Z]^T y\\), giving us \\[ \\begin{aligned} \\delta &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\eta)\\\\ &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\mathbb{E}_q[\\bar Z^T \\bar Z]^{-1} \\mathbb{E}_q[\\bar Z]^T y) \\end{aligned} \\] Finally, we’ll replace \\(\\delta\\) with \\(\\sigma^2\\): \\[ \\begin{aligned} \\sigma^2 &amp;= \\frac{1}{D}( y^T y - y^T \\mathbb{E}_q[\\bar Z] \\mathbb{E}_q[\\bar Z^T \\bar Z]^{-1} \\mathbb{E}_q[\\bar Z]^T y)\\\\ &amp;= \\frac{1}{D}( y^T y - y^T (\\frac{1}{N} \\sum_n \\phi_n) (\\sum_n \\sum_{n \\neq m} \\phi_n \\phi_m^T + \\sum_n \\text{diag}(\\phi_n)))^{-1} (\\frac{1}{N} \\sum_n \\phi_n)^T y) \\end{aligned} \\] 12.4 The Correlated Topic Model This model is similar to LDA, except we replace the Dirichlet distribution of topics over documents with a logistic Normal distribution with parameters \\(\\mu\\) and \\(\\Sigma\\) of length \\(K\\) and dimensions \\(K \\times K\\), respectively. The generative model is the following: The joint distribution over all latent variables \\(\\theta\\) and \\(z\\) and observed data \\(w\\) is then \\[ p(\\theta,z,w|\\alpha,\\beta) = \\prod_d p(\\eta_d|\\mu,\\Sigma) \\prod_n p(w_{d,n}|z_{d,n},\\beta) p(z_{d,n}|\\eta_d) \\] where \\[ \\begin{aligned} &amp;p(\\eta_d|\\mu,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k |\\Sigma|}} \\exp [-\\frac{1}{2}(\\eta - \\mu)^T \\Sigma^{-1} (\\eta - \\mu)] \\text{, (Multivariate Normal)}\\\\ &amp;p(z_{d,n}|\\eta_d) = \\prod_n \\prod_i \\theta_i^{1[z_n=i]} \\text{, (Multinomial)}\\\\ &amp;p(w_{d,n}|z_{d,n},\\beta) = \\prod_v \\prod_i \\beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]} \\text{, (Multinomial)} \\end{aligned} \\] 12.4.1 Multinomial Distribution in Exponential Form Let’s convert this multinomial \\(p(z|\\eta)\\) into its natural parameter form by exploiting the fact that it belongs to the exponential family. The game is that we need to express this distribution in the form \\(p(x|\\eta) = h(z) \\exp [\\eta^T \\phi (x) - A(\\eta)]\\). Note, we’ll work with only one word \\(n\\) to simplify things, so \\[ \\begin{aligned} p(z|\\theta) &amp;= \\prod_i^K \\theta_i^{1[z_n=i]}\\\\ &amp;= \\prod_i^K \\exp [\\log \\theta_i^{1[z_n=i]}]\\\\ &amp;= \\exp [\\sum_i^K \\log \\theta_i^{1[z_n=i]}]\\\\ &amp;= \\exp [\\sum_i^K 1[z_n=i] \\log \\theta_i] \\end{aligned} \\] Now we’ll split the sum from all \\(\\sum_i^K\\) to \\(\\sum_i^{K-1} + 1-\\sum_i^{K-1}\\): \\[ \\begin{aligned} p(z|\\theta) &amp;= \\exp [\\sum_i 1[z_n=i] \\log \\theta_i]\\\\ &amp;= \\exp [\\sum_i^{K-1} 1[z_n=i] \\log \\theta_i + (1-\\sum_i^{K-1} 1[z_n=i]) \\log (1-\\sum_i^{K-1}\\theta_i)] \\end{aligned} \\] Then we expand and group terms: \\[ \\begin{aligned} p(z|\\theta) &amp;= \\exp [\\sum_i^{K-1} 1[z_n=i] \\log \\theta_i + \\log (1-\\sum_i^{K-1}\\theta_i) -\\sum_i^{K-1} 1[z_n=i] \\log (1-\\sum_i^{K-1}\\theta_i)]\\\\ &amp;= \\exp [\\sum_i^{K-1} 1[z_n=i] (\\log \\theta_i - \\log (1-\\sum_i^{K-1}\\theta_i)) + \\log (1-\\sum_i^{K-1}\\theta_i)]\\\\ &amp;= \\exp [\\sum_i^{K-1} 1[z_n=i] \\log (\\frac{\\theta_i}{1-\\sum_j^{K-1}\\theta_j}) + \\log (1-\\sum_i^{K-1}\\theta_i)]\\\\ \\end{aligned} \\] We can finally see the exponential form taking shape where \\[ \\eta_i = \\log(\\frac{\\theta_i}{1-\\sum_j^{K-1}\\theta_j}) = \\log(\\frac{\\theta_i}{\\theta_i}) \\] Now we can determine \\(\\theta_i\\) by expressing this equation in terms of \\(\\eta_i\\): \\[ \\begin{aligned} \\eta_i &amp;= \\log (\\frac{\\theta_i}{\\theta_i})\\\\ \\exp [\\eta_i] &amp;= \\frac{\\theta_i}{\\theta_i}\\\\ \\theta_i &amp;= \\theta_i \\exp [\\eta_i]\\\\ &amp;= (1-\\sum_j^{K-1}\\theta_j) \\exp [\\eta_i]\\\\ &amp;= \\frac{1}{\\frac{1}{(1-\\sum_j^{K-1}\\theta_j)}} \\exp [\\eta_i] \\end{aligned} \\] For convenience, we’ll assume \\(\\theta_i=\\), so \\(\\sum_i^{K-1}\\theta_i = \\sum_i^{K-1}\\theta_i + \\theta_i = \\sum_i^{K-1}\\theta_i + 0 = \\sum_i^{K}\\theta_i\\). \\[ \\begin{aligned} \\theta_i &amp;= \\frac{1}{\\frac{1}{(1-\\sum_j^{K-1}\\theta_j)}} \\exp [\\eta_i]\\\\ &amp;= \\frac{1}{\\frac{1}{(1-\\sum_j^{K}\\theta_j)}} \\exp [\\eta_i] \\end{aligned} \\] Then we can take advantage of the following constraint: \\(\\sum_j^K \\theta_j = 1\\), so \\[ \\begin{aligned} \\theta_i &amp;= \\frac{1}{\\frac{1}{(1-\\sum_j^{K}\\theta_j)}} \\exp [\\eta_i]\\\\ &amp;= \\frac{1}{\\frac{\\sum_j^K \\theta_j}{(1-\\sum_j^{K}\\theta_j)}} \\exp [\\eta_i]\\\\ &amp;= \\frac{1}{\\sum_j^K\\frac{ \\theta_j}{(1-\\sum_j^{K}\\theta_j)}} \\exp [\\eta_i]\\\\ &amp;= \\frac{1}{\\sum_j^K \\exp[\\log \\frac{ \\theta_j}{(1-\\sum_j^{K}\\theta_j)}]} \\exp [\\eta_i]\\\\ &amp;= \\frac{1}{\\sum_j^K \\exp[\\log \\frac{ \\theta_j}{\\theta_i}]} \\exp [\\eta_i]\\\\ &amp;= \\frac{\\exp [\\eta_i]}{\\sum_j^K \\exp[\\eta_j]}\\\\ \\end{aligned} \\] which is the softmax function. Now we can extract our natural parameters: \\[ \\begin{aligned} h(z) &amp;= 1\\\\ \\phi(x) &amp;= [1[z_n=1], ..., 1[z_n=K-1]]\\\\ \\eta &amp;= [\\log(\\frac{\\theta_1}{1-\\sum_j^{K-1}\\theta_j}), ..., \\log(\\frac{\\theta_{K-1}}{1-\\sum_j^{K-1}\\theta_j}),0]\\\\ &amp;= [\\log(\\frac{\\theta_1}{\\theta^K}),...,\\log(\\frac{\\theta_{K-1}}{\\theta^K}),0] \\end{aligned} \\] We’ll use the following notation \\(\\phi (x) := z\\). For the cumulant, let’s again use \\(\\eta_i = \\log (\\frac{\\theta_i}{\\theta_i})\\): \\[ \\begin{aligned} \\eta_i &amp;= \\log (\\frac{\\theta_i}{\\theta_i})\\\\ \\theta_i &amp;= \\theta_i \\exp [\\eta_i]\\\\ \\sum_i^{K-1} \\theta_i &amp;= \\theta_i \\sum_i^{K-1} \\exp [\\eta_i]\\\\ 1-(1-\\sum_i^{K-1} \\theta_i) &amp;= \\theta_i \\sum_i^{K-1} \\exp [\\eta_i]\\\\ 1-\\theta_i &amp;= \\theta_i \\sum_i^{K-1} \\exp [\\eta_i]\\\\ 1 &amp;= \\theta_i (1+\\sum_i^{K-1} \\exp [\\eta_i]\\\\ \\theta_i &amp;= \\frac{1}{1+\\sum_i^{K-1} \\exp [\\eta_i]}\\\\ 1-\\sum_i^{K-1} \\theta_i &amp;= \\frac{1}{1+\\sum_i^{K-1} \\exp [\\eta_i]} \\end{aligned} \\] Therefore \\[ \\begin{aligned} A(\\eta) &amp;= -\\log (1-\\sum_i^{K-1} \\theta_i)\\\\ &amp;= -\\log (\\frac{1}{1+\\sum_i^{K-1} \\exp [\\eta_i]})\\\\ &amp;= \\log (1+\\sum_i^{K-1} \\exp [\\eta_i])\\\\ \\end{aligned} \\] And if we assume that \\(\\theta_i = 0\\), we have \\[ \\begin{aligned} A(\\eta) &amp;= \\log (1+\\sum_i^{K-1} \\exp [\\eta_i])\\\\ &amp;= \\log (\\sum_i^{K} \\exp [\\eta_i])\\\\ \\end{aligned} \\] Thus, \\(p(z|\\eta)\\) in its natural paramterization is \\[ \\begin{aligned} p(z_n|\\eta) &amp;= 1 \\times \\exp [\\eta^T z_n - A(\\eta)]\\\\ &amp;= \\exp [\\eta^T z_n - \\log (\\sum_i^{K} \\exp [\\eta_i])] \\end{aligned} \\] 12.4.2 Variational EM Now, our variational distribution for CTM will be a again be a mean field distribution where we assume complete independence between each \\(\\eta\\) and between all topic assignments \\(z\\), i.e., a fully factored form. Note, unlike before, we’re showing the variational distribution for only one document: \\[ q(\\eta,z|\\lambda,\\nu,\\phi) = \\prod_i q(\\eta_i|\\lambda_i,\\nu_i^2) \\prod_n q(z_n|\\phi_n) \\] where \\(\\phi\\) is \\(K \\times N\\), as before, and \\(\\eta_i \\thicksim \\text{Normal}(\\gamma_i,\\nu_i)\\) – that is, each \\(\\eta_i\\) is distributed by its own univariate Gaussian. For the joint distribution over latent variables, we have \\[ \\begin{aligned} \\log p&amp;(w|\\mu, \\Sigma, \\beta) \\geq L(\\gamma,\\nu,\\phi|\\mu, \\Sigma, \\beta) \\\\ &amp;= \\mathbb{E}_q [\\log p(\\eta,z,w|\\mu, \\Sigma, \\beta)] - \\mathbb{E}_q [\\log q(\\eta,z)] \\\\ &amp;= \\mathbb{E}_q [\\log p(\\eta|\\mu,\\Sigma)p(z|\\theta)p(w|z,\\beta)] - \\mathbb{E}_q [\\log q(\\eta)q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\eta|\\mu,\\Sigma) + \\log p(z|\\eta) + \\log p(w|z,\\beta)] - \\mathbb{E}_q [\\log q(\\eta) + \\log q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\eta|\\mu,\\Sigma)] + \\mathbb{E}_q [\\log p(z|\\eta)] + \\mathbb{E}_q [\\log p(w|z,\\beta)] \\\\ &amp;\\qquad - \\mathbb{E}_q [\\log q(\\eta)] - \\mathbb{E}_q[\\log q(z)]\\\\ &amp;= \\mathbb{E}_q [\\log p(\\eta|\\mu,\\Sigma)] + \\mathbb{E}_q [\\log p(z|\\eta)] + \\mathbb{E}_q [\\log p(w|z,\\beta)] + \\mathbb{H}_q [\\lambda,\\nu^2] + \\mathbb{H}_q[\\phi] \\end{aligned} \\] 12.4.3 Expectation of \\(p(w|z,\\beta)\\) This is analogous to LDA: \\[ \\mathbb{E}_q [\\log p(w|z,\\beta)] = \\sum_v \\sum_i 1[w_{d,n}=v] \\phi_{n,i} \\log \\beta_{i,v} \\] 12.4.4 Expectation of \\(p(z|\\eta)\\) In LDA, working with \\(p(z|\\theta)\\) was simple because of the conjugacy between Multinomial and Dirichlet distributions, respectively. Now, however, we are working with a Normal prior, which is not conjugate with the Multinomial. Similar to before, \\(z_n \\thicksim \\text{Multinomial}(f(\\eta))\\). Now recall that \\(\\eta_i \\thicksim \\text{Normal}(\\gamma_i,\\nu_i)\\) and then mapped onto the simplex via \\(f(\\eta_i)= \\exp \\eta_i / \\sum_j \\exp \\eta_j\\). You can see this relationship by returning to the natural parameterization of the Multinomial distribution \\(p(z|\\eta)\\): \\[ \\begin{aligned} \\log p(z|\\eta) &amp;= \\log \\exp [\\eta^T z_n - \\log (\\sum_i^{K} \\exp [\\eta_i])]\\\\ &amp;= \\eta^T z_n - \\log (\\sum_i^{K} \\exp [\\eta_i])\\\\ &amp;= \\log \\exp [\\eta^T z_n] - \\log (\\sum_i^{K} \\exp [\\eta_i])\\\\ &amp;= \\log [\\frac{\\exp [\\eta^T z_n]}{\\sum_i^{K} \\exp [\\eta_i]}]\\\\ \\end{aligned} \\] which shows \\(\\eta\\) being mapped to the simplex. Now, let’s derive the expectation: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z_n|\\eta)] &amp;= \\mathbb{E}_q [\\log \\exp [\\eta^T z_n - \\log(\\sum_i^{K} \\exp [\\eta_i])]] \\\\ &amp;= \\mathbb{E}_q [\\eta^T z_n - \\log(\\sum_i^{K} \\exp [\\eta_i])] \\\\ &amp;= \\mathbb{E}_q [\\eta^T z_n] - \\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])] \\\\ \\end{aligned} \\] The left side is easy. Recall that \\(z_n\\) are simply indicator functions for a particular topic assignment, so for a given document and word \\(n\\), \\(z_n\\) is a K-vector of indicator values where the topic \\(k\\) is equal to \\(1\\) and all other topics are \\(0\\). Its expectation is the corresponding variational parameter \\(\\phi_{n,i}\\), which is simply a matrix of values that reflects the frequency in which a token \\(n\\) takes on the topic assignment \\(i\\). Because \\(\\eta_i\\) is a Gaussian with mean \\(\\lambda_i\\) that corresponds to topic \\(i\\), its expectation is simply \\(\\lambda\\). Thus, the expectation of \\(\\eta^T z_n\\) is simply the sum across tokens of the product between the average prior value \\(\\lambda_i\\) for token \\(i\\) and the number of times token \\(n\\) was assigned to topic \\(i\\): \\[ \\begin{aligned} \\mathbb{E}_q [\\eta^T z_n] = \\sum_n \\sum_i \\lambda_i \\phi_{n,i} \\\\ \\end{aligned} \\] Giving us \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z_n|\\eta)] &amp;= \\mathbb{E}_q [\\eta^T z_n] - \\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])]\\\\ &amp;= \\sum_n \\sum_i \\lambda_i \\phi_{n,i} - \\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])] \\end{aligned} \\] The right side, on the other hand, is intractable, but we can introduce an upper bound to \\(\\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])]\\). Think about it like this. For variational inference, we are maximizing a lower bound of the log probability of our model. The term we are focusing on is \\(\\mathbb{E}_q [\\eta^T z_n] - \\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])]\\), which we’ll rewrite as \\(\\mathbb{E}_q [A - B]\\). We need to ensure that our lower bound from the variational distribution remains the lower bound. Therefore, our approximation of \\(\\mathbb{E}_q [A - B]\\) must be smaller, which would give us a smaller approximated lower bound and hence smaller than the true lower bound. Since \\(B\\) is the part that is intractable, we only need to approximate it, and not the left term. If we increase \\(B\\), we’ll consequently decrease our approximation of \\(\\mathbb{E}_q [\\log p(z_n|\\eta)]\\), giving us an approximated lower bound that’s smaller than the true lower bound, which is what we want. Therefore, we need to find \\(C\\) such that \\(\\mathbb{E}_q [A - B] \\leq C\\). To find “\\(C\\)”, we do the following. We’ll approximate the intractable sum as \\(\\log x \\approx x-1\\), which is the case when \\(x\\) is around \\(1\\). Also, \\(\\log x\\) is never greater than \\(x-1\\) and hence \\(x-1\\) serves as a tight upper bound: We will also introduce a new variational parameter \\(\\xi\\). \\[ \\begin{aligned} \\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])] &amp;= \\mathbb{E}_q [\\log(\\xi \\xi^{-1} \\sum_i^{K} \\exp [\\eta_i])]\\\\ &amp;= \\mathbb{E}_q [\\log \\xi + \\log (\\xi^{-1} \\sum_i^{K} \\exp [\\eta_i])]\\\\ &amp;= \\log \\xi + \\mathbb{E}_q [\\log (\\xi^{-1} \\sum_i^{K} \\exp [\\eta_i])]\\\\ &amp;\\leq \\log \\xi + \\mathbb{E}_q [\\xi^{-1} \\sum_i^{K} \\exp [\\eta_i] - 1]\\\\ &amp;\\leq \\log \\xi + \\xi^{-1} \\sum_i^{K} \\mathbb{E}_q [\\exp [\\eta_i]] - 1\\\\ \\end{aligned} \\] And so we have our almost final form for this expectation: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z_n|\\eta)] &amp;= \\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - \\mathbb{E}_q [\\log(\\sum_i^{K} \\exp [\\eta_i])])\\\\ &amp;=\\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - \\log \\xi + \\mathbb{E}_q [\\log (\\xi^{-1} \\sum_i^{K} \\exp [\\eta_i])])\\\\ &amp;\\geq \\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - (\\log \\xi + \\xi^{-1} \\sum_i^{K} \\mathbb{E}_q [\\exp [\\eta_i]] - 1)) \\end{aligned} \\] Now, because \\(\\eta_i \\thicksim \\text{Normal} (\\lambda_i,\\nu_i^2)\\), \\(\\mathbb{E}_q [\\exp [\\eta_i]] = \\exp [\\lambda_i + \\nu_i^2 /2]\\), which allows us to write our final form: \\[ \\begin{aligned} \\mathbb{E}_q [\\log p(z_n|\\eta)] &amp;\\geq \\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - (\\log \\xi + \\xi^{-1} \\sum_i^{K} \\mathbb{E}_q [\\exp [\\eta_i]] - 1))\\\\ &amp;\\geq \\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - (\\log \\xi + \\xi^{-1} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}] - 1))\\\\ &amp;\\geq \\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - \\log \\xi - \\xi^{-1} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}] + 1) \\end{aligned} \\] 12.4.5 Expectation of \\(p(\\eta|\\mu,\\sigma)\\) We know that \\(p(\\eta|\\mu,\\Sigma) \\thicksim \\text{Normal}_i (\\mu,\\Sigma)\\), so \\[ \\begin{aligned} \\log p(\\eta|\\mu,\\Sigma) &amp;= \\frac{1}{2} \\log |\\Sigma^{-1}| - \\frac{K}{2} \\log 2\\pi - \\frac{1}{2} \\mathbb{E}_q [(\\eta - \\mu)^T \\Sigma^{-1} (\\eta - \\mu)] \\end{aligned} \\] We can rewrite \\((\\eta - \\mu)^T \\Sigma^{-1} (\\eta - \\mu)\\) in a more manageable form. Using \\(x^T A x = {\\text{tr}}(x^T A x) = {\\text{tr}}(A x^T x)\\), we have the following: \\[ \\begin{aligned} (\\eta - \\mu)^T \\Sigma^{-1} (\\eta - \\mu) &amp;= {\\text{tr}}(\\Sigma^{-1} (\\eta - \\mu)^T (\\eta - \\mu))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1} \\sum_i (\\eta_i - \\mu) (\\eta_i - \\mu))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1} \\sum_i (\\eta_i + (\\lambda_i - \\lambda_i) - \\mu) (\\eta_i + (\\lambda_i - \\lambda_i) - \\mu))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1} \\sum_i (\\eta_i + \\lambda_i - \\lambda_i - \\mu) (\\eta_i + \\lambda_i - \\lambda_i - \\mu))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1} (\\sum_i (\\eta_i - \\lambda_i)(\\eta_i - \\lambda_i ) + \\sum_i (\\lambda_i - \\mu)(\\lambda_i - \\mu)))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1}\\sum_i (\\eta_i - \\lambda_i)(\\eta_i - \\lambda_i ) + \\Sigma^{-1}\\sum_i (\\lambda_i - \\mu)(\\lambda_i - \\mu))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1}\\sum_i (\\eta_i - \\lambda_i)(\\eta_i - \\lambda_i )) + {\\text{tr}}(\\Sigma^{-1}\\sum_i (\\lambda_i - \\mu)(\\lambda_i - \\mu))\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1}\\sum_i (\\eta_i - \\lambda_i)^2) + {\\text{tr}}(\\Sigma^{-1}(\\lambda - \\mu)(\\lambda - \\mu)^T)\\\\ &amp;= {\\text{tr}}(\\Sigma^{-1}\\sum_i (\\eta_i - \\lambda_i)^2) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) \\end{aligned} \\] Now, let’s take the expectation: \\[ \\begin{aligned} \\mathbb{E}_q [(\\eta - \\mu)^T &amp;\\Sigma^{-1} (\\eta - \\mu)] \\\\ &amp;= \\mathbb{E}_q [{\\text{tr}}(\\Sigma^{-1}\\sum_i (\\eta_i - \\lambda_i)^2) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu)] \\\\ &amp;= {\\text{tr}}(\\Sigma^{-1} \\mathbb{E}_q[\\sum_i (\\eta_i - \\lambda_i)^2]) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) \\end{aligned} \\] where \\(\\mathbb{E}_q[\\sum_i (\\eta_i - \\lambda_i)^2]\\) is simply the variance of \\(eta\\) – i.e., \\(\\nu^2\\) – which we’ll place on the diagonal of a square matrix. \\[ \\begin{aligned} \\mathbb{E}_q [(\\eta - \\mu)^T &amp;\\Sigma^{-1} (\\eta - \\mu)] \\\\ &amp;= {\\text{tr}}(\\Sigma^{-1} \\mathbb{E}_q[\\sum_i (\\eta_i - \\lambda_i)^2]) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) \\\\ &amp;= {\\text{tr}}(\\text{diag}(\\nu^2)\\Sigma^{-1}) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) \\end{aligned} \\] Therefore, \\[ \\begin{aligned} \\log p(\\eta|\\mu,\\Sigma) &amp;= \\frac{1}{2} \\log |\\Sigma^{-1}| - \\frac{K}{2} \\log 2\\pi - \\frac{1}{2} \\mathbb{E}_q [(\\eta - \\mu)^T \\Sigma^{-1} (\\eta - \\mu)] \\\\ &amp;= \\frac{1}{2} \\log |\\Sigma^{-1}| - \\frac{K}{2} \\log 2\\pi - \\frac{1}{2} ({\\text{tr}}(\\text{diag}(\\nu^2)\\Sigma^{-1}) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu))\\\\ &amp;= \\frac{1}{2} \\log |\\Sigma^{-1}| - \\frac{K}{2} \\log 2\\pi - \\frac{1}{2} ({\\text{tr}}(\\text{diag}(\\nu^2)\\Sigma^{-1}) + (\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu))\\\\ &amp;= \\frac{1}{2} \\log |\\Sigma^{-1}| - \\frac{K}{2} \\log 2\\pi - \\frac{1}{2} {\\text{tr}}(\\text{diag}(\\nu^2)\\Sigma^{-1}) - \\frac{1}{2}(\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) \\end{aligned} \\] 12.4.6 Entropy of \\(\\lambda\\), \\(\\nu\\), and \\(\\phi\\) Again, we can look these up. \\[ \\begin{aligned} \\mathbb{H}_q [\\lambda,\\nu^2] &amp;= \\frac{1}{2} \\sum_i \\log[2 \\pi e \\nu_i^2] \\\\ &amp;= \\frac{1}{2} \\sum_i (\\log 2\\pi + 1 + \\log \\nu_i^2)\\\\ \\mathbb{H}_q [\\phi_{d,n}] &amp;= -\\sum_i \\phi_{d,n,i} \\log \\phi_{d,n,i} \\end{aligned} \\] 12.4.7 Complete Objective Function Our objective function is then \\[ \\begin{aligned} \\mathbb{E}_q &amp;[\\log p(\\eta,z,w|\\mu, \\Sigma, \\beta)] - \\mathbb{E}_q [\\log q(\\eta,z)] \\\\ &amp;= \\mathbb{E}_q [\\log p(\\eta|\\mu,\\Sigma)] + \\mathbb{E}_q [\\log p(z|\\eta)] + \\mathbb{E}_q [\\log p(w|z,\\beta)] + \\mathbb{H}_q [\\lambda,\\nu^2] + \\mathbb{H}_q[\\phi] \\\\ &amp;= \\frac{1}{2} \\log |\\Sigma^{-1}| - \\frac{K}{2} \\log 2\\pi - \\frac{1}{2} {\\text{tr}}(\\text{diag}(\\nu^2)\\Sigma^{-1}) - \\frac{1}{2}(\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) \\\\ &amp;\\qquad + \\sum_n (\\sum_i \\lambda_i \\phi_{n,i} - \\log \\xi - \\xi^{-1} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}] + 1) \\\\ &amp;\\qquad + \\sum_v \\sum_i 1[w_{d,n}=v] \\phi_{n,i} \\log \\beta_{i,v} \\\\ &amp;\\qquad + \\frac{1}{2} \\sum_i (\\log 2\\pi + 1 + \\log \\nu_i^2) - \\sum_i \\phi_{d,n,i} \\log \\phi_{d,n,i} \\end{aligned} \\] 12.4.8 Parameter Optimization 12.4.8.1 Optimization for \\(\\beta\\) This is the same as LDA: \\[ \\beta_{i,v} \\propto \\sum_n \\sum_i \\sum_v 1[w_n = v] \\phi_{n,i} \\] 12.4.8.2 Optimization for \\(\\mu\\)} Extracting the terms involving \\(\\mu\\), we have \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\mu} &amp;= \\frac{\\partial}{\\partial \\mu}[- \\frac{1}{2} \\sum_d (\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)] \\\\ &amp;= \\frac{\\partial}{\\partial \\mu}[- \\frac{1}{2}\\sum_d (\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)] \\\\ &amp;= (-\\frac{1}{2})(-2) \\Sigma^{-1}\\sum_d(\\lambda_d-\\mu)\\\\ &amp;= \\Sigma^{-1}\\sum_d(\\lambda_d-\\mu) \\end{aligned} \\] Set it to zero. \\[ \\begin{aligned} 0 &amp;= \\Sigma^{-1}\\sum_d(\\lambda_d-\\mu)\\\\ &amp;= \\sum_d \\lambda_d - \\sum_d \\mu \\\\ &amp;= \\sum_d \\lambda_d - D \\mu \\\\ D \\mu &amp;= \\sum_d \\lambda_d \\\\ \\mu &amp;= \\frac{1}{D} \\sum_d \\lambda_d \\end{aligned} \\] 12.4.8.3 Optimization for \\(\\Sigma\\) Let’s repeat the process, but for terms associated with \\(\\Sigma\\): \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\Sigma} &amp;= \\frac{\\partial}{\\partial \\Sigma}[\\sum_d(-\\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} {\\text{tr}}(\\text{diag}(\\nu_d^2)\\Sigma^{-1}) - \\frac{1}{2}(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu))] \\\\ &amp;= -\\frac{1}{2}\\frac{\\partial}{\\partial \\Sigma} [\\sum_d (\\log |\\Sigma| + {\\text{tr}}(\\text{diag}(\\nu_d^2)\\Sigma^{-1}) + (\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu))] \\\\ &amp;= -\\frac{1}{2}\\frac{\\partial}{\\partial \\Sigma} [D \\log |\\Sigma| + {\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2)\\Sigma^{-1}) + \\sum_d(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)] \\\\ &amp;= -\\frac{1}{2}(D \\frac{\\partial}{\\partial \\Sigma}[\\log |\\Sigma|] + \\frac{\\partial}{\\partial \\Sigma}[{\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2)\\Sigma^{-1})] \\\\ &amp;\\qquad + \\frac{\\partial}{\\partial \\Sigma}[\\sum_d(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)]) \\end{aligned} \\] Let’s first focus on \\(\\frac{\\partial}{\\partial \\Sigma} [\\sum_d(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)]\\). We can use the trace trick we saw above to make this derivative easier to work with: \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\Sigma} &amp;[\\sum_d(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)] \\\\ &amp;= \\frac{\\partial}{\\partial \\Sigma} [{\\text{tr}}(\\sum_d(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu))]\\\\ &amp;= \\frac{\\partial}{\\partial \\Sigma} [{\\text{tr}}(\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T\\Sigma^{-1})]\\\\ &amp;= {\\text{tr}}(\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T\\frac{\\partial}{\\partial \\Sigma}\\Sigma^{-1})\\\\ &amp;= {\\text{tr}}(\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T (-\\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} ))\\\\ &amp;= {\\text{tr}}(-\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}) \\end{aligned} \\] Now, let’s deal with \\(\\frac{\\partial}{\\partial \\Sigma}[\\log |\\Sigma|]\\). \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\Sigma}[\\log |\\Sigma|] &amp;= {\\text{tr}}(\\Sigma^{-1} \\partial \\Sigma) \\end{aligned} \\] And finally, \\(\\frac{\\partial}{\\partial \\Sigma}[{\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2)\\Sigma^{-1})]\\). \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\Sigma}[{\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2)\\Sigma^{-1})] &amp;= {\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2) (-\\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} ))\\\\ &amp;= {\\text{tr}}(-\\sum_d\\text{diag}(\\nu_d^2) (\\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} )) \\end{aligned} \\] Now let’s replace these derivatives in our original equation. \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\Sigma} &amp;= -\\frac{1}{2}(D \\frac{\\partial}{\\partial \\Sigma}[\\log |\\Sigma|] + \\frac{\\partial}{\\partial \\Sigma}[{\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2)\\Sigma^{-1})] \\\\ &amp;\\qquad + \\frac{\\partial}{\\partial \\Sigma}[\\sum_d(\\lambda_d - \\mu)^T\\Sigma^{-1}(\\lambda_d - \\mu)])\\\\ &amp;= -\\frac{1}{2}(D {\\text{tr}}(\\Sigma^{-1} \\partial \\Sigma) + {\\text{tr}}(-\\sum_d\\text{diag}(\\nu_d^2) (\\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} )) \\\\ &amp;\\qquad + {\\text{tr}}(-\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}))\\\\ &amp;= -\\frac{1}{2}({\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma) + {\\text{tr}}(-\\sum_d\\text{diag}(\\nu_d^2) (\\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} )) \\\\ &amp;\\qquad + {\\text{tr}}(-\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}))\\\\ &amp;= -\\frac{1}{2}({\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma - \\sum_d\\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} \\\\ &amp;\\qquad - \\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}))\\\\ \\end{aligned} \\] Then we set this to zero and solve. \\[ \\begin{aligned} 0 &amp;= -\\frac{1}{2}({\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma - \\sum_d\\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} \\\\ &amp;\\qquad - \\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}))\\\\ &amp;= {\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma - \\sum_d\\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1} \\\\ &amp;\\qquad - \\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1})\\\\ &amp;= {\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma) - {\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}) \\\\ &amp;\\qquad - {\\text{tr}}(\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1})\\\\ &amp;= {\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma) - {\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}) \\\\ &amp;\\qquad - {\\text{tr}}(\\sum_d(\\lambda_d - \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1})\\\\ \\end{aligned} \\] Then we exploit \\({\\text{tr}}(ABCD) = {\\text{tr}}(DABC)\\). \\[ \\begin{aligned} {\\text{tr}}(D\\Sigma^{-1} \\partial \\Sigma) &amp;= {\\text{tr}}(\\sum_d\\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1}) + {\\text{tr}}(\\sum_d(\\lambda_d + \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma \\Sigma^{-1})\\\\ &amp;= {\\text{tr}}(\\sum_d \\Sigma^{-1}) \\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma + {\\text{tr}}(\\sum_d \\Sigma^{-1} (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma)\\\\ D\\Sigma^{-1} \\partial \\Sigma &amp;= \\sum_d \\Sigma^{-1} \\text{diag}(\\nu_d^2) \\Sigma^{-1}\\partial \\Sigma + \\sum_d \\Sigma^{-1} (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\partial \\Sigma\\\\ D\\Sigma^{-1} \\partial \\Sigma &amp;= (\\sum_d \\Sigma^{-1} \\text{diag}(\\nu_d^2) \\Sigma^{-1} + \\sum_d \\Sigma^{-1} (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1})\\partial \\Sigma\\\\ D\\Sigma^{-1} &amp;= \\sum_d \\Sigma^{-1} \\text{diag}(\\nu_d^2) \\Sigma^{-1} + \\sum_d \\Sigma^{-1} (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T \\Sigma^{-1}\\\\ D &amp;= \\sum_d \\Sigma^{-1} \\text{diag}(\\nu_d^2) + \\sum_d \\Sigma^{-1} (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T\\\\ \\Sigma D &amp;= \\sum_d \\text{diag}(\\nu_d^2) + \\sum_d (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T\\\\ \\Sigma &amp;= \\frac{1}{D}(\\sum_d \\text{diag}(\\nu_d^2) + (\\lambda_d + \\mu)(\\lambda_d - \\mu)^T) \\end{aligned} \\] 12.4.8.4 Optimization for \\(\\xi\\) Isolating terms, we get \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\xi} &amp;= \\frac{\\partial}{\\partial \\xi}[ - \\log \\xi - \\xi^{-1} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}]]\\\\ &amp;= - \\frac{1}{\\xi} + \\frac{1}{\\xi^{2}} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}] \\end{aligned} \\] Then we set this equal to zero: \\[ \\begin{aligned} 0 &amp;= - \\frac{1}{\\xi} + \\frac{1}{\\xi^{2}} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}]\\\\ \\frac{1}{\\xi} &amp;= \\frac{1}{\\xi^{2}} \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}]\\\\ \\xi &amp;= \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}] \\end{aligned} \\] 12.4.8.5 Optimization for \\(\\phi\\) Again, isolating terms and adding the constraint \\(\\sum_n \\phi_{n,i} = 1\\), we get \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\phi_{n,i}} &amp;= \\frac{\\partial}{\\partial \\phi_{n,i}}[\\gamma_i \\phi_{n,i} + \\sum_v 1[w_{n}=v]\\phi_{n,i} \\log \\beta_{i,v} -\\phi_{n,i} \\log \\phi_{n,i} + \\zeta(\\sum_n \\phi_{n,i}-1)]\\\\ &amp;= \\gamma_i + \\sum_v 1[w_{n}=v]\\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\zeta \\end{aligned} \\] which we set to zero \\[ \\begin{aligned} 0 &amp;= \\gamma_i + \\sum_v 1[w_{n}=v]\\log \\beta_{i,v} - \\log \\phi_{n,i} - 1 + \\zeta\\\\ \\log \\phi_{n,i} &amp;= \\gamma_i + \\sum_v 1[w_{n}=v]\\log \\beta_{i,v} - 1 + \\zeta\\\\ \\phi_{n,i} &amp;= \\exp[\\gamma_i]\\exp[\\log \\beta_{i,v}^{1[w_{n}=v]}]\\exp[ - 1 + \\zeta]\\\\ &amp;= \\exp[\\gamma_i]\\beta_{i,v}^{1[w_{n}=v]}\\exp[ - 1 + \\zeta]\\\\ &amp;= \\exp[ - 1 + \\zeta]\\beta_{i,v}^{1[w_{n}=v]}\\exp[\\gamma_i]\\\\ &amp;= c_{n,i}\\beta_{i,v}^{1[w_{n}=v]}\\exp[\\gamma_i]\\\\ &amp;\\propto \\beta_{i,v}^{1[w_{n}=v]}\\exp[\\gamma_i] \\end{aligned} \\] where \\(c_{n,i}=\\exp[ - 1 + \\zeta]\\). As in LDA, we will calculate \\(\\phi\\) and then normalize to satisfy this constraint. 12.4.8.6 Optimization for \\(\\lambda\\) Isolating terms and rewriting some of the sums as vectors, we get the following expression. \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\lambda} &amp;= \\frac{\\partial}{\\partial \\lambda}[- \\frac{1}{2}(\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) + \\sum_n \\sum_i \\lambda_i \\phi_{n,i} - \\xi^{-1} \\sum_n \\sum_i^{K} \\exp [\\lambda_i + \\frac{\\nu_i^2}{2}]]\\\\ &amp;= \\frac{\\partial}{\\partial \\lambda}[- \\frac{1}{2}(\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) + \\sum_n \\lambda \\phi_{n,\\cdot} - \\xi^{-1} \\sum_n \\exp [\\lambda + \\frac{\\nu^2}{2}]]\\\\ &amp;= \\frac{\\partial}{\\partial \\lambda}[- \\frac{1}{2}(\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) + \\sum_n \\lambda \\phi_{n,\\cdot} - \\frac{N}{\\xi} \\exp [\\lambda + \\frac{\\nu^2}{2}]]\\\\ \\end{aligned} \\] Now, let’s compute the derivative: \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\lambda} &amp;= \\frac{\\partial}{\\partial \\lambda}[- \\frac{1}{2}(\\lambda - \\mu)^T\\Sigma^{-1}(\\lambda - \\mu) + \\sum_n \\lambda \\phi_{n,\\cdot} - \\frac{N}{\\xi} \\exp [\\lambda + \\frac{\\nu^2}{2}]]\\\\ &amp;= - \\frac{1}{2}(2\\Sigma^{-1}(\\lambda - \\mu)) + \\sum_n \\phi_{n,\\cdot} - \\frac{N}{\\xi} \\exp [\\lambda + \\frac{\\nu^2}{2}]\\\\ &amp;= - \\Sigma^{-1}(\\lambda - \\mu) + \\sum_n \\phi_{n,\\cdot} - \\frac{N}{\\xi} \\exp [\\lambda + \\frac{\\nu^2}{2}] \\end{aligned} \\] This cannot be solved analytically, but we can apply gradient descent. 12.4.8.7 Optimization for \\(\\nu^2\\) For our last optimization, we’ll again isolate terms. Let’s rewrite \\(\\nu_i^2\\) as \\(\\delta_i := \\nu_i^2\\) to prevent confusion. \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\delta} &amp;= \\frac{\\partial}{\\partial \\delta_i}[- \\frac{1}{2} \\delta_i\\Sigma_{ii}^{-1} - \\xi^{-1} \\sum_n \\exp [\\lambda_i + \\frac{\\delta_i}{2}] + \\frac{1}{2} \\log \\delta_i ]\\\\ &amp;= \\frac{\\partial}{\\partial \\delta_i}[- \\frac{1}{2} \\delta_i\\Sigma_{ii}^{-1} - \\frac{N}{\\xi}\\exp [\\lambda_i + \\frac{\\delta_i}{2}] + \\frac{1}{2} \\log \\delta_i ]\\\\ &amp;= -\\frac{1}{2}\\Sigma_{ii}^{-1} - \\frac{N}{2\\xi}\\exp [\\lambda_i + \\frac{\\delta_i}{2}] + \\frac{1}{2\\delta}\\\\ \\end{aligned} \\] which, in terms of \\(\\nu^2\\), is \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\nu^2} &amp;= -\\frac{1}{2}\\Sigma_{ii}^{-1} - \\frac{N}{2\\xi}\\exp [\\lambda_i + \\frac{\\nu^2}{2}] + \\frac{1}{2 \\nu^2}\\\\ \\end{aligned} \\] Like \\(\\frac{\\partial L}{\\partial \\lambda}\\), this cannot be solved analytically. We can use Newton’s method, but we first need to solve for the Hessian: \\[ \\begin{aligned} \\frac{\\partial^2 L}{\\partial \\delta^2} &amp;= \\frac{\\partial}{\\partial \\delta_i}[-\\frac{1}{2}\\Sigma_{ii}^{-1} - \\frac{N}{2\\xi}\\exp [\\lambda_i + \\frac{\\delta_i}{2}] + \\frac{1}{2\\delta}]\\\\ &amp;= - \\frac{N}{4\\xi}\\exp [\\lambda_i + \\frac{\\delta_i}{2}] - \\frac{1}{2\\delta^2}\\\\ \\end{aligned} \\] which in terms of \\(\\nu^2\\) is \\[ \\begin{aligned} \\frac{\\partial^2 L}{\\partial (\\nu^2)^2} &amp;= - \\frac{N}{4\\xi}\\exp [\\lambda_i + \\frac{\\nu^2}{2}] - \\frac{1}{2(\\nu^2)^2}\\\\ \\end{aligned} \\] 12.5 Dirichlet Distribution dsim &lt;- function(alpha=c(1,1,1),draws){ if (length(alpha)==1) alpha &lt;- rep(alpha,3) elements &lt;- length(alpha) dis &lt;- rdirichlet(draws, alpha) disz &lt;- ddirichlet(dis, alpha) triangle &lt;- data.frame(cbind(dis,disz)) names(triangle) &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;d&quot;) df &lt;- data.frame(&quot;Atom&quot;=as.factor(rep(LETTERS[1:elements], each=draws)), &quot;Probability&quot;=matrix(dis,ncol=1), &quot;Draw&quot;=as.factor(rep(1:draws,elements))) p &lt;- ggplot(subset(df,Draw %in% 1:15), aes(x=Atom,y=Probability,ymin=0,ymax=Probability)) + geom_linerange(colour=&quot;Blue&quot;,size=1.5) + geom_point(colour=&quot;Blue&quot;,size=4) + scale_y_continuous(lim=c(0,1)) + facet_wrap(~Draw,ncol=5) + theme(panel.background = element_rect(), title=element_text(size=20), strip.text=element_text(size=13), axis.text=element_text(size=13), axis.title=element_text(size=18,face=&quot;bold&quot;)) + ggtitle(bquote(alpha == (.(paste(alpha,collapse=&quot; &quot;))))) if (elements==3){ q &lt;- ggtern(data=triangle,aes(x=C,y=A,z=B)) + geom_density_tern(aes(weight=d,color=..level..),size=1.2) + geom_point(color=&quot;black&quot;,size=4,alpha=.5) + scale_color_gradient(low=&quot;red&quot;,high=&quot;white&quot;) + theme_rgbw() + theme(panel.background = element_rect(), tern.axis.arrow.text =element_text(size=28,face=&quot;bold&quot;), tern.axis.text=element_text(size=18) ) + guides(color=FALSE) print(plot_grid(q,p,ncol=2)) } else { print(p) } } dsim(1,draws=100) dsim(c(1,10,1),draws=100) dsim(.05,draws=100) "],
["ml.html", "Chapter 13 Machine Learning 13.1 Cross Valdiation 13.2 Naive Bayes 13.3 SVM 13.4 K-means 13.5 Gaussian Mixtures 13.6 PCA 13.7 Viterbi Algorithm 13.8 Gradient Descent 13.9 Nonparametric Bayesian Processes 13.10 Iteratively Reweighted Least Squares 13.11 Neural Network", " Chapter 13 Machine Learning 13.1 Cross Valdiation Proof that k-fold CV has less variance around the mean than LOOCV – better for k-fold CV. 13.1.1 LOOCV N = 1000 # data size p = .10 # probability of a misclassification set.seed(100) d1 &lt;- rbinom(N,1,p) # bernoulli sampling: sample 1, check if it&#39;s a match mean(d1) ## [1] 0.113 mean(d1)*(1-mean(d1)) # var ## [1] 0.100231 var(d1) ## [1] 0.1003313 10-fold CV k = 10 # number of CV replications n = N/k # sample size for each k from data N set.seed(100) d2 &lt;- rbinom(N,n,p)/n # binomial sampling: from 1000, # sample 100, sum number that are wrong of 100, # divide by sample size to get error rate mean(d2) ## [1] 0.10191 (k/N)*mean(d2)*(1-mean(d2)) # var ## [1] 0.0009152435 var(d2) ## [1] 0.0009229749 note that 10xCV var is &lt; LOOCV var. 13.2 Naive Bayes sex &lt;- rep(c(&quot;M&quot;,&quot;F&quot;),each=4) # feature 1 h &lt;- c(6,5.92,5.58,5.92,5,5.5,5.42,5.75) # feature 2 w &lt;- c(180,190,170,165,100,150,130,150) # feature 3 f &lt;- c(12,11,12,10,6,8,7,9) # feature 4 df1 &lt;- data.frame(sex,h,w,f) uh &lt;- tapply(df1$h,df1$sex,mean) uf &lt;- tapply(df1$f,df1$sex,mean) uw &lt;- tapply(df1$w,df1$sex,mean) sh &lt;- tapply(df1$h,df1$sex,sd) sf &lt;- tapply(df1$f,df1$sex,sd) sw &lt;- tapply(df1$w,df1$sex,sd) new &lt;- data.frame(&quot;h&quot;=6,&quot;w&quot;=130,&quot;f&quot;=8) ps &lt;- table(df1$sex)/length(df1$sex) # P(F) P(M) phgs &lt;- dnorm(new$h,uh,sh) # P(h|F) P(h|M) pwgs &lt;- dnorm(new$w,uw,sw) # P(h|F) P(h|M) pfgs &lt;- dnorm(new$f,uf,sf) # P(h|F) P(h|M) postf &lt;- ps[1]*phgs[1]*pwgs[1]*pfgs[1] # P(sex|h,w,f) = P(sex)*P(h|sex)*P(w|sex)*P(f|sex) postm &lt;- ps[2]*phgs[2]*pwgs[2]*pfgs[2] if (postm &gt; postf) &quot;M&quot; else &quot;F&quot; ## [1] &quot;F&quot; 13.3 SVM data(iris) train &lt;- iris train$y &lt;-ifelse(train[,5]==&quot;setosa&quot;, 1, -1) train &lt;- train[order(train$y, decreasing=TRUE),] X &lt;- as.matrix(train[,c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]) y &lt;- as.matrix(train$y) n &lt;- dim(X)[1] \\[ \\begin{aligned} \\max \\alpha &amp;W(\\alpha) = \\sum{\\alpha_1} - -.5 \\sum{y_i y_j \\alpha_i \\alpha_j x_i^T x_j}\\\\ &amp;\\text{s.t.} \\quad \\alpha_i \\ge 0\\\\ &amp;\\text{s.t.} \\quad \\sum_{\\alpha_i * y_i} = 0 \\end{aligned} \\] is equivalent to \\[ \\begin{aligned} \\min \\alpha - &amp;\\alpha + 0.5 \\alpha^T * H * \\alpha\\\\ &amp;\\text{s.t.} \\quad \\alpha \\ge 0\\\\ &amp;\\text{s.t.} \\quad A \\alpha \\le 0\\\\ &amp;\\text{where} \\quad H(i,j) = y_i y_j x_i^T x_j\\\\ &amp;\\text{where} \\quad A = y^T\\\\ &amp;\\text{note} \\quad \\max z \\equiv \\min -z \\end{aligned} \\] And ipop is \\[ \\begin{aligned} \\min_\\alpha c &amp;\\alpha + 0.5 x^T H x\\\\ &amp;\\text{s.t.} \\quad b \\le A \\alpha \\le b + r\\\\ &amp;\\text{s.t.} \\quad l \\le \\alpha \\le u\\\\ &amp;\\text{thus} \\quad c=-1\\\\ &amp;\\text{thus} \\quad u = \\infty, l=0 \\quad \\text{but will set $u$ to a large number}\\\\ &amp;\\text{thus} \\quad b=0, r=0 \\quad \\text{to remove this contraint} \\end{aligned} \\] H &lt;- matrix(NA,n,n) for (i in 1:n){ for (j in 1:n){ H[i,j] &lt;- y[i]*y[j]*t(X[i,])%*%X[j,] } } A &lt;- t(y) c &lt;- matrix(rep(-1,n)) l &lt;- matrix(rep(0,n)) b &lt;- 0 u &lt;- matrix(rep(1e5,n)) r &lt;- 0 alpha &lt;- primal(ipop(c,H,A,b,l,u,r)) nonzero &lt;- which(abs(alpha) &gt; 1e-5) w &lt;- matrix(NA,nrow=length(nonzero),ncol=ncol(X)) for (i in seq_along(nonzero)){ w[i,] &lt;- alpha[nonzero[i]]*y[nonzero[i]]*X[nonzero[i],] } w &lt;- colSums(w) b0 &lt;- -(max(sapply(1:sum(y==-1), function(i) matrix(w,ncol=2) %*% X[y==-1,][i,])) + min(sapply(1:sum(y==1), function(i) matrix(w,ncol=2) %*% X[y==1,][i,])))/2 slope &lt;- -w[1]/w[2] intercept &lt;- -b0/w[2] plot(X,pch=19,col=ifelse(1:n %in% nonzero,&quot;green&quot;,&quot;black&quot;)) # green ~ support vectors abline(intercept,slope,col=&quot;red&quot;) sigma = 1 rbf &lt;- rbfdot(sigma = sigma) H_rbf &lt;- kernelMatrix(rbf,X) 13.3.1 Manual rbf kernal XtX &lt;- X%*%t(X) # crossprod(t(X)) XX &lt;- matrix(1, n) %*% diag(XtX) D &lt;- XX - 2 * XtX + t(XX) H &lt;- exp(-D/(2 * sigma)) alpha &lt;- primal(ipop(c,H,A,b,l,u,r)) nonzero &lt;- which(abs(alpha) &gt; 1e-5) w &lt;- matrix(NA,nrow=length(nonzero),ncol=ncol(X)) for (i in seq_along(nonzero)){ w[i,] &lt;- alpha[nonzero[i]]*y[nonzero[i]]*X[nonzero[i],] } w &lt;- colSums(w) b0 &lt;- -(max(sapply(1:sum(y==-1), function(i) matrix(w,ncol=2) %*% X[y==-1,][i,])) + min(sapply(1:sum(y==1), function(i) matrix(w,ncol=2) %*% X[y==1,][i,])))/2 slope &lt;- -w[1]/w[2] intercept &lt;- -b0/w[2] plot(X) abline(intercept,slope,col=&quot;red&quot;) 13.4 K-means library(tidyverse) library(gganimate) distance &lt;- function(x,c){ d &lt;- apply(c,1,function(y) sqrt((x[1]-y[1])^2 + (x[2]-y[2])^2)) w &lt;- which.min(d) return(w) } set.seed(123) x1 &lt;- rnorm(100,5,1) y1 &lt;- rnorm(100,0,2) x2 &lt;- rnorm(100,10,1) y2 &lt;- 3 + rnorm(100,0,2) x3 &lt;- rnorm(100,0,1) y3 &lt;- 8 + rnorm(100,0,2) x4 &lt;- rnorm(100,6,1) y4 &lt;- 15 + rnorm(100,0,1) x5 &lt;- rnorm(100,5.5,.5) y5 &lt;- 8 + rnorm(100,0,1) data &lt;- data.frame(&quot;x&quot; = c(x1,x2,x3,x4,x5), &quot;y&quot; = c(y1,y2,y3,y4,y5), &quot;class&quot; = rep(1:5,each=length(x1))) c &lt;- matrix(c(min(data$x),max(data$x),min(data$x),max(data$x),mean(data$x), min(data$y),max(data$y),max(data$y),min(data$x),mean(data$y)),ncol=2) data.means &lt;- data.frame(cbind(c,0)) names(data.means) &lt;- c(&quot;x&quot;,&quot;y&quot;,&quot;class&quot;) print(true &lt;- ggplot(data,aes(x,y,colour=factor(class),size=2)) + geom_point(alpha=.7) + geom_point(data=data.means,colour=&quot;red&quot;) + geom_text(data=data.means,label=&quot;mean&quot;,vjust=2)) data$class &lt;- 0 c.old &lt;- 0 while (abs(sum(c-c.old)) != 0){ data$class &lt;- apply(data[,1:2],1,function(x) distance(x,c)) c.old &lt;- c c &lt;- cbind(tapply(data$x,data$class,mean),tapply(data$y,data$class,mean)) data.means &lt;- data.frame(cbind(c,0)) names(data.means) &lt;- c(&quot;x&quot;,&quot;y&quot;,&quot;class&quot;) } group_means$iteration &lt;- as.integer(group_means$iteration) ggplot(data=group_means, aes(x,y)) + geom_point(color=&#39;black&#39;,alpha=.7,size=8) + geom_point(data=dat,aes(x,y,color=as.factor(class)),alpha=.3,size=2) + scale_color_brewer(type=&#39;qual&#39;,palette=2) + theme_bw() + theme(legend.position=&#39;none&#39;) + transition_time(iteration) + labs(title = &#39;Iteration: {frame_time}&#39;, x = &#39;&#39;, y = &#39;&#39;) + ease_aes(&#39;linear&#39;) 13.5 Gaussian Mixtures library(tidyverse) data &lt;- c(rnorm(50,12,1),rnorm(50,4,1)) ua &lt;- .19; sa &lt;- .5 ub &lt;- .65; sb &lt;- .5 for (i in 1:1000){ pda &lt;- exp(-(data-ua)^2) pdb &lt;- exp(-(data-ub)^2) pa &lt;- pda/(pda+pdb) pb &lt;- pdb/(pda+pdb) ua &lt;- sum(pa*data)/sum(pa) sa &lt;- sum(pa*(data-ua)^2)/sum(pa) ub &lt;- sum(pb*data)/sum(pb) sb &lt;- sum(pb*(data-ub)^2)/sum(pb) cat(ua,sa,&quot;\\n&quot;,ub,sb,&quot;\\n&quot;) } set.seed(43) N &lt;- 500 p &lt;- rbinom(500,1,.3) y &lt;- (1-p)*rnorm(N,4,1) + p*rnorm(N,-1,1) mu1 &lt;- rnorm(1) mu2 &lt;- rnorm(1) p &lt;- .5 for (t in 1:2500){ gamma1 &lt;- dnorm(y,mu1,1) gamma2 &lt;- dnorm(y,mu2,1) gamma &lt;- (p*gamma2)/((1-p)*gamma1 + p*gamma2) mu_hat1 &lt;- sum((1-gamma)*y)/sum(1-gamma) mu_hat2 &lt;- sum(gamma*y)/sum(gamma) mu1 &lt;- mu_hat1 mu2 &lt;- mu_hat2 p &lt;- sum(gamma)/N } mu1 ## [1] -1.109935 mu2 ## [1] 4.000447 p ## [1] 0.6854409 iter &lt;- 1000 mu1_vector &lt;- vector(length=iter) mu2_vector &lt;- vector(length=iter) p_vector &lt;- vector(length=iter) for (t in 1:iter){ gamma1 &lt;- dnorm(y,mu1,1) gamma2 &lt;- dnorm(y,mu2,1) gamma &lt;- (p*gamma2)/((1-p)*gamma1 + p*gamma2) delta &lt;- rbinom(N,1,gamma) mu_hat1 &lt;- sum((1-delta)*y)/sum(1-delta) mu_hat2 &lt;- sum(delta*y)/sum(delta) mu1 &lt;- rnorm(1,mu_hat1,1) mu2 &lt;- rnorm(1,mu_hat2,1) p &lt;- sum(gamma)/N mu1_vector[t] &lt;- mu1 mu2_vector[t] &lt;- mu2 p_vector[t] &lt;- p } qplot(1:iter,mu1_vector,geom=&#39;line&#39;,colour=1) + geom_line(aes(1:iter,mu2_vector),colour=2) + theme(legend.position=&#39;none&#39;) + labs(title=&#39;mu&#39;,x=&#39;iteration&#39;,y=&#39;value&#39;) qplot(1:iter,p_vector,geom=&#39;line&#39;,colour=3) + theme(legend.position=&#39;none&#39;) + labs(title=&#39;p&#39;,x=&#39;iteration&#39;,y=&#39;value&#39;) 13.6 PCA set.seed(4131) x &lt;- 1:101 y1 &lt;- x[1:25] + rnorm(25,0,20) y2 &lt;- x[26:50] + rnorm(25,-2,5) y3 &lt;- x[51:75] + rnorm(25,10,7) y4 &lt;- x[76:101] + rnorm(26,0,15) x &lt;- scale(x) y &lt;- scale(c(y1,y2,y3,y4)) x &lt;- cbind(x,y) x &lt;- (x-mean(x))/sd(x) df &lt;- data.frame(cbind(data.frame(x),rep(LETTERS[1:4],c(25,25,25,26)))) names(df) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;group&#39;) ggplot(df,aes(x=x,y=y,colour=group)) + geom_point() evd &lt;- eigen(cov(x)) pc1 &lt;- evd$vectors[,1] pc2 &lt;- evd$vectors[,2] proj1 &lt;- x %*% pc1 proj2 &lt;- x %*% pc2 vare &lt;- sum(x[,1]*pc1[1])^2 + sum(x[,2]*pc1[2])^2 df2 &lt;- data.frame(cbind(proj1,0),df$group) names(df2) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;group&#39;) ggplot(df,aes(x=x,y=y,colour=group)) + geom_point(alpha=.3) + geom_point(data=df2,alpha=1) set.seed(4131) x1 &lt;- c(rnorm(100,68,5),rnorm(100,78,5),rnorm(100,63,4)) x2 &lt;- c(rnorm(100,215,40),rnorm(100,200,20),rnorm(100,125,15)) x3 &lt;- c(rnorm(100,280,50),rnorm(100,180,15),rnorm(100,95,15)) x1 &lt;- scale(x1) x2 &lt;- scale(x2) x3 &lt;- scale(x3) x &lt;- cbind(x1,x2,x3) df &lt;- data.frame(cbind(data.frame(x),rep(c(&quot;Compact Guys&quot;,&quot;Lanky Guys&quot;,&quot;Women&quot;),each=100))) names(df) &lt;- c(&#39;x1&#39;,&#39;x2&#39;,&#39;x3&#39;,&#39;group&#39;) evd &lt;- eigen(cov(x)) pc1 &lt;- evd$vectors[,1] pc2 &lt;- evd$vectors[,2] pc3 &lt;- evd$vectors[,3] proj1 &lt;- x %*% pc1 proj2 &lt;- x %*% pc2 proj3 &lt;- x %*% pc3 vare &lt;- sum(x[,1]*pc1[1])^2 + sum(x[,2]*pc1[2])^2 + sum(x[,3]*pc1[3])^2 df2 &lt;- data.frame(cbind(proj1,proj2),df$group) names(df2) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;group&#39;) ggplot(df2,aes(x=x,y=y,colour=group)) + geom_point(alpha=1) 13.7 Viterbi Algorithm init &lt;- log(c(.5,.5),2) trans1 &lt;- log(c(.5,.5),2) trans2 &lt;- log(c(.4,.6),2) vis1 &lt;- log(c(.2,.3,.3,.2),2) vis2 &lt;- log(c(.3,.2,.2,.3),2) seq &lt;- &quot;GGCACTGAA&quot; dic &lt;- c(&quot;A&quot;,&quot;C&quot;,&quot;G&quot;,&quot;T&quot;) viterbi &lt;- function(seq){ if (nchar(seq) == 1){ nt &lt;- which(dic == seq) print(comp &lt;- c(init[1] + vis1[nt], init[2] + vis2[nt])) return(c(comp)) }else{ nt &lt;- which(dic == substr(seq,nchar(seq),nchar(seq))) past &lt;- viterbi(substr(seq,1,nchar(seq)-1)) if (past[1] &gt; past[2]) ans &lt;- 1 else ans &lt;- 2 print(ans) print( choice &lt;- c( vis1[nt] + max(past[1] + trans1[1], past[2] + trans2[1]), vis2[nt] + max(past[1] + trans1[2], past[2] + trans2[2]) ) ) } } init &lt;- c(.5,.5) trans1 &lt;- c(.5,.5) trans2 &lt;- c(.4,.6) vis1 &lt;- c(.2,.3,.3,.2) vis2 &lt;- c(.3,.2,.2,.3) path &lt;- c(3,3,2,1) latent1 &lt;- 0 latent2 &lt;- 0 prev1 &lt;- init[1]*vis1[path[1]] prev2 &lt;- init[2]*vis2[path[1]] for (i in 2:length(path)){ prevtemp1 &lt;- prev1*trans1[1]*vis1[path[i]] + prev2*trans2[1]*vis1[path[i]] prevtemp2 &lt;- prev2*trans2[2]*vis2[path[i]] + prev1*trans1[2]*vis2[path[i]] prev1 &lt;- prevtemp1 prev2 &lt;- prevtemp2 } print(prev1 + prev2) ## [1] 0.00384315 13.8 Gradient Descent set.seed(12345) x &lt;- sample(seq(from = 0, to = 2, by = 0.1), size = 50, replace = TRUE) y &lt;- 2 * x + rnorm(50) x &lt;- x - mean(x) y &lt;- y - mean(y) X &lt;- cbind(1,x) y &lt;- as.vector(y) iter &lt;- 5000 a &lt;- 0.01 b &lt;- rep(0,ncol(X)) # b0 loss &lt;- matrix(0,nrow=iter,ncol=1) B &lt;- matrix(0,nrow=iter,ncol=1) for (i in 1:iter){ fb &lt;- (1/2) * norm(y - X %*% b,&quot;F&quot;) loss[i,] &lt;- fb grad.fb &lt;- -(t(X) %*% (y - X %*% b)) # flip sign to DESCEND b &lt;- b - a*grad.fb B[i] &lt;- b[2,] } out &lt;- pretty(c(min(x)-2,max(x)+6),10000) out2 &lt;- sapply(1:length(out), function(x) (1/2) * norm(y - X %*% rbind(0,out)[,x],&quot;F&quot;)) ggplot(tibble(b=B,loss=loss,iteration=1:length(b)) %&gt;% filter(iteration &lt; 50),aes(b,loss)) + ggplot(tibble(b=B,loss=loss,iteration=1:length(b)) %&gt;% filter(iteration &lt; 50),aes(b,loss)) + geom_line(data=tibble(x=out,y=out2),aes(x,y),alpha=.3,size=2) + geom_point(color=&#39;red&#39;,alpha=1,size=1.2) + geom_line(color=&#39;red&#39;,alpha=.8,size=.5) + transition_reveal(iteration,range=c(1L,25L)) + ease_aes(&#39;quadratic-out&#39;) + theme_bw() + xlim(0,3.75) + ylim(2,6) + theme(aspect.ratio=.5) + labs(title = &#39;&#39;, x = &#39;&#39;, y = &#39;Loss&#39;) 13.8.1 Linear Regression 13.8.1.1 SGD N &lt;- 50 X &lt;- cbind(1,runif(N,-1,1),runif(N,-1,1)) k &lt;- ncol(X) theta &lt;- rnorm(k) y &lt;- X %*% theta + rnorm(N) th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 0.01 for (i in 1:10000){ grad &lt;- matrix(rep(0,k),nrow=k) for (j in 1:N){ grad &lt;- grad - X[j,] %*% (y[j]-X[j,]%*%th) } th &lt;- th - eta*grad } t(th) ## [,1] [,2] [,3] ## [1,] 0.07688319 -1.346672 0.9435691 coef(lm(y ~ X[,-1])) ## (Intercept) X[, -1]1 X[, -1]2 ## 0.07688319 -1.34667198 0.94356907 13.8.1.2 Vectorized th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 0.01 for (i in 1:10000){ grad &lt;- t(X) %*% (X %*% th - y) th &lt;- th - eta*grad } t(th) ## [,1] [,2] [,3] ## [1,] 0.07688319 -1.346672 0.9435691 coef(lm(y ~ X[,-1])) ## (Intercept) X[, -1]1 X[, -1]2 ## 0.07688319 -1.34667198 0.94356907 13.8.2 Logistic Regression invlogit &lt;- function(x) 1/(1+exp(-x)) N &lt;- 500 X &lt;- cbind(1,runif(N,-1,1)) k &lt;- ncol(X) theta &lt;- rnorm(k) y &lt;- rbinom(N,1,invlogit(X %*% theta)) N &lt;- 500 X &lt;- cbind(runif(N,-1,1),runif(N,-1,1)) theta &lt;- c(1.5,-3) y &lt;- 1+ifelse(X %*% theta + rnorm(N) &lt; 0, 0, 1) 13.8.2.1 SGD th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 0.25 for (i in 1:5000){ g &lt;- matrix(rep(0,k),nrow=k) for (j in 1:N){ g &lt;- g - X[j,] %*% (y[j]-invlogit(X[j,]%*%th)) } th &lt;- th - eta*g } scores &lt;- X %*% th pred &lt;- ifelse(scores&gt;0,1,0) plot(X,col=y+1,pch=19) plot(X,col=pred+1,pch=19) 13.8.2.2 Vectorized th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- .01 for (i in 1:10000){ g &lt;- t(X) %*% (invlogit(X %*% th)-y) th &lt;- th - eta*g } scores &lt;- X %*% th pred &lt;- ifelse(scores&gt;0,1,0) plot(X,col=y+1,pch=19) plot(X,col=pred+1,pch=19) 13.8.2.3 Newtons th &lt;- matrix(rep(0,k),nrow=k) eta &lt;- 1 for (i in 1:5000){ p &lt;- invlogit(X %*% th) S &lt;- diag(c(p * (1-p)),N,N) H &lt;- t(X) %*% S %*% X g &lt;- t(X) %*% (p-y) th &lt;- th - eta * solve(H) %*% g } scores &lt;- X %*% th pred &lt;- ifelse(scores&gt;0,1,0) plot(X,col=y+1,pch=19) plot(X,col=pred+1,pch=19) 13.8.3 Softmax regression N &lt;- 1000 X &lt;- cbind(1,runif(N,0,100),runif(N,0,100)) theta &lt;- rbind(c(100,-1,-1),c(100,0,-2)) y &lt;- 1 + ifelse(X %*% theta[1,] + rnorm(N,0,10) &lt; 0, 0, ifelse(X %*% theta[2,] + rnorm(N,0,10) &lt; 0, 1, 2)) X[,-1] &lt;- scale(X[,-1]) K &lt;- ncol(X) J &lt;- length(unique(y)) 13.8.3.1 SGD th &lt;- matrix(rep(0,K*J),nrow=K,ncol=J) g &lt;- th eta &lt;- .5 for (i in 1:5000){ a &lt;- exp(X %*% th) # exp(thetak&#39; * xi) a_sum &lt;- rowSums(a) # SUMexp(thetaj&#39; * xi) p &lt;- t(sapply(1:N,function(n) a[n,]/a_sum[n])) # P(yi=k|xi,theta) = exp(thetak&#39; * xi)/SUMexp(thetaj&#39; * xi) # -SUM[xi * (1{yi=k} - P(yi=k|xi;theta))] = SUM[xi * (P(yi=k|xi;theta)) - 1{yi=k}] # therefore P(yi=k|xi;theta)) - 1{yi=k} is simply p-1 for all p where yi=k for (n in 1:N){ p[n,y[n]] &lt;- p[n,y[n]] - 1 # P(yi=k|xi;theta)) - 1{yi=k} } p &lt;- p/N # this is not necessary but adjusts the size of the estimates to avoid very large values g &lt;- t(X) %*% p # SUM[xi * (P(yi=k|xi;theta)) - 1{yi=k}] th &lt;- th - eta*g } scores &lt;- X %*% th pred &lt;- apply(scores,1,which.max) plot(X[,-1],col=y,pch=19) plot(X[,-1],col=pred,pch=19) 13.8.3.2 SGD with bias term N &lt;- 100 D &lt;- 2 K &lt;- 3 X &lt;- matrix(0,N*K,D) y &lt;- matrix(0,N*K) ind1 &lt;- 0 ind2 &lt;- 0 for (k in 1:K){ r &lt;- seq(0,1,length=N) t &lt;- seq((k-1)*4,(k)*4,length=N) + rnorm(N)*0.2 X[(1+ind1):(N+ind2),] &lt;- cbind(r*sin(t), r*cos(t)) y[(1+ind1):(N+ind2)] &lt;- k ind1 &lt;- ind1 + N ind2 &lt;- ind2 + N } W &lt;- matrix(0.01*rnorm(D*K),D,K) b &lt;- matrix(0,1,K) step &lt;- 1 reg &lt;- 1e-3 for (i in 1:5000){ a &lt;- exp(X %*% W + rep(b,N*K)) a_sum &lt;- rowSums(a) p &lt;- t(sapply(1:(N*K),function(n) a[n,]/a_sum[n])) for (n in 1:(N*K)){ p[n,y[n]] &lt;- p[n,y[n]] - 1 } p &lt;- p/(N*K) gW &lt;- t(X) %*% p gb &lt;- colSums(p) gW &lt;- gW + reg*W W &lt;- W + -step*gW b &lt;- b + -step*gb } scores &lt;- X %*% W + rep(b,N*K) pred &lt;- apply(scores,1,which.max) plot(X,col=y,pch=19) plot(X,col=pred,pch=19) 13.9 Nonparametric Bayesian Processes 13.9.1 Chinese Restaurant chinese_restaurant &lt;- function(N, alpha){ tables &lt;- vector(length=N) tables[1] &lt;- 1 open &lt;- 2 for (i in 2:N){ choice &lt;- rbinom(1,1,alpha/(i+alpha)) if (choice == 1){ tables[open] &lt;- tables[open] + 1 open &lt;- open + 1 }else{ occupied &lt;- which(tables != 0) prob &lt;- tables[occupied]/(i+alpha) seat &lt;- sample(occupied,1,FALSE,prob) tables[seat] &lt;- tables[seat] + 1 } } return(tables) } chinese_restaurant(30,10) ## [1] 8 3 3 2 1 2 1 4 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 13.9.2 Polyas Urn polyas_urn &lt;- function(N,alpha){ balls &lt;- NULL for (i in 1:N){ choice &lt;- rbinom(1,1,alpha/(length(balls)+alpha)) if (choice == 1){ ball &lt;- rnorm(1) balls &lt;- c(balls,ball) }else{ ball &lt;- balls[sample(1:length(balls),1)] balls &lt;- c(balls,ball) } } return(balls) } rep_polyas_urn &lt;- function(N,alpha,R){ out &lt;- data.frame(replicate(R,polyas_urn(N,alpha))) colnames(out) &lt;- 1:R out %&gt;% gather(r,sample) %&gt;% mutate(r=as.factor(r)) %&gt;% ggplot(aes(x=sample,y = ..scaled..)) + geom_density(colour=&quot;black&quot;,size=1,fill=&quot;darkgreen&quot;) + facet_wrap(~r) + xlim(-3,3) + ylab(&quot;&quot;) + xlab(&quot;&quot;) } rep_polyas_urn(25,500,12) 13.9.3 Stick Breaking stick_breaking &lt;- function(N,alpha){ p &lt;- rbeta(N,1,alpha) len &lt;- 1 w &lt;- p[1] for (i in 2:N){ len &lt;- len-w[i-1] w_new &lt;- p[i]*(len) w &lt;- c(w,w_new) } return(w) } rep_stick_breaking &lt;- function(N,alpha,R){ out &lt;- data.frame(replicate(R,stick_breaking(N,alpha)),1:N) colnames(out) &lt;- c(1:R,&quot;Breaks&quot;) out %&gt;% gather(r,Probability,-Breaks) %&gt;% mutate(r=as.factor(r),Breaks=as.factor(Breaks)) %&gt;% ggplot(aes(x=Breaks,y=Probability,ymin=0,ymax=Probability)) + geom_linerange(colour=&quot;Blue&quot;,size=1) + geom_point(colour=&quot;Blue&quot;,size=4) + scale_y_continuous(lim=c(0,1)) + facet_wrap(~r) + theme(panel.background = element_rect(), title=element_text(size=20), strip.text=element_text(size=13), axis.text=element_text(size=13), axis.title=element_text(size=18,face=&quot;bold&quot;)) + ggtitle(bquote(alpha == .(paste(alpha,collapse=&quot; &quot;)))) } rep_stick_breaking(10,1,12) 13.10 Iteratively Reweighted Least Squares inv_logit &lt;- function(x) return(1/(1+exp(-x))) N &lt;- 100 k &lt;- 1 X &lt;- cbind(1,matrix(runif(N*k,-1,1))) theta_true &lt;- matrix(c(.25,-.75),ncol=1) y &lt;- rbinom(N,1,inv_logit(X %*% theta_true)) summary(glm(y ~ X[,-1], family=binomial(link=&quot;logit&quot;))) ## ## Call: ## glm(formula = y ~ X[, -1], family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7842 -1.2277 0.7490 0.9695 1.2757 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.5825 0.2153 2.705 0.00683 ** ## X[, -1] -0.8247 0.3847 -2.144 0.03206 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.79 on 99 degrees of freedom ## Residual deviance: 126.96 on 98 degrees of freedom ## AIC: 130.96 ## ## Number of Fisher Scoring iterations: 4 irls &lt;- function(X,y,tol=1e-6){ k &lt;- ncol(X) N &lt;- nrow(X) theta &lt;- matrix(rep(0,k),ncol=1) theta_new &lt;- Inf while (max(abs(theta - theta_new)) &gt; tol){ a &lt;- X %*% theta p &lt;- inv_logit(a) s &lt;- diag(c(p*(1-p)),N,N) xsx &lt;- t(X) %*% s %*% X sxt &lt;- s %*% X %*% theta theta_new &lt;- theta theta &lt;- solve(xsx) %*% t(X) %*% (sxt + y - p) } return(theta) } 13.11 Neural Network import numpy as np import random import cPickle import gzip import os import sys def load_data(): f = gzip.open(&#39;mnist.pkl.gz&#39;, &#39;rb&#39;) training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data) def load_data_wrapper(): tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip(training_inputs, training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip(validation_inputs, va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip(test_inputs, te_d[1]) return (training_data, validation_data, test_data) def vectorized_result(j): e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): return sigmoid(z)*(1-sigmoid(z)) training_data, validation_data, test_data = load_data_wrapper() sizes = [784, 30, 10] num_layers = len(sizes) eta = 3.0 # must be real, not integer (so not 3) epochs = 30 n = len(training_data) n_test = len(test_data) mini_batch_size = 10 biases = [np.zeros((y, 1)) for y in sizes[1:]] weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] # n/mini_batch_size # mini_batches is the result of breaking the training_data into length 25 batches, so 2000 mini_batches for mini_batch in mini_batches: # start gradient at 0 nabla_b = [np.zeros(b.shape) for b in biases] nabla_w = [np.zeros(w.shape) for w in weights] for x, y in mini_batch: delta_nabla_b = [np.zeros(b.shape) for b in biases] delta_nabla_w = [np.zeros(w.shape) for w in weights] # forward pass activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(biases, weights): z = np.dot(w, activation) + b zs.append(z) activation = sigmoid(z) activations.append(activation) # dot product between w1 in layer 1-2 with activation=input, add b1 for layer 2, set output as activation1 # dot product between w2 in layer 2-3 with activation=activation1, add b2 for layer 3, set output as activation2 # backward pass delta = (activations[-1]-y) * sigmoid_prime(zs[-1]) # dC/dz_lj = (a - y) * o&#39;(z), cost wrt output layer delta_nabla_b[-1] = delta # dC/db_lj = delta_lj delta_nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # dC/dw_ljk = a_(l-1)k * delta_lj for l in xrange(2, num_layers): delta = np.dot(weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l]) # dC/dz_lj delta_nabla_b[-l] = delta # dC/db_lj delta_nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # dC/dw_ljk nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, nabla_b)] weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, nabla_w)] test_results = [(np.argmax(feedforward(x,biases,weights)), y) for (x, y) in test_data] test_results = sum(int(x == y) for (x, y) in test_results) print &quot;Epoch {0}: {1} / {2} | mean_w = {3} | mean_nabla_w = {4}&quot;.format(j, test_results, n_test,np.mean(weights[1]),np.mean(nabla_w[1])) "]
]
