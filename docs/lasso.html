<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Lasso | Worked Bioninformatics, Statistics, and Machine Learning Examples</title>
  <meta name="description" content="Code, notes, lectures, and research that should be helpful for learning purposes." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Lasso | Worked Bioninformatics, Statistics, and Machine Learning Examples" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://sw1.github.io/teaching/" />
  
  <meta property="og:description" content="Code, notes, lectures, and research that should be helpful for learning purposes." />
  <meta name="github-repo" content="sw1/teaching" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Lasso | Worked Bioninformatics, Statistics, and Machine Learning Examples" />
  
  <meta name="twitter:description" content="Code, notes, lectures, and research that should be helpful for learning purposes." />
  

<meta name="author" content="Stephen Woloszynek" />


<meta name="date" content="2020-02-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multcomp.html"/>
<link rel="next" href="tm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li>Data Science Examples</li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="dynprog.html"><a href="dynprog.html"><i class="fa fa-check"></i><b>2</b> Dynamic Programming</a><ul>
<li class="chapter" data-level="2.1" data-path="dynprog.html"><a href="dynprog.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="dynprog.html"><a href="dynprog.html#rod-cutting"><i class="fa fa-check"></i><b>2.2</b> Rod cutting</a></li>
<li class="chapter" data-level="2.3" data-path="dynprog.html"><a href="dynprog.html#fibonacci-rabbits"><i class="fa fa-check"></i><b>2.3</b> Fibonacci rabbits</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="align.html"><a href="align.html"><i class="fa fa-check"></i><b>3</b> Alignment</a><ul>
<li class="chapter" data-level="3.1" data-path="align.html"><a href="align.html#longest-common-subsequence"><i class="fa fa-check"></i><b>3.1</b> Longest Common Subsequence</a></li>
<li class="chapter" data-level="3.2" data-path="align.html"><a href="align.html#global-alignment"><i class="fa fa-check"></i><b>3.2</b> Global Alignment</a></li>
<li class="chapter" data-level="3.3" data-path="align.html"><a href="align.html#local-alignment"><i class="fa fa-check"></i><b>3.3</b> Local Alignment</a></li>
<li class="chapter" data-level="3.4" data-path="align.html"><a href="align.html#local-alignment-homework"><i class="fa fa-check"></i><b>3.4</b> Local Alignment: Homework</a></li>
<li class="chapter" data-level="3.5" data-path="align.html"><a href="align.html#global-alignment-code-r"><i class="fa fa-check"></i><b>3.5</b> Global Alignment Code (R)</a></li>
<li class="chapter" data-level="3.6" data-path="align.html"><a href="align.html#global-alignment-code-python"><i class="fa fa-check"></i><b>3.6</b> Global Alignment Code (Python)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="alignalg.html"><a href="alignalg.html"><i class="fa fa-check"></i><b>4</b> Alignment Algorithms</a><ul>
<li class="chapter" data-level="4.1" data-path="alignalg.html"><a href="alignalg.html#longest-common-subsequence-1"><i class="fa fa-check"></i><b>4.1</b> Longest Common Subsequence</a></li>
<li class="chapter" data-level="4.2" data-path="alignalg.html"><a href="alignalg.html#global-alignment-r"><i class="fa fa-check"></i><b>4.2</b> Global Alignment (R)</a></li>
<li class="chapter" data-level="4.3" data-path="alignalg.html"><a href="alignalg.html#global-alignment-python"><i class="fa fa-check"></i><b>4.3</b> Global Alignment (Python)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="biocond.html"><a href="biocond.html"><i class="fa fa-check"></i><b>5</b> Bioconductor</a><ul>
<li class="chapter" data-level="5.0.1" data-path="biocond.html"><a href="biocond.html#loading-fasta-files"><i class="fa fa-check"></i><b>5.0.1</b> Loading FASTA Files</a></li>
<li class="chapter" data-level="5.0.2" data-path="biocond.html"><a href="biocond.html#creating-sequence-sets"><i class="fa fa-check"></i><b>5.0.2</b> Creating Sequence Sets</a></li>
<li class="chapter" data-level="5.0.3" data-path="biocond.html"><a href="biocond.html#sample-metadata"><i class="fa fa-check"></i><b>5.0.3</b> Sample Metadata</a></li>
<li class="chapter" data-level="5.1" data-path="biocond.html"><a href="biocond.html#creating-gc-functions"><i class="fa fa-check"></i><b>5.1</b> Creating GC Functions</a></li>
<li class="chapter" data-level="5.2" data-path="biocond.html"><a href="biocond.html#ncbi-esearch"><i class="fa fa-check"></i><b>5.2</b> NCBI ESearch</a></li>
<li class="chapter" data-level="5.3" data-path="biocond.html"><a href="biocond.html#cds"><i class="fa fa-check"></i><b>5.3</b> CDS</a></li>
<li class="chapter" data-level="5.4" data-path="biocond.html"><a href="biocond.html#whole-genomes"><i class="fa fa-check"></i><b>5.4</b> Whole Genomes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sra.html"><a href="sra.html"><i class="fa fa-check"></i><b>6</b> Retrieving Projects</a><ul>
<li class="chapter" data-level="6.0.1" data-path="sra.html"><a href="sra.html#fastq-dump-for-paired-end-reads"><i class="fa fa-check"></i><b>6.0.1</b> Fastq Dump for Paired End Reads</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="phyloseq.html"><a href="phyloseq.html"><i class="fa fa-check"></i><b>7</b> Phyloseq</a></li>
<li class="chapter" data-level="8" data-path="dada.html"><a href="dada.html"><i class="fa fa-check"></i><b>8</b> Dada2</a><ul>
<li class="chapter" data-level="8.1" data-path="dada.html"><a href="dada.html#fastq-prep"><i class="fa fa-check"></i><b>8.1</b> FASTQ Prep</a></li>
<li class="chapter" data-level="8.2" data-path="dada.html"><a href="dada.html#otu-picking"><i class="fa fa-check"></i><b>8.2</b> OTU Picking</a></li>
<li class="chapter" data-level="8.3" data-path="dada.html"><a href="dada.html#running-dada2-on-proteus"><i class="fa fa-check"></i><b>8.3</b> Running Dada2 on Proteus</a><ul>
<li class="chapter" data-level="8.3.1" data-path="dada.html"><a href="dada.html#method-1-using-namegrp-shared-r-library"><i class="fa fa-check"></i><b>8.3.1</b> Method 1: Using nameGrp Shared R Library</a></li>
<li class="chapter" data-level="8.3.2" data-path="dada.html"><a href="dada.html#method-2-creating-a-local-library"><i class="fa fa-check"></i><b>8.3.2</b> Method 2: Creating a Local Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="qiime.html"><a href="qiime.html"><i class="fa fa-check"></i><b>9</b> Qiime</a><ul>
<li class="chapter" data-level="9.1" data-path="qiime.html"><a href="qiime.html#otu-picking-1"><i class="fa fa-check"></i><b>9.1</b> OTU Picking</a></li>
<li class="chapter" data-level="9.2" data-path="qiime.html"><a href="qiime.html#summarizing-our-results"><i class="fa fa-check"></i><b>9.2</b> Summarizing Our Results</a></li>
<li class="chapter" data-level="9.3" data-path="qiime.html"><a href="qiime.html#loading-qiime-results-into-phyloseq"><i class="fa fa-check"></i><b>9.3</b> Loading QIIME Results into Phyloseq</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multcomp.html"><a href="multcomp.html"><i class="fa fa-check"></i><b>10</b> Multiple Comparisons</a><ul>
<li class="chapter" data-level="10.1" data-path="multcomp.html"><a href="multcomp.html#hypothesis-testing-and-power"><i class="fa fa-check"></i><b>10.1</b> Hypothesis Testing and Power</a></li>
<li class="chapter" data-level="10.2" data-path="multcomp.html"><a href="multcomp.html#multiple-comparisons"><i class="fa fa-check"></i><b>10.2</b> Multiple Comparisons</a><ul>
<li class="chapter" data-level="10.2.1" data-path="multcomp.html"><a href="multcomp.html#bonferroni"><i class="fa fa-check"></i><b>10.2.1</b> Bonferroni</a></li>
<li class="chapter" data-level="10.2.2" data-path="multcomp.html"><a href="multcomp.html#false-discovery-rate"><i class="fa fa-check"></i><b>10.2.2</b> False Discovery Rate</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>11</b> Lasso</a><ul>
<li class="chapter" data-level="11.1" data-path="lasso.html"><a href="lasso.html#big-data-and-feature-selection"><i class="fa fa-check"></i><b>11.1</b> Big Data and Feature Selection</a></li>
<li class="chapter" data-level="11.2" data-path="lasso.html"><a href="lasso.html#lasso-regression"><i class="fa fa-check"></i><b>11.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="11.3" data-path="lasso.html"><a href="lasso.html#multivariate-lasso-regression"><i class="fa fa-check"></i><b>11.3</b> Multivariate Lasso Regression</a></li>
<li class="chapter" data-level="11.4" data-path="lasso.html"><a href="lasso.html#parallel-coordinate-descent"><i class="fa fa-check"></i><b>11.4</b> Parallel Coordinate Descent</a></li>
<li class="chapter" data-level="11.5" data-path="lasso.html"><a href="lasso.html#simulations"><i class="fa fa-check"></i><b>11.5</b> Simulations</a><ul>
<li class="chapter" data-level="11.5.1" data-path="lasso.html"><a href="lasso.html#times-1000-matrix-5-target-coefficients"><i class="fa fa-check"></i><b>11.5.1</b> <span class="math inline">\(50 \times 1000\)</span> Matrix | 5 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.2" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-5-target-coefficients"><i class="fa fa-check"></i><b>11.5.2</b> <span class="math inline">\(50 \times 10000\)</span> Matrix | 5 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.3" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-5-target-coefficients-1"><i class="fa fa-check"></i><b>11.5.3</b> <span class="math inline">\(200 \times 10000\)</span> Matrix | 5 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.4" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-15-target-coefficients"><i class="fa fa-check"></i><b>11.5.4</b> <span class="math inline">\(50 \times 10000\)</span> Matrix | 15 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.5" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-30-target-coefficients"><i class="fa fa-check"></i><b>11.5.5</b> <span class="math inline">\(50 \times 10000\)</span> Matrix | 30 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.6" data-path="lasso.html"><a href="lasso.html#scaling-the-coeficients"><i class="fa fa-check"></i><b>11.5.6</b> Scaling the coeficients</a></li>
<li class="chapter" data-level="11.5.7" data-path="lasso.html"><a href="lasso.html#comparison"><i class="fa fa-check"></i><b>11.5.7</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lasso.html"><a href="lasso.html#conclusion"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a></li>
<li class="chapter" data-level="11.7" data-path="lasso.html"><a href="lasso.html#code-lasso"><i class="fa fa-check"></i><b>11.7</b> Code: Lasso</a></li>
<li class="chapter" data-level="11.8" data-path="lasso.html"><a href="lasso.html#code-lasso-via-cyclic-gradient-descent"><i class="fa fa-check"></i><b>11.8</b> Code: Lasso via cyclic gradient descent</a></li>
<li class="chapter" data-level="11.9" data-path="lasso.html"><a href="lasso.html#code-parallel-lasso"><i class="fa fa-check"></i><b>11.9</b> Code: Parallel lasso</a></li>
<li class="chapter" data-level="11.10" data-path="lasso.html"><a href="lasso.html#references"><i class="fa fa-check"></i><b>11.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="tm.html"><a href="tm.html"><i class="fa fa-check"></i><b>12</b> Topic Models</a><ul>
<li class="chapter" data-level="12.1" data-path="tm.html"><a href="tm.html#variational-inference"><i class="fa fa-check"></i><b>12.1</b> Variational Inference</a><ul>
<li class="chapter" data-level="12.1.1" data-path="tm.html"><a href="tm.html#evidence-lower-bound-elbo"><i class="fa fa-check"></i><b>12.1.1</b> Evidence Lower Bound (ELBO)</a></li>
<li class="chapter" data-level="12.1.2" data-path="tm.html"><a href="tm.html#elbo-and-kl-divergence"><i class="fa fa-check"></i><b>12.1.2</b> ELBO and KL Divergence</a></li>
<li class="chapter" data-level="12.1.3" data-path="tm.html"><a href="tm.html#mean-field-method"><i class="fa fa-check"></i><b>12.1.3</b> Mean Field Method</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="tm.html"><a href="tm.html#lda"><i class="fa fa-check"></i><b>12.2</b> LDA</a><ul>
<li class="chapter" data-level="12.2.1" data-path="tm.html"><a href="tm.html#expectation-of-pthetaalpha"><i class="fa fa-check"></i><b>12.2.1</b> Expectation of p(<span class="math inline">\(\theta|\alpha)\)</span></a></li>
<li class="chapter" data-level="12.2.2" data-path="tm.html"><a href="tm.html#expectation-of-pztheta"><i class="fa fa-check"></i><b>12.2.2</b> Expectation of p(<span class="math inline">\(z|\theta)\)</span></a></li>
<li class="chapter" data-level="12.2.3" data-path="tm.html"><a href="tm.html#expectation-of-pwzbeta"><i class="fa fa-check"></i><b>12.2.3</b> Expectation of p(<span class="math inline">\(w|z,\beta)\)</span></a></li>
<li class="chapter" data-level="12.2.4" data-path="tm.html"><a href="tm.html#entropy-of-gamma-and-phi"><i class="fa fa-check"></i><b>12.2.4</b> Entropy of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\phi\)</span></a></li>
<li class="chapter" data-level="12.2.5" data-path="tm.html"><a href="tm.html#complete-objective-function"><i class="fa fa-check"></i><b>12.2.5</b> Complete Objective Function</a></li>
<li class="chapter" data-level="12.2.6" data-path="tm.html"><a href="tm.html#parameter-optimization"><i class="fa fa-check"></i><b>12.2.6</b> Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="tm.html"><a href="tm.html#supervised-lda"><i class="fa fa-check"></i><b>12.3</b> Supervised LDA</a><ul>
<li class="chapter" data-level="12.3.1" data-path="tm.html"><a href="tm.html#expectations-of-pthetaalpha-pztheta-and-pwzbeta"><i class="fa fa-check"></i><b>12.3.1</b> Expectations of <span class="math inline">\(p(\theta|\alpha)\)</span>, <span class="math inline">\(p(z|\theta)\)</span>, and <span class="math inline">\(p(w|z,\beta)\)</span></a></li>
<li class="chapter" data-level="12.3.2" data-path="tm.html"><a href="tm.html#expectation-of-pyzetasigma2"><i class="fa fa-check"></i><b>12.3.2</b> Expectation of <span class="math inline">\(p(y|z,\eta,\sigma^2)\)</span></a></li>
<li class="chapter" data-level="12.3.3" data-path="tm.html"><a href="tm.html#entropy-of-gamma-and-phi-1"><i class="fa fa-check"></i><b>12.3.3</b> Entropy of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\phi\)</span></a></li>
<li class="chapter" data-level="12.3.4" data-path="tm.html"><a href="tm.html#complete-objective-function-1"><i class="fa fa-check"></i><b>12.3.4</b> Complete Objective Function</a></li>
<li class="chapter" data-level="12.3.5" data-path="tm.html"><a href="tm.html#parameter-optimization-1"><i class="fa fa-check"></i><b>12.3.5</b> Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="tm.html"><a href="tm.html#the-correlated-topic-model"><i class="fa fa-check"></i><b>12.4</b> The Correlated Topic Model</a><ul>
<li class="chapter" data-level="12.4.1" data-path="tm.html"><a href="tm.html#multinomial-distribution-in-exponential-form"><i class="fa fa-check"></i><b>12.4.1</b> Multinomial Distribution in Exponential Form</a></li>
<li class="chapter" data-level="12.4.2" data-path="tm.html"><a href="tm.html#variational-em"><i class="fa fa-check"></i><b>12.4.2</b> Variational EM</a></li>
<li class="chapter" data-level="12.4.3" data-path="tm.html"><a href="tm.html#expectation-of-pwzbeta-1"><i class="fa fa-check"></i><b>12.4.3</b> Expectation of <span class="math inline">\(p(w|z,\beta)\)</span></a></li>
<li class="chapter" data-level="12.4.4" data-path="tm.html"><a href="tm.html#expectation-of-pzeta"><i class="fa fa-check"></i><b>12.4.4</b> Expectation of <span class="math inline">\(p(z|\eta)\)</span></a></li>
<li class="chapter" data-level="12.4.5" data-path="tm.html"><a href="tm.html#expectation-of-petamusigma"><i class="fa fa-check"></i><b>12.4.5</b> Expectation of <span class="math inline">\(p(\eta|\mu,\sigma)\)</span></a></li>
<li class="chapter" data-level="12.4.6" data-path="tm.html"><a href="tm.html#entropy-of-lambda-nu-and-phi"><i class="fa fa-check"></i><b>12.4.6</b> Entropy of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\nu\)</span>, and <span class="math inline">\(\phi\)</span></a></li>
<li class="chapter" data-level="12.4.7" data-path="tm.html"><a href="tm.html#complete-objective-function-2"><i class="fa fa-check"></i><b>12.4.7</b> Complete Objective Function</a></li>
<li class="chapter" data-level="12.4.8" data-path="tm.html"><a href="tm.html#parameter-optimization-2"><i class="fa fa-check"></i><b>12.4.8</b> Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="tm.html"><a href="tm.html#dirichlet-distribution"><i class="fa fa-check"></i><b>12.5</b> Dirichlet Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>13</b> Machine Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="ml.html"><a href="ml.html#cross-valdiation"><i class="fa fa-check"></i><b>13.1</b> Cross Valdiation</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ml.html"><a href="ml.html#loocv"><i class="fa fa-check"></i><b>13.1.1</b> LOOCV</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ml.html"><a href="ml.html#naive-bayes"><i class="fa fa-check"></i><b>13.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="13.3" data-path="ml.html"><a href="ml.html#svm"><i class="fa fa-check"></i><b>13.3</b> SVM</a><ul>
<li class="chapter" data-level="13.3.1" data-path="ml.html"><a href="ml.html#manual-rbf-kernal"><i class="fa fa-check"></i><b>13.3.1</b> Manual rbf kernal</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ml.html"><a href="ml.html#k-means"><i class="fa fa-check"></i><b>13.4</b> K-means</a></li>
<li class="chapter" data-level="13.5" data-path="ml.html"><a href="ml.html#gaussian-mixtures"><i class="fa fa-check"></i><b>13.5</b> Gaussian Mixtures</a></li>
<li class="chapter" data-level="13.6" data-path="ml.html"><a href="ml.html#pca"><i class="fa fa-check"></i><b>13.6</b> PCA</a></li>
<li class="chapter" data-level="13.7" data-path="ml.html"><a href="ml.html#viterbi-algorithm"><i class="fa fa-check"></i><b>13.7</b> Viterbi Algorithm</a></li>
<li class="chapter" data-level="13.8" data-path="ml.html"><a href="ml.html#gradient-descent"><i class="fa fa-check"></i><b>13.8</b> Gradient Descent</a><ul>
<li class="chapter" data-level="13.8.1" data-path="ml.html"><a href="ml.html#linear-regression"><i class="fa fa-check"></i><b>13.8.1</b> Linear Regression</a></li>
<li class="chapter" data-level="13.8.2" data-path="ml.html"><a href="ml.html#logistic-regression"><i class="fa fa-check"></i><b>13.8.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="13.8.3" data-path="ml.html"><a href="ml.html#softmax-regression"><i class="fa fa-check"></i><b>13.8.3</b> Softmax regression</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="ml.html"><a href="ml.html#nonparametric-bayesian-processes"><i class="fa fa-check"></i><b>13.9</b> Nonparametric Bayesian Processes</a><ul>
<li class="chapter" data-level="13.9.1" data-path="ml.html"><a href="ml.html#chinese-restaurant"><i class="fa fa-check"></i><b>13.9.1</b> Chinese Restaurant</a></li>
<li class="chapter" data-level="13.9.2" data-path="ml.html"><a href="ml.html#polyas-urn"><i class="fa fa-check"></i><b>13.9.2</b> Polyas Urn</a></li>
<li class="chapter" data-level="13.9.3" data-path="ml.html"><a href="ml.html#stick-breaking"><i class="fa fa-check"></i><b>13.9.3</b> Stick Breaking</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="ml.html"><a href="ml.html#iteratively-reweighted-least-squares"><i class="fa fa-check"></i><b>13.10</b> Iteratively Reweighted Least Squares</a></li>
<li class="chapter" data-level="13.11" data-path="ml.html"><a href="ml.html#neural-network"><i class="fa fa-check"></i><b>13.11</b> Neural Network</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="microbiome.html"><a href="microbiome.html"><i class="fa fa-check"></i><b>14</b> Microbiome Data Analysis</a><ul>
<li class="chapter" data-level="14.1" data-path="microbiome.html"><a href="microbiome.html#loading-and-exploring-the-guerroro-negro-data"><i class="fa fa-check"></i><b>14.1</b> Loading and Exploring the Guerroro Negro Data</a></li>
<li class="chapter" data-level="14.2" data-path="microbiome.html"><a href="microbiome.html#statistical-analysis"><i class="fa fa-check"></i><b>14.2</b> Statistical Analysis</a><ul>
<li class="chapter" data-level="14.2.1" data-path="microbiome.html"><a href="microbiome.html#pca-1"><i class="fa fa-check"></i><b>14.2.1</b> PCA</a></li>
<li class="chapter" data-level="14.2.2" data-path="microbiome.html"><a href="microbiome.html#rda"><i class="fa fa-check"></i><b>14.2.2</b> RDA</a></li>
<li class="chapter" data-level="14.2.3" data-path="microbiome.html"><a href="microbiome.html#ca"><i class="fa fa-check"></i><b>14.2.3</b> CA</a></li>
<li class="chapter" data-level="14.2.4" data-path="microbiome.html"><a href="microbiome.html#cca"><i class="fa fa-check"></i><b>14.2.4</b> CCA</a></li>
<li class="chapter" data-level="14.2.5" data-path="microbiome.html"><a href="microbiome.html#pcoa"><i class="fa fa-check"></i><b>14.2.5</b> PCoA</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/sw1/" target="blank">Published by Stephen Woloszynek</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Worked Bioninformatics, Statistics, and Machine Learning Examples</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lasso" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Lasso</h1>
<div id="big-data-and-feature-selection" class="section level2">
<h2><span class="header-section-number">11.1</span> Big Data and Feature Selection</h2>
<p>A common hurdle found when dealing with big data is determining which covariates are relevant prior to model fitting. One could fit a full model, but when dimensionality of the potential features reaches the order of thousands, both the demand on computational resources and time often makes such a strategy impractical. Choosing relevant features ahead of time allows the user to simultaneously explore relevant models and features, while avoiding the unnecessary computational cost of iteratively performing a step-wise model fitting procedure (either forwards or backwards) to arrive at a good fit.</p>
<p>Feature selection methods often employ information theory measures to determine the most explanatory set of features for a particular outcome (e.g., joint mutual information and minimum redundancy maximum relevance). Another strategy is to perform L1 regularized regression (i.e., lasso), where, like ordinary regression, the least squares loss function is minimized, but constraint is also applied to the absolute sum of regression coefficients. Using the L1 norm has important consequences in feature selection. If one uses the L2 norm (i.e., ridge regression), every feature will be conserved in the final solution; hence, if one starts with 100 covariates, then the final regression equation will too have 100 non-zero covariates. The L1 norm, on the other hand, shrinks less predictive covariates to zero as a function of the weighting parameter <span class="math inline">\(\lambda\)</span>. The final regression equation is therefore a sparse solution in the case of the L1 norm.</p>
</div>
<div id="lasso-regression" class="section level2">
<h2><span class="header-section-number">11.2</span> Lasso Regression</h2>
<p>Lasso can be formulated as follows: <span class="math display">\[\underset{\beta}{\mathrm{min}} \sum_{i=1}^n (y_i - \beta x_{i})^2 s.t. |\beta| \le s\]</span>. The constraint on <span class="math inline">\(\beta\)</span> can be interpreted as for any value s, there is a corresponding value <span class="math inline">\(\lambda\)</span> that places an upper bound on the absolute sum of the coefficients. A large s value – and hence <em>small</em> <span class="math inline">\(\lambda\)</span> value – results in a sum that is large, implying many coefficients were not set to zero. Small values of s yield a small sum and hence a sparse solution.</p>
<p>Solving the lasso problem is more difficult than simple linear regression. The first half of the equation above is the same equation seen in OLS regression. Solving this is easy: simply take the derivative, set to zero, and solve for <span class="math inline">\(\beta\)</span>. We can see this first hand in gradient descent implementations of regression. The right half of the equation is where things get complicated; because the derivative of <span class="math inline">\(|\beta|\)</span> is not defined at <span class="math inline">\(\beta=0\)</span>, a subdifferential approach is needed, resulting in the following sub thresholding function:</p>
<p><span class="math display">\[
  \hat \beta_j = \left\{\def\arraystretch{1.2}%
  \begin{array}{@{}c@{\quad}l@{}}
    y_j - \lambda/2 &amp; \text{if $y_j &gt; \lambda/2$}\\
    y_j + \lambda/2 &amp; \text{if $y_j &lt; - \lambda/2$}\\
    0               &amp; \text{if $|y_j| \le \lambda/2$}\\
  \end{array}\right.
\]</span></p>
<p>We can see the effect of varying <span class="math inline">\(\lambda\)</span> values in the figure below:</p>
<div class="figure">
<img src="figs/lasso/1.png" />

</div>
<p>The value of <span class="math inline">\(y_j\)</span> is represented by the black lines, whereas the subthresholding transformation is represented by the blue lines. For values outside of the <span class="math inline">\(|y_j|\)</span> interval, the function essentially drives <span class="math inline">\(y_j\)</span> closer to 0. Once it’s within that interval, <span class="math inline">\(y_j\)</span> is <em>set</em> to zero. The size of this interval is determined by <span class="math inline">\(\lambda\)</span>, with larger values producing a larger interval, and consequently, a more sparse solution.</p>
</div>
<div id="multivariate-lasso-regression" class="section level2">
<h2><span class="header-section-number">11.3</span> Multivariate Lasso Regression</h2>
<p>In the multivariate case, the lasso problem becomes <span class="math display">\[\underset{\beta}{\mathrm{min}} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\beta||_1\]</span>, which can only be solved explicitly if <span class="math inline">\(X^TX=I\)</span>, a case that isn’t met when <span class="math inline">\(p&gt;n\)</span>. Coordinate descent provides a solution for the multivariate regression problem. Here, the subthresholding function becomes:</p>
<p><span class="math display">\[
  \hat \beta_j = \left\{\def\arraystretch{1.2}%
  \begin{array}{@{}c@{\quad}l@{}}
    c_j - p\lambda/a_j &amp; \text{if $c_j &gt; p\lambda$}\\
    c_j + p\lambda/a_j &amp; \text{if $c_j &lt; - p\lambda$}\\
    0                  &amp; \text{if $|c_j| \le p\lambda$}\\
  \end{array}\right.
\]</span></p>
<p>where p is the dimensionality of the feature vector. Because it’s easier to optimize each <span class="math inline">\(jth\)</span> coefficient individually, instead of solving for the entire feature vector simultaneously, we solve for the <span class="math inline">\(jth\)</span> coefficient while holding all others fixed. This results in <span class="math inline">\(c_j = X_{\cdot,j}^T(y - X_{\cdot,-j}\beta_{-j})\)</span> and <span class="math inline">\(a_j = \sum_{i=1}^n x_{i,-j}\)</span>. This is often performed sequentially, i.e., for each iteration each <span class="math inline">\(\beta\)</span> is updated, one-by-one. This is called the shooting algorithm. A slight adjustment allows for easy parallel implementation.</p>
</div>
<div id="parallel-coordinate-descent" class="section level2">
<h2><span class="header-section-number">11.4</span> Parallel Coordinate Descent</h2>
<p>Unlike before where each <span class="math inline">\(\beta_j\)</span> is updated during each iteration until convergence, an alternative strategy is to randomly sample index <span class="math inline">\(j\)</span> to update during each iteration. While this method would require more iterations, each iteration will be faster, and it will still ultimately converge. Because the <span class="math inline">\(jth\)</span> index is now being sampled, an obvious extension is to simply distribute a randomly sampled index to each of m processors on a cluster. Each processor can than update its respective <span class="math inline">\(\beta_j\)</span> independently, resulting in m simultaneous updates per iteration.</p>
<p>This requires a rather simple implementation via MPI. Set vector <strong>B</strong> of length p to 0. Given m processors, m indexes are drawn from a uniform distribution ranging 1 to m. This index vector is then sent to each processor via MPI_Scatter. Each processor independently updates <span class="math inline">\(a_j\)</span>, <span class="math inline">\(c_j\)</span>, and ultimately <span class="math inline">\(\beta_j\)</span>. The updated <span class="math inline">\(\beta_j\)</span>s are then returned to the head node using MPI_gather, where the original vector <strong>B</strong> is then updated and then distributed across all processes with MPI_bcast for future iterations. This is repeated until convergence – i.e., when two sequential iterations result in a change of the objective function less than a predetermined tolerance value. The objective function function is defined as <span class="math display">\[\frac{1}{2m} ||y-X\beta||_2^2 + \lambda \sum_{i=1}^n |\beta|\]</span>.</p>
</div>
<div id="simulations" class="section level2">
<h2><span class="header-section-number">11.5</span> Simulations</h2>
<p>All matrices were generated as follows: X, a <span class="math inline">\(m \times n\)</span> matrix, and k <span class="math inline">\(\beta\)</span> values were sampled from a normal distribution (0,1). The k <span class="math inline">\(\beta\)</span> values were set to k randomly sampled indexes sampled from a uniform distribution (1,n) in a <span class="math inline">\(m \times 1\)</span> vector . All other values were set to 0. Finally, y was calculated via <span class="math inline">\(y = X^T b\)</span>.</p>
<div id="times-1000-matrix-5-target-coefficients" class="section level3">
<h3><span class="header-section-number">11.5.1</span> <span class="math inline">\(50 \times 1000\)</span> Matrix | 5 Target Coefficients</h3>
<p>Before any analysis of the results was performed, there seemed to be numerical issues when performing the algorithm on small <span class="math inline">\(\lambda\)</span> values (<span class="math inline">\(&lt;.05\)</span>). The objective and all <span class="math inline">\(\beta\)</span> coefficients would increase exponentially after every update. Only with larger <span class="math inline">\(\lambda\)</span> values would the algorithm converge (and note it would always converge to the correct values). It turned out this was unrelated to the code; there were no errors. Instead, it was a function of the number of cores set. All initial runs were performed with 64 cores, resulting in the numerical issue for small <span class="math inline">\(\lambda\)</span> values. Note that this is neither an MPI problem nor a problem inherent to the cluster. The issue lies in selecting more simultaneous updates than there is data (rows in matrix X), which, in this simulation, there are 50. See the figure below:</p>
<div class="figure">
<img src="figs/lasso/2.png" />

</div>
<p>If we increase the sample size to 100, then the convergence problem is gone:</p>
<div class="figure">
<img src="figs/lasso/3.png" />

</div>
<p>And the ability to achieve convergence as a function of processors is also dictated by <span class="math inline">\(\lambda\)</span>:</p>
<div class="figure">
<img src="figs/lasso/4.png" />

</div>
<p>Here are the lasso traces for 10 independent samples for the above parameter settings</p>
<div class="figure">
<img src="figs/lasso/5.jpg" />

</div>
<p>The text on the right hand side represents the indexes of the true coefficients used to generate the system for each sample. Their height represents their true value. We can easily see which traces correspond to which index, and moreover, we can see the <span class="math inline">\(\lambda\)</span> value in which the traces achieve the correct value. Every sample managed to both identify and estimate the correct index and its coefficient at some point throughout the trace. The figures seem to suggest that <span class="math inline">\(\lambda=-.005\)</span> did the best job at estimating the true values. Coefficients with larger values were more robust to smaller <span class="math inline">\(\lambda\)</span> values. For example, in sample 7, coefficients 154 and 895 were nonzero for <span class="math inline">\(\lambda\)</span> values well over 1. Coefficient 399 was driven to zero around <span class="math inline">\(\lambda=.5\)</span>, whereas the remaining two coefficients were set to 0 much sooner.</p>
<p>Here is the error at each <span class="math inline">\(\lambda\)</span> averaged across samples:</p>
<div class="figure">
<img src="figs/lasso/6.jpg" />

</div>
<p>The error term here is measured as the following: <span class="math display">\[\mathrm{Error}=\frac{||\hat x - x||_2}{||x||_2}\]</span>. This figure confirms <span class="math inline">\(\lambda=.005\)</span> performing the best. It should be noted that there was an iteration cap, so the very small <span class="math inline">\(\lambda\)</span> values likely just failed to reach a reasonable approximation. While they clearly were converging and hence approaching a reasonable solution, it simply would have taken too long and these values were the result of 20,000 iterations using 48 cores. The error began to rise somewhat around <span class="math inline">\(\lambda=.779\)</span> The hump in this region and the subsequent decline can be attributed to the variability regarding when certain coefficients are driven to 0. Samples 1 and 2 have a few relatively robust coefficients that help decrease the error, whereas the coefficients in sample 4 are probably key contributors in the hump around <span class="math inline">\(\lambda=.779\)</span>.</p>
</div>
<div id="times-10000-matrix-5-target-coefficients" class="section level3">
<h3><span class="header-section-number">11.5.2</span> <span class="math inline">\(50 \times 10000\)</span> Matrix | 5 Target Coefficients</h3>
<div class="figure">
<img src="figs/lasso/7.jpg" />

</div>
<p>Now we are dealing with 10 times more unwanted coefficients, and the figures make it quite obvious that lasso is having more trouble than the <span class="math inline">\(50 \times 1000\)</span> case. Sample 10 managed to capture only 1 coefficient for a prolonged stretch, and it also had quite a few false positives for moderate <span class="math inline">\(\lambda\)</span> values. Sample 3 also had quite a few false positives, but farther along than sample 10. Samples 7 and 8 detected only 2 of the 5 coefficients. Like the previous simulation, larger true coefficients were more robust at being detected across <span class="math inline">\(\lambda\)</span> values, exemplified quite well by sample 2.</p>
<div class="figure">
<img src="figs/lasso/8.jpg" />

</div>
<p>Unlike the last simulation, <span class="math inline">\(\lambda=.005\)</span> performed poorly. The best value seemed to be around <span class="math inline">\(\lambda=.058\)</span>, although each value ranging from .058 to .371 did quite well. Still, even for .058, the error was larger (approximately .300) than .005 (.009) for the last simulation</p>
</div>
<div id="times-10000-matrix-5-target-coefficients-1" class="section level3">
<h3><span class="header-section-number">11.5.3</span> <span class="math inline">\(200 \times 10000\)</span> Matrix | 5 Target Coefficients</h3>
<div class="figure">
<img src="figs/lasso/9.jpg" />

</div>
<p>By increasing the number of data points from 50 to 200, there seems to be a return to the behavior we saw in the <span class="math inline">\(50 \times 1000\)</span> simulation. Notwithstanding sample 6, all of the samples managed to estimate at least 4 of the 5 coefficients. Again, if the coefficient was truly large, then it behaved robustly across all <span class="math inline">\(\lambda\)</span> values.</p>
<div class="figure">
<img src="figs/lasso/10.jpg" />

</div>
<p>The error too seems similar to the <span class="math inline">\(50 \times 1000\)</span> simulation. The minimum ended up being at <span class="math inline">\(\lambda=0.007\)</span>, analogous to the minimum of .005 we saw in the smaller system. The error managed to stay low even for a slightly larger <span class="math inline">\(\lambda\)</span> (.01). Like all of the simulations so far, there is a hump at the moderate <span class="math inline">\(\lambda\)</span> values.</p>
<p>Based on the last 3 simulations, there seems to be a trade-off between the amount of data (i.e., the number of rows in <span class="math inline">\(X\)</span>) and the number of coefficients (i.e., the columns in <span class="math inline">\(X\)</span>). More coefficients gives lasso problems, but given more data, then lasso returns to form. Let’s see how lasso does at estimating more coefficients.</p>
</div>
<div id="times-10000-matrix-15-target-coefficients" class="section level3">
<h3><span class="header-section-number">11.5.4</span> <span class="math inline">\(50 \times 10000\)</span> Matrix | 15 Target Coefficients</h3>
<div class="figure">
<img src="figs/lasso/11.jpg" />

</div>
<p>Now lasso seems to be struggling to find the true coefficients. Other than sample 4, which managed to only capture about 6 of the 15 coefficients, all of the samples performed poorly. Other than a few large coefficients, most traces were lost in false positives. Also, unlike the systems with only 5 true coefficients, this simulation resulted in many incorrect traces of similar magnitude to the true coefficients, represented by the black lines. It’s surprising that lasso even failed to capture the larger coefficients (see the labels far from the x-axis). The fact that there are quite a few coefficients not much larger than 0 in terms of magnitude (recall they are sampled from a normal distribution (0,1)) gives lasso problems setting any potentially non-zero coefficient to zero. This may be remedied by a smaller tolerance, but most likely, it won’t make a difference. For example, with <span class="math inline">\(\lambda=2.\)</span> (sample 4), the tolerance level was reached after only 1999 iterations, managing only to capture 4 coefficients, all of which are far smaller in magnitude than their true counterparts. A smaller <span class="math inline">\(\lambda\)</span> of 0.01 (sample 10) reached the iteration cap of 20,000, but ended up with 317 coefficients, far more than the target 15. A more moderate value, <span class="math inline">\(\lambda=1.05\)</span> (sample 1), broke after 5499 iterations, ending with 21 coefficients, which is closer to 15, but nevertheless failed to capture any of the true estimates of the larger coefficients since no trace reaches the required height.</p>
<div class="figure">
<img src="figs/lasso/12.jpg" />

</div>
<p>Note the gap in the plot was due to the aforementioned convergence/core issue; hence, those <span class="math inline">\(\lambda\)</span> values were omitted. The error here essentially reaches a plateau for the majority of the <span class="math inline">\(\lambda\)</span> values. The error drops at the end, but this is not due to improved estimation, but instead a result of driving many of the incorrect estimates to zero.</p>
<p>Considering this situation, if one has a dataset with many potentially informative coefficients, unless some yield significantly large estimates relative to the majority, then lasso will likely have problems. Since this simulation dealt with true coefficients near 0, many false positives resulted. A noisy dataset probably requires the use of larger <span class="math inline">\(\lambda\)</span> values and the hope that the some coefficients are significantly more informative than the others.</p>
</div>
<div id="times-10000-matrix-30-target-coefficients" class="section level3">
<h3><span class="header-section-number">11.5.5</span> <span class="math inline">\(50 \times 10000\)</span> Matrix | 30 Target Coefficients</h3>
<div class="figure">
<img src="figs/lasso/13.jpg" />

</div>
<p>And here’s an even noisier system, with 30 true coefficients. Only sample 8 was able to separate a coefficient from the crown. Driving <span class="math inline">\(\lambda\)</span> up further for this sample would likely isolate it. Sample 5 also had a large robust estimate, but it was a false positive. Clearly the conclusion reached in the last simulation can only be confirmed here.</p>
<div class="figure">
<img src="figs/lasso/14.jpg" />

</div>
<p>The trend here is similar to the system with 15 true values. If we increase <span class="math inline">\(\lambda\)</span> further, we would likely see the same drop in error we see before since we’d be driving the many incorrect estimates to zero.</p>
</div>
<div id="scaling-the-coeficients" class="section level3">
<h3><span class="header-section-number">11.5.6</span> Scaling the coeficients</h3>
<p>It seems plausible that as we increase the number of false coefficients, we decrease the chance of identifying true coefficients unless those coefficients are much different than the majority – i.e., more informative. Instead of increasing the sampling size this time, let’s increase the magnitude of the true coefficients by scaling them 10-fold. Using a <em>very</em> large <span class="math inline">\(50 \times 100000\)</span> matrix, we get the following:</p>
<div class="figure">
<img src="figs/lasso/15.jpg" />

</div>
<p>Even with 100,000 potential coefficients to work with, lasso managed to cleanly identify one of the five targets. Moreover, while there are quite a few false positives around <span class="math inline">\(\lambda=.211\)</span> (30), there are <em>far</em> less than 100,000, and 3 total true coefficients were still detectable. At <span class="math inline">\(\lambda=.525\)</span>, 4 coefficients were returned, albeit only 1 being correct. Had we used larger <span class="math inline">\(\lambda\)</span> values, we likely would have ended with at most 1 true and 1 false coefficient. Note that the 5 starting coefficients had values of -4.79, 9.88, 3.12, 1.22, and 7.02. That robust coefficient was, to no one’s surprise, the coefficient with the largest value in magnitude of the 5, 9.88.</p>
</div>
<div id="comparison" class="section level3">
<h3><span class="header-section-number">11.5.7</span> Comparison</h3>
<div class="figure">
<img src="figs/lasso/16.jpg" />

</div>
<p>Here is a figure showing all of the error traces. The small system (<span class="math inline">\(50 \times 1000\)</span>) and the large system with more data (<span class="math inline">\(200 \times 10000\)</span>) performed the best assuming a small <span class="math inline">\(\lambda\)</span> was used. Interestingly, if only uses a larger <span class="math inline">\(\lambda\)</span>, then that small system tends to perform the worst. Still, it’s quite clear that identifying informative features is a function of the amount of data, the number of features, the number of true coefficients, and their size.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">11.6</span> Conclusion</h2>
<p>A parallelized implementation of the coordinate descent algorithm for lasso clearly works well for large datasets. The issue with cores and the number of rows is an interesting quirk, but nevertheless easily avoidable. Once that was accounted for, no numerical issues resulted, and estimates were consistent with the predetermined values designed for the simulations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>Boston$medv <span class="co"># median housing value</span>
X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(Boston[,-<span class="dv">14</span>]) <span class="co"># all other predictors</span>

y &lt;-<span class="st"> </span><span class="kw">scale</span>(y,<span class="dt">center=</span><span class="ot">TRUE</span>,<span class="dt">scale=</span><span class="ot">FALSE</span>)
X &lt;-<span class="st"> </span><span class="kw">scale</span>(X,<span class="dt">center =</span> <span class="ot">TRUE</span>,<span class="dt">scale =</span> <span class="ot">FALSE</span>)

B &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(X))
lambda &lt;-<span class="st"> </span><span class="dv">2</span>
tol &lt;-<span class="st"> </span><span class="fl">1e-8</span>
m &lt;-<span class="st"> </span><span class="kw">length</span>(y)

iter &lt;-<span class="st"> </span><span class="dv">1000</span>

obj &lt;-<span class="st"> </span><span class="kw">numeric</span>(iter +<span class="st"> </span><span class="dv">1</span>)
B.list &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span>:(iter +<span class="st"> </span><span class="dv">1</span>), function(x) x)
B.list[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>B

for (j in <span class="dv">1</span>:iter) {
  
  k &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(B),<span class="dv">1</span>)
  
  <span class="co"># 0 for B0 but centered so 0</span>
  <span class="co"># calculate Bj for for all j not in k</span>

  cj &lt;-<span class="st"> </span>(X[,k] %*%<span class="st"> </span>(y -<span class="st"> </span>X[,-k] %*%<span class="st"> </span>B[-k])) <span class="co"># *2</span>
  aj &lt;-<span class="st"> </span><span class="kw">sum</span>(X[,k]^<span class="dv">2</span>)  <span class="co"># *2</span>
  
  <span class="co"># shrink</span>
  Bj &lt;-<span class="st"> </span><span class="dv">0</span>
  if (cj &lt;<span class="st"> </span>-lambda*m) Bj &lt;-<span class="st"> </span>(cj +<span class="st"> </span>lambda*m)/aj
  if (cj &gt;<span class="st"> </span>lambda*m) Bj &lt;-<span class="st"> </span>(cj -<span class="st"> </span>lambda*m)/aj

  B[k] &lt;-<span class="st"> </span>Bj
  
  B.list[[(j +<span class="st"> </span><span class="dv">1</span>)]] &lt;-<span class="st"> </span>B
  obj[j] &lt;-<span class="st"> </span>(<span class="dv">1</span>/<span class="dv">2</span>)*(<span class="dv">1</span>/m)*<span class="kw">norm</span>(y -<span class="st"> </span>X %*%<span class="st"> </span>B,<span class="st">&quot;F&quot;</span>)^<span class="dv">2</span> +<span class="st"> </span>lambda*<span class="kw">sum</span>(<span class="kw">abs</span>(B))
  
  <span class="co">#if (sqrt(sum((B.list[[j]] - B.list[[j+1]])^2)) &lt; tol) break</span>
} 

g &lt;-<span class="st"> </span>(<span class="dv">1</span>/m)*<span class="kw">t</span>(X) %*%<span class="st"> </span>(y -<span class="st"> </span>X %*%<span class="st"> </span>B)
if (<span class="kw">any</span>(<span class="kw">abs</span>(g[<span class="kw">which</span>(B ==<span class="st"> </span><span class="dv">0</span>)]) &gt;<span class="st"> </span>lambda +<span class="st"> </span>tol)) <span class="kw">print</span>(<span class="st">&quot;No minimum&quot;</span>)
if (<span class="kw">any</span>(g[<span class="kw">which</span>(B !=<span class="st"> </span><span class="dv">0</span>)] &gt;<span class="st"> </span>tol +<span class="st"> </span>lambda*<span class="kw">sign</span>(B[<span class="kw">which</span>(B !=<span class="st"> </span><span class="dv">0</span>)]))) <span class="kw">print</span>(<span class="st">&quot;No minimum&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;No minimum&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B</code></pre></div>
<pre><code>##  [1] -0.021552320294  0.035523120761  0.000000000000  0.000000000000
##  [5]  0.000000000000  0.000000000000  0.043564050481 -0.067585897623
##  [9]  0.173365654135 -0.011672463466 -0.557109230058  0.007065846342
## [13] -0.821549115759</code></pre>
</div>
<div id="code-lasso" class="section level2">
<h2><span class="header-section-number">11.7</span> Code: Lasso</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>Boston$medv <span class="co"># median housing value</span>
X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(Boston[,-<span class="dv">14</span>]) <span class="co"># all other predictors</span>

y &lt;-<span class="st"> </span><span class="kw">scale</span>(y,<span class="dt">center=</span><span class="ot">TRUE</span>,<span class="dt">scale=</span><span class="ot">FALSE</span>)
X &lt;-<span class="st"> </span><span class="kw">scale</span>(X,<span class="dt">center =</span> <span class="ot">TRUE</span>,<span class="dt">scale =</span> <span class="ot">FALSE</span>)

B &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(X))
lambda &lt;-<span class="st"> </span><span class="dv">2</span>
tol &lt;-<span class="st"> </span><span class="fl">1e-8</span>
m &lt;-<span class="st"> </span><span class="kw">length</span>(y)

iter &lt;-<span class="st"> </span><span class="dv">1000</span>

obj &lt;-<span class="st"> </span><span class="kw">numeric</span>(iter +<span class="st"> </span><span class="dv">1</span>)
B.list &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span>:(iter +<span class="st"> </span><span class="dv">1</span>), function(x) x)
B.list[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>B

for (j in <span class="dv">1</span>:iter) {
  
  k &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(B),<span class="dv">1</span>)
  
  <span class="co"># 0 for B0 but centered so 0</span>
  <span class="co"># calculate Bj for for all j not in k</span>

  cj &lt;-<span class="st"> </span>(X[,k] %*%<span class="st"> </span>(y -<span class="st"> </span>X[,-k] %*%<span class="st"> </span>B[-k])) <span class="co"># *2</span>
  aj &lt;-<span class="st"> </span><span class="kw">sum</span>(X[,k]^<span class="dv">2</span>)  <span class="co"># *2</span>
  
  <span class="co"># shrink</span>
  Bj &lt;-<span class="st"> </span><span class="dv">0</span>
  if (cj &lt;<span class="st"> </span>-lambda*m) Bj &lt;-<span class="st"> </span>(cj +<span class="st"> </span>lambda*m)/aj
  if (cj &gt;<span class="st"> </span>lambda*m) Bj &lt;-<span class="st"> </span>(cj -<span class="st"> </span>lambda*m)/aj

  B[k] &lt;-<span class="st"> </span>Bj
  
  B.list[[(j +<span class="st"> </span><span class="dv">1</span>)]] &lt;-<span class="st"> </span>B
  obj[j] &lt;-<span class="st"> </span>(<span class="dv">1</span>/<span class="dv">2</span>)*(<span class="dv">1</span>/m)*<span class="kw">norm</span>(y -<span class="st"> </span>X %*%<span class="st"> </span>B,<span class="st">&quot;F&quot;</span>)^<span class="dv">2</span> +<span class="st"> </span>lambda*<span class="kw">sum</span>(<span class="kw">abs</span>(B))
  
  <span class="co">#if (sqrt(sum((B.list[[j]] - B.list[[j+1]])^2)) &lt; tol) break</span>
} 

g &lt;-<span class="st"> </span>(<span class="dv">1</span>/m)*<span class="kw">t</span>(X) %*%<span class="st"> </span>(y -<span class="st"> </span>X %*%<span class="st"> </span>B)
if (<span class="kw">any</span>(<span class="kw">abs</span>(g[<span class="kw">which</span>(B ==<span class="st"> </span><span class="dv">0</span>)]) &gt;<span class="st"> </span>lambda +<span class="st"> </span>tol)) <span class="kw">print</span>(<span class="st">&quot;No min&quot;</span>)
if (<span class="kw">any</span>(g[<span class="kw">which</span>(B !=<span class="st"> </span><span class="dv">0</span>)] &gt;<span class="st"> </span>tol +<span class="st"> </span>lambda*<span class="kw">sign</span>(B[<span class="kw">which</span>(B !=<span class="st"> </span><span class="dv">0</span>)]))) <span class="kw">print</span>(<span class="st">&quot;No min&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;No min&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B</code></pre></div>
<pre><code>##  [1] -0.02157812903  0.03552762001  0.00000000000  0.00000000000  0.00000000000
##  [6]  0.00000000000  0.04356367353 -0.06769672154  0.17352502255 -0.01168198944
## [11] -0.55710594088  0.00706537907 -0.82151479590</code></pre>
</div>
<div id="code-lasso-via-cyclic-gradient-descent" class="section level2">
<h2><span class="header-section-number">11.8</span> Code: Lasso via cyclic gradient descent</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>Boston$medv <span class="co"># median housing value</span>
X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(Boston[,-<span class="dv">14</span>]) <span class="co"># all other predictors</span>

y &lt;-<span class="st"> </span><span class="kw">scale</span>(y,<span class="dt">center=</span><span class="ot">TRUE</span>,<span class="dt">scale=</span><span class="ot">FALSE</span>)
X &lt;-<span class="st"> </span><span class="kw">scale</span>(X,<span class="dt">center =</span> <span class="ot">TRUE</span>,<span class="dt">scale =</span> <span class="ot">FALSE</span>)

B &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(X))
lambda &lt;-<span class="st"> </span><span class="dv">2</span>
tol &lt;-<span class="st"> </span><span class="fl">1e-8</span>
m &lt;-<span class="st"> </span><span class="kw">length</span>(y)

iter &lt;-<span class="st"> </span><span class="dv">500</span>

obj &lt;-<span class="st"> </span><span class="kw">numeric</span>(iter +<span class="st"> </span><span class="dv">1</span>)
B.list &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span>:(iter +<span class="st"> </span><span class="dv">1</span>), function(x) x)
B.list[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>B

<span class="co"># cyclic gradient descent</span>
for (j in <span class="dv">1</span>:iter) {
  
  k &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(B),<span class="dv">2</span>)
  kk &lt;-<span class="st"> </span>k[<span class="dv">1</span>]
  k &lt;-<span class="st"> </span>k[<span class="dv">2</span>]
  
  <span class="co"># 0 for B0 but centered so 0</span>
  <span class="co"># calculate Bj for for all j not in k</span>
  
  cj &lt;-<span class="st"> </span>(X[,k] %*%<span class="st"> </span>(y -<span class="st"> </span>X[,-k] %*%<span class="st"> </span>B[-k])) <span class="co"># *2</span>
  aj &lt;-<span class="st"> </span><span class="kw">sum</span>(X[,k]^<span class="dv">2</span>)  <span class="co"># *2</span>
  Bj &lt;-<span class="st"> </span><span class="dv">0</span>
  if (cj &lt;<span class="st"> </span>-lambda*m) Bj &lt;-<span class="st"> </span>(cj +<span class="st"> </span>lambda*m)/aj
  if (cj &gt;<span class="st"> </span>lambda*m) Bj &lt;-<span class="st"> </span>(cj -<span class="st"> </span>lambda*m)/aj
  B[k] &lt;-<span class="st"> </span>Bj
  
  cj &lt;-<span class="st"> </span>(X[,kk] %*%<span class="st"> </span>(y -<span class="st"> </span>X[,-kk] %*%<span class="st"> </span>B[-kk])) <span class="co"># *2</span>
  aj &lt;-<span class="st"> </span><span class="kw">sum</span>(X[,kk]^<span class="dv">2</span>)  <span class="co"># *2</span>
  Bj &lt;-<span class="st"> </span><span class="dv">0</span>
  if (cj &lt;<span class="st"> </span>-lambda*m) Bj &lt;-<span class="st"> </span>(cj +<span class="st"> </span>lambda*m)/aj
  if (cj &gt;<span class="st"> </span>lambda*m) Bj &lt;-<span class="st"> </span>(cj -<span class="st"> </span>lambda*m)/aj
  B[kk] &lt;-<span class="st"> </span>Bj
  
  <span class="co">#B.list[[(j + 1)]] &lt;- B</span>
  <span class="co">#obj[j] &lt;- (1/2)*(1/m)*norm(y - X %*% B,&quot;F&quot;)^2 + lambda*sum(abs(B))</span>
  
  <span class="co">#if (sqrt(sum((B.list[[j]] - B.list[[j+1]])^2)) &lt; tol) break</span>
} 

g &lt;-<span class="st"> </span>(<span class="dv">1</span>/m)*<span class="kw">t</span>(X) %*%<span class="st"> </span>(y -<span class="st"> </span>X %*%<span class="st"> </span>B)
if (<span class="kw">any</span>(<span class="kw">abs</span>(g[<span class="kw">which</span>(B ==<span class="st"> </span><span class="dv">0</span>)]) &gt;<span class="st"> </span>lambda +<span class="st"> </span>tol)) <span class="kw">print</span>(<span class="st">&quot;No min&quot;</span>)
if (<span class="kw">any</span>(g[<span class="kw">which</span>(B !=<span class="st"> </span><span class="dv">0</span>)] &gt;<span class="st"> </span>tol +<span class="st"> </span>lambda*<span class="kw">sign</span>(B[<span class="kw">which</span>(B !=<span class="st"> </span><span class="dv">0</span>)]))) <span class="kw">print</span>(<span class="st">&quot;No min&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;No min&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B</code></pre></div>
<pre><code>##  [1] -0.021551619080  0.035518538258  0.000000000000  0.000000000000
##  [5]  0.000000000000  0.000000000000  0.043562505958 -0.067622063355
##  [9]  0.173361916541 -0.011674045311 -0.557132361153  0.007066159619
## [13] -0.821537236860</code></pre>
</div>
<div id="code-parallel-lasso" class="section level2">
<h2><span class="header-section-number">11.9</span> Code: Parallel lasso</h2>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="ot">#include &lt;petscsys.h&gt;</span>
<span class="ot">#include &lt;petscis.h&gt;</span>
<span class="ot">#include &lt;petscvec.h&gt;</span>
<span class="ot">#include &lt;petscmat.h&gt;</span>
<span class="ot">#include &lt;petscmath.h&gt;</span>
<span class="ot">#include &lt;stdio.h&gt;</span>

<span class="dt">float</span> coord_descent(PetscScalar *y_array, <span class="dt">float</span> *beta_array, <span class="dt">int</span> rend, <span class="dt">int</span> cend, Mat matrix, <span class="dt">int</span> idx, <span class="dt">float</span> lambda){

    <span class="dt">int</span> col,row,ncols;
    <span class="dt">const</span> PetscInt *cols;
    <span class="dt">const</span> PetscScalar *vals;
    <span class="dt">float</span> cj = <span class="dv">0</span>, aj = <span class="dv">0</span>,rsum = <span class="dv">0</span>;

    <span class="kw">for</span> (row=<span class="dv">0</span>; row&lt;rend; row++){
        MatGetRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals);
        <span class="kw">for</span> (col=<span class="dv">0</span>; col&lt;cend; col++){
            <span class="kw">if</span> (col != idx){
                rsum += vals[col]*beta_array[col];
            }<span class="kw">else</span>{  
                aj += vals[idx]*vals[idx];
            }
        }
        cj += vals[idx]*(y_array[row] - rsum);
        rsum = <span class="dv">0</span>;
    MatRestoreRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals);
    }

    <span class="dt">float</span> Bj = <span class="fl">0.0</span>;
    <span class="kw">if</span> (cj &lt; -lambda*rend) Bj = (cj + lambda*rend)/aj;
    <span class="kw">if</span> (cj &gt;  lambda*rend) Bj = (cj - lambda*rend)/aj;

    <span class="kw">return</span> Bj;
}

<span class="dt">float</span> objective(PetscScalar *y_array, <span class="dt">float</span> *beta_array, <span class="dt">int</span> rend, <span class="dt">int</span> cend, Mat matrix, <span class="dt">float</span> lambda){

        <span class="dt">int</span> col,row,ncols;
        <span class="dt">const</span> PetscInt *cols;
        <span class="dt">const</span> PetscScalar *vals;
        <span class="dt">float</span> cj = <span class="dv">0</span>, Bj = <span class="dv">0</span>,rsum = <span class="dv">0</span>;

        <span class="kw">for</span> (row=<span class="dv">0</span>; row&lt;rend; row++){
                MatGetRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals);
                <span class="kw">for</span> (col=<span class="dv">0</span>; col&lt;cend; col++){
                                rsum += vals[col]*beta_array[col];
                }
                cj += (y_array[row] - rsum) * (y_array[row] - rsum);
                rsum = <span class="dv">0</span>;
                MatRestoreRow(matrix,row,&amp;ncols,&amp;cols,&amp;vals);
        }

    <span class="kw">for</span> (col=<span class="dv">0</span>;col&lt;cend;col++){
        Bj += PetscAbsReal(beta_array[col]);
    }
        
    <span class="kw">return</span> .<span class="dv">5</span> * (<span class="dv">1</span>/(<span class="dt">float</span>)rend) * PetscSqrtReal(cj) * PetscSqrtReal(cj) + lambda * Bj;
}



<span class="dt">int</span> main(<span class="dt">int</span> argc,<span class="dt">char</span> **argv){
    
    Mat X;
    Vec y;
    PetscInt i,j,k,it,rstart,rend,row,cstart,cend,col,N,save_step=<span class="dv">100</span>,iter=<span class="dv">10000</span>;
    PetscScalar rcounter,ind,tol=<span class="fl">1e-6</span>,L=.<span class="dv">15</span>,obj_last=<span class="fl">1e10</span>; 
    PetscRandom rng;
    PetscViewer viewer;
    PetscScalar *y_array;
    PetscInt rank,size;

    PetscInitialize(&amp;argc,&amp;argv,NULL,NULL);
    MPI_Comm_rank(PETSC_COMM_WORLD,&amp;rank);
    MPI_Comm_size(PETSC_COMM_WORLD,&amp;size);

    <span class="dt">int</span> sendind[size];
    <span class="dt">int</span> getind[size];

    PetscRandomCreate(PETSC_COMM_WORLD, &amp;rng);
<span class="ot">#if defined(PETSC_HAVE_DRAND48)</span>
        PetscRandomSetType(rng, PETSCRAND48);
<span class="ot">#elif defined(PETSC_HAVE_RAND)</span>
        PetscRandomSetType(rng, PETSCRAND);
<span class="ot">#endif</span>
        PetscRandomSetFromOptions(rng);
    
    PetscOptionsGetInt(NULL,<span class="st">&quot;-i&quot;</span>,&amp;iter,NULL);
    PetscOptionsGetInt(NULL,<span class="st">&quot;-s&quot;</span>,&amp;save_step,NULL);
    PetscOptionsGetReal(NULL,<span class="st">&quot;-l&quot;</span>,&amp;L,NULL);

    PetscViewerBinaryOpen(PETSC_COMM_SELF,<span class="st">&quot;/home/sw424/NumComp/lasso/datafinal/lasso_50_10000_5_1/samp_1/X.bin&quot;</span>,FILE_MODE_READ,&amp;viewer);
    <span class="co">//PetscViewerBinaryOpen(PETSC_COMM_SELF,&quot;data/X.bin&quot;,FILE_MODE_READ,&amp;viewer);</span>
    MatCreate(PETSC_COMM_SELF,&amp;X);
    MatSetType(X,MATSEQDENSE);
    MatLoad(X,viewer);
    PetscViewerDestroy(&amp;viewer);

    MatGetOwnershipRange(X,&amp;rstart,&amp;rend);
    MatGetOwnershipRangeColumn(X,&amp;cstart,&amp;cend);
    
    PetscViewerBinaryOpen(PETSC_COMM_SELF,<span class="st">&quot;/home/sw424/NumComp/lasso/datafinal/lasso_50_10000_5_1/samp_1/y.bin&quot;</span>,FILE_MODE_READ,&amp;viewer);
    <span class="co">//PetscViewerBinaryOpen(PETSC_COMM_SELF,&quot;data/y.bin&quot;,FILE_MODE_READ,&amp;viewer);</span>
    VecCreate(PETSC_COMM_SELF,&amp;y);
    VecSetType(y,VECSEQ); <span class="co">// new</span>
    <span class="co">//VecSetSizes(y,PETSC_DECIDE,rend);</span>
    VecLoad(y,viewer);
    <span class="co">//VecView(y,PETSC_VIEWER_STDOUT_WORLD);</span>
    PetscViewerDestroy(&amp;viewer);

    <span class="dt">float</span> B[cend];
    memset(B,<span class="dv">0</span>,<span class="kw">sizeof</span> B);   

    PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;Lambda = %f</span><span class="ch">\n\n</span><span class="st">&quot;</span>,L);
    PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;Iter</span><span class="ch">\t</span><span class="st">Obj</span><span class="ch">\n</span><span class="st">&quot;</span>);

<span class="kw">for</span> (it=<span class="dv">0</span>;it&lt;iter;it++){
    <span class="kw">for</span> (i=<span class="dv">0</span>;i&lt;size;i++){
        PetscRandomSetInterval(rng,<span class="dv">0</span>,cend);
            PetscRandomGetValueReal(rng,&amp;ind);
            sendind[i] = PetscFloorReal(ind);
        <span class="co">//PetscPrintf(PETSC_COMM_WORLD,&quot;Iter %i | Rank %i | Rand %i: %i\n&quot;,it,rank,i,sendind[i]);</span>
    }

    MPI_Scatter(sendind,<span class="dv">1</span>,MPI_INT,
            getind,<span class="dv">1</span>,MPI_INT,
            <span class="dv">0</span>,PETSC_COMM_WORLD);

    
    
    VecGetArray(y,&amp;y_array);
    <span class="dt">float</span> Bj = coord_descent(y_array,B,rend,cend,X,getind[<span class="dv">0</span>],L);
    <span class="dt">double</span> obj = objective(y_array,B,rend,cend,X,L); <span class="co">// maybe better precision</span>
    <span class="co">//float obj = objective(y_array,B,rend,cend,X,L); </span>
    VecRestoreArray(y,&amp;y_array);

    <span class="co">//PetscPrintf(PETSC_COMM_SELF,&quot;Iter %i | Rank: %i | B: %f\n&quot;,it,rank,Bj);</span>
    
    <span class="dt">float</span> *Bnew = (<span class="dt">float</span> *)malloc(<span class="kw">sizeof</span>(<span class="dt">float</span>)*size);
    
    MPI_Gather(&amp;Bj,<span class="dv">1</span>,MPI_FLOAT,Bnew,<span class="dv">1</span>,MPI_FLOAT,<span class="dv">0</span>,PETSC_COMM_WORLD);
    
    <span class="kw">for</span> (i=<span class="dv">0</span>;i&lt;size;i++){
        B[sendind[i]] = Bnew[i]; 
        <span class="co">//PetscPrintf(PETSC_COMM_WORLD,&quot;Iter %i | Rank %i | Rand %i: %i | B: %f\n&quot;,it,rank,i,sendind[i],Bnew[i]);</span>
    }
    

    MPI_Bcast(B,cend,MPI_FLOAT,<span class="dv">0</span>,PETSC_COMM_WORLD);
    

    <span class="kw">if</span>((it<span class="dv">+1</span>) % save_step == <span class="dv">0</span>){
        PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;%i</span><span class="ch">\t</span><span class="st">%f</span><span class="ch">\t</span><span class="st">&quot;</span>,it,obj);
        <span class="co">//for (i=0;i&lt;cend;i++){</span>
        <span class="co">//  PetscPrintf(PETSC_COMM_WORLD,&quot;%f\t&quot;,B[i]);</span>
        <span class="co">//}</span>
        PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>);
    
    <span class="co">//if(obj_last-obj &lt; tol){</span>
    <span class="kw">if</span>(PetscAbsReal(obj_last-obj) &lt; tol){
          <span class="kw">break</span>;
      }
      obj_last = obj;
    }

    
}
    
    PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">Broke after %i iterations</span><span class="ch">\n</span><span class="st">&quot;</span>,it);
    PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">Final Beta Coefficients:</span><span class="ch">\n</span><span class="st">&quot;</span>);
    <span class="kw">for</span> (i=<span class="dv">0</span>;i&lt;cend;i++){
    <span class="kw">if</span> (B[i] != <span class="dv">0</span>){
          PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;%i: %f</span><span class="ch">\n</span><span class="st">&quot;</span>,i,B[i]);
    }
    }
    PetscPrintf(PETSC_COMM_WORLD,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>);
    

    PetscRandomDestroy(&amp;rng);
    MatDestroy(&amp;X);
    
    PetscFinalize();
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre></div>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">11.10</span> References</h2>
<ul>
<li>Parallel Coordinate Descent for L1-Regularized Loss Minimization. Bradley J. K., Kyrola, A., Bickson, D., and Guestrin, C. 2011. Proceedings of the 28th International Conference on Machine Learning.</li>
<li>Machine Learning: A Probabilistic Perspective. Murphy, K. P., 2012. MIT Press, 1st ed.</li>
<li>The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie, T., Tibshirani, R., and Friedman, J. 2003. Springer. 3rd ed.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multcomp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
