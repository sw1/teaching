<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Topic Models | Worked Bioninformatics, Statistics, and Machine Learning Examples</title>
  <meta name="description" content="Code, notes, lectures, and research that should be helpful for learning purposes." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Topic Models | Worked Bioninformatics, Statistics, and Machine Learning Examples" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, notes, lectures, and research that should be helpful for learning purposes." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Topic Models | Worked Bioninformatics, Statistics, and Machine Learning Examples" />
  
  <meta name="twitter:description" content="Code, notes, lectures, and research that should be helpful for learning purposes." />
  

<meta name="author" content="Stephen Woloszynek" />


<meta name="date" content="2020-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lasso.html"/>
<link rel="next" href="ml.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="dynprog.html"><a href="dynprog.html"><i class="fa fa-check"></i><b>2</b> Dynamic Programming</a><ul>
<li class="chapter" data-level="2.1" data-path="dynprog.html"><a href="dynprog.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="dynprog.html"><a href="dynprog.html#rod-cutting"><i class="fa fa-check"></i><b>2.2</b> Rod cutting</a></li>
<li class="chapter" data-level="2.3" data-path="dynprog.html"><a href="dynprog.html#fibonacci-rabbits"><i class="fa fa-check"></i><b>2.3</b> Fibonacci rabbits</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="align.html"><a href="align.html"><i class="fa fa-check"></i><b>3</b> Alignment</a><ul>
<li class="chapter" data-level="3.1" data-path="align.html"><a href="align.html#longest-common-subsequence"><i class="fa fa-check"></i><b>3.1</b> Longest Common Subsequence</a></li>
<li class="chapter" data-level="3.2" data-path="align.html"><a href="align.html#global-alignment"><i class="fa fa-check"></i><b>3.2</b> Global Alignment</a></li>
<li class="chapter" data-level="3.3" data-path="align.html"><a href="align.html#local-alignment"><i class="fa fa-check"></i><b>3.3</b> Local Alignment</a></li>
<li class="chapter" data-level="3.4" data-path="align.html"><a href="align.html#local-alignment-homework"><i class="fa fa-check"></i><b>3.4</b> Local Alignment: Homework</a></li>
<li class="chapter" data-level="3.5" data-path="align.html"><a href="align.html#global-alignment-code-r"><i class="fa fa-check"></i><b>3.5</b> Global Alignment Code (R)</a></li>
<li class="chapter" data-level="3.6" data-path="align.html"><a href="align.html#global-alignment-code-python"><i class="fa fa-check"></i><b>3.6</b> Global Alignment Code (Python)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="alignalg.html"><a href="alignalg.html"><i class="fa fa-check"></i><b>4</b> Alignment Algorithms</a><ul>
<li class="chapter" data-level="4.1" data-path="alignalg.html"><a href="alignalg.html#longest-common-subsequence-1"><i class="fa fa-check"></i><b>4.1</b> Longest Common Subsequence</a></li>
<li class="chapter" data-level="4.2" data-path="alignalg.html"><a href="alignalg.html#global-alignment-r"><i class="fa fa-check"></i><b>4.2</b> Global Alignment (R)</a></li>
<li class="chapter" data-level="4.3" data-path="alignalg.html"><a href="alignalg.html#global-alignment-python"><i class="fa fa-check"></i><b>4.3</b> Global Alignment (Python)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="biocond.html"><a href="biocond.html"><i class="fa fa-check"></i><b>5</b> Bioconductor</a><ul>
<li class="chapter" data-level="5.0.1" data-path="biocond.html"><a href="biocond.html#loading-fasta-files"><i class="fa fa-check"></i><b>5.0.1</b> Loading FASTA Files</a></li>
<li class="chapter" data-level="5.0.2" data-path="biocond.html"><a href="biocond.html#creating-sequence-sets"><i class="fa fa-check"></i><b>5.0.2</b> Creating Sequence Sets</a></li>
<li class="chapter" data-level="5.0.3" data-path="biocond.html"><a href="biocond.html#sample-metadata"><i class="fa fa-check"></i><b>5.0.3</b> Sample Metadata</a></li>
<li class="chapter" data-level="5.1" data-path="biocond.html"><a href="biocond.html#creating-gc-functions"><i class="fa fa-check"></i><b>5.1</b> Creating GC Functions</a></li>
<li class="chapter" data-level="5.2" data-path="biocond.html"><a href="biocond.html#ncbi-esearch"><i class="fa fa-check"></i><b>5.2</b> NCBI ESearch</a></li>
<li class="chapter" data-level="5.3" data-path="biocond.html"><a href="biocond.html#cds"><i class="fa fa-check"></i><b>5.3</b> CDS</a></li>
<li class="chapter" data-level="5.4" data-path="biocond.html"><a href="biocond.html#whole-genomes"><i class="fa fa-check"></i><b>5.4</b> Whole Genomes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sra.html"><a href="sra.html"><i class="fa fa-check"></i><b>6</b> Retrieving Projects</a><ul>
<li class="chapter" data-level="6.0.1" data-path="sra.html"><a href="sra.html#fastq-dump-for-paired-end-reads"><i class="fa fa-check"></i><b>6.0.1</b> Fastq Dump for Paired End Reads</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="phyloseq.html"><a href="phyloseq.html"><i class="fa fa-check"></i><b>7</b> Phyloseq</a></li>
<li class="chapter" data-level="8" data-path="dada.html"><a href="dada.html"><i class="fa fa-check"></i><b>8</b> Dada2</a><ul>
<li class="chapter" data-level="8.1" data-path="dada.html"><a href="dada.html#fastq-prep"><i class="fa fa-check"></i><b>8.1</b> FASTQ Prep</a></li>
<li class="chapter" data-level="8.2" data-path="dada.html"><a href="dada.html#otu-picking"><i class="fa fa-check"></i><b>8.2</b> OTU Picking</a></li>
<li class="chapter" data-level="8.3" data-path="dada.html"><a href="dada.html#running-dada2-on-proteus"><i class="fa fa-check"></i><b>8.3</b> Running Dada2 on Proteus</a><ul>
<li class="chapter" data-level="8.3.1" data-path="dada.html"><a href="dada.html#method-1-using-namegrp-shared-r-library"><i class="fa fa-check"></i><b>8.3.1</b> Method 1: Using nameGrp Shared R Library</a></li>
<li class="chapter" data-level="8.3.2" data-path="dada.html"><a href="dada.html#method-2-creating-a-local-library"><i class="fa fa-check"></i><b>8.3.2</b> Method 2: Creating a Local Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="qiime.html"><a href="qiime.html"><i class="fa fa-check"></i><b>9</b> Qiime</a><ul>
<li class="chapter" data-level="9.1" data-path="qiime.html"><a href="qiime.html#otu-picking-1"><i class="fa fa-check"></i><b>9.1</b> OTU Picking</a></li>
<li class="chapter" data-level="9.2" data-path="qiime.html"><a href="qiime.html#summarizing-our-results"><i class="fa fa-check"></i><b>9.2</b> Summarizing Our Results</a></li>
<li class="chapter" data-level="9.3" data-path="qiime.html"><a href="qiime.html#loading-qiime-results-into-phyloseq"><i class="fa fa-check"></i><b>9.3</b> Loading QIIME Results into Phyloseq</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multcomp.html"><a href="multcomp.html"><i class="fa fa-check"></i><b>10</b> Multiple Comparisons</a><ul>
<li class="chapter" data-level="10.1" data-path="multcomp.html"><a href="multcomp.html#hypothesis-testing-and-power"><i class="fa fa-check"></i><b>10.1</b> Hypothesis Testing and Power</a></li>
<li class="chapter" data-level="10.2" data-path="multcomp.html"><a href="multcomp.html#multiple-comparisons"><i class="fa fa-check"></i><b>10.2</b> Multiple Comparisons</a><ul>
<li class="chapter" data-level="10.2.1" data-path="multcomp.html"><a href="multcomp.html#bonferroni"><i class="fa fa-check"></i><b>10.2.1</b> Bonferroni</a></li>
<li class="chapter" data-level="10.2.2" data-path="multcomp.html"><a href="multcomp.html#false-discovery-rate"><i class="fa fa-check"></i><b>10.2.2</b> False Discovery Rate</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>11</b> Lasso</a><ul>
<li class="chapter" data-level="11.1" data-path="lasso.html"><a href="lasso.html#big-data-and-feature-selection"><i class="fa fa-check"></i><b>11.1</b> Big Data and Feature Selection</a></li>
<li class="chapter" data-level="11.2" data-path="lasso.html"><a href="lasso.html#lasso-regression"><i class="fa fa-check"></i><b>11.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="11.3" data-path="lasso.html"><a href="lasso.html#multivariate-lasso-regression"><i class="fa fa-check"></i><b>11.3</b> Multivariate Lasso Regression</a></li>
<li class="chapter" data-level="11.4" data-path="lasso.html"><a href="lasso.html#parallel-coordinate-descent"><i class="fa fa-check"></i><b>11.4</b> Parallel Coordinate Descent</a></li>
<li class="chapter" data-level="11.5" data-path="lasso.html"><a href="lasso.html#simulations"><i class="fa fa-check"></i><b>11.5</b> Simulations</a><ul>
<li class="chapter" data-level="11.5.1" data-path="lasso.html"><a href="lasso.html#times-1000-matrix-5-target-coefficients"><i class="fa fa-check"></i><b>11.5.1</b> <span class="math inline">\(50 \times 1000\)</span> Matrix | 5 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.2" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-5-target-coefficients"><i class="fa fa-check"></i><b>11.5.2</b> <span class="math inline">\(50 \times 10000\)</span> Matrix | 5 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.3" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-5-target-coefficients-1"><i class="fa fa-check"></i><b>11.5.3</b> <span class="math inline">\(200 \times 10000\)</span> Matrix | 5 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.4" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-15-target-coefficients"><i class="fa fa-check"></i><b>11.5.4</b> <span class="math inline">\(50 \times 10000\)</span> Matrix | 15 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.5" data-path="lasso.html"><a href="lasso.html#times-10000-matrix-30-target-coefficients"><i class="fa fa-check"></i><b>11.5.5</b> <span class="math inline">\(50 \times 10000\)</span> Matrix | 30 Target Coefficients</a></li>
<li class="chapter" data-level="11.5.6" data-path="lasso.html"><a href="lasso.html#scaling-the-coeficients"><i class="fa fa-check"></i><b>11.5.6</b> Scaling the coeficients</a></li>
<li class="chapter" data-level="11.5.7" data-path="lasso.html"><a href="lasso.html#comparison"><i class="fa fa-check"></i><b>11.5.7</b> Comparison</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="lasso.html"><a href="lasso.html#conclusion"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a></li>
<li class="chapter" data-level="11.7" data-path="lasso.html"><a href="lasso.html#code-lasso"><i class="fa fa-check"></i><b>11.7</b> Code: Lasso</a></li>
<li class="chapter" data-level="11.8" data-path="lasso.html"><a href="lasso.html#code-lasso-via-cyclic-gradient-descent"><i class="fa fa-check"></i><b>11.8</b> Code: Lasso via cyclic gradient descent</a></li>
<li class="chapter" data-level="11.9" data-path="lasso.html"><a href="lasso.html#code-parallel-lasso"><i class="fa fa-check"></i><b>11.9</b> Code: Parallel lasso</a></li>
<li class="chapter" data-level="11.10" data-path="lasso.html"><a href="lasso.html#references"><i class="fa fa-check"></i><b>11.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="tm.html"><a href="tm.html"><i class="fa fa-check"></i><b>12</b> Topic Models</a><ul>
<li class="chapter" data-level="12.1" data-path="tm.html"><a href="tm.html#variational-inference"><i class="fa fa-check"></i><b>12.1</b> Variational Inference</a><ul>
<li class="chapter" data-level="12.1.1" data-path="tm.html"><a href="tm.html#evidence-lower-bound-elbo"><i class="fa fa-check"></i><b>12.1.1</b> Evidence Lower Bound (ELBO)</a></li>
<li class="chapter" data-level="12.1.2" data-path="tm.html"><a href="tm.html#elbo-and-kl-divergence"><i class="fa fa-check"></i><b>12.1.2</b> ELBO and KL Divergence</a></li>
<li class="chapter" data-level="12.1.3" data-path="tm.html"><a href="tm.html#mean-field-method"><i class="fa fa-check"></i><b>12.1.3</b> Mean Field Method</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="tm.html"><a href="tm.html#lda"><i class="fa fa-check"></i><b>12.2</b> LDA</a><ul>
<li class="chapter" data-level="12.2.1" data-path="tm.html"><a href="tm.html#expectation-of-pthetaalpha"><i class="fa fa-check"></i><b>12.2.1</b> Expectation of p(<span class="math inline">\(\theta|\alpha)\)</span></a></li>
<li class="chapter" data-level="12.2.2" data-path="tm.html"><a href="tm.html#expectation-of-pztheta"><i class="fa fa-check"></i><b>12.2.2</b> Expectation of p(<span class="math inline">\(z|\theta)\)</span></a></li>
<li class="chapter" data-level="12.2.3" data-path="tm.html"><a href="tm.html#expectation-of-pwzbeta"><i class="fa fa-check"></i><b>12.2.3</b> Expectation of p(<span class="math inline">\(w|z,\beta)\)</span></a></li>
<li class="chapter" data-level="12.2.4" data-path="tm.html"><a href="tm.html#entropy-of-gamma-and-phi"><i class="fa fa-check"></i><b>12.2.4</b> Entropy of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\phi\)</span></a></li>
<li class="chapter" data-level="12.2.5" data-path="tm.html"><a href="tm.html#complete-objective-function"><i class="fa fa-check"></i><b>12.2.5</b> Complete Objective Function</a></li>
<li class="chapter" data-level="12.2.6" data-path="tm.html"><a href="tm.html#parameter-optimization"><i class="fa fa-check"></i><b>12.2.6</b> Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="tm.html"><a href="tm.html#supervised-lda"><i class="fa fa-check"></i><b>12.3</b> Supervised LDA</a><ul>
<li class="chapter" data-level="12.3.1" data-path="tm.html"><a href="tm.html#expectations-of-pthetaalpha-pztheta-and-pwzbeta"><i class="fa fa-check"></i><b>12.3.1</b> Expectations of <span class="math inline">\(p(\theta|\alpha)\)</span>, <span class="math inline">\(p(z|\theta)\)</span>, and <span class="math inline">\(p(w|z,\beta)\)</span></a></li>
<li class="chapter" data-level="12.3.2" data-path="tm.html"><a href="tm.html#expectation-of-pyzetasigma2"><i class="fa fa-check"></i><b>12.3.2</b> Expectation of <span class="math inline">\(p(y|z,\eta,\sigma^2)\)</span></a></li>
<li class="chapter" data-level="12.3.3" data-path="tm.html"><a href="tm.html#entropy-of-gamma-and-phi-1"><i class="fa fa-check"></i><b>12.3.3</b> Entropy of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\phi\)</span></a></li>
<li class="chapter" data-level="12.3.4" data-path="tm.html"><a href="tm.html#complete-objective-function-1"><i class="fa fa-check"></i><b>12.3.4</b> Complete Objective Function</a></li>
<li class="chapter" data-level="12.3.5" data-path="tm.html"><a href="tm.html#parameter-optimization-1"><i class="fa fa-check"></i><b>12.3.5</b> Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="tm.html"><a href="tm.html#the-correlated-topic-model"><i class="fa fa-check"></i><b>12.4</b> The Correlated Topic Model</a><ul>
<li class="chapter" data-level="12.4.1" data-path="tm.html"><a href="tm.html#multinomial-distribution-in-exponential-form"><i class="fa fa-check"></i><b>12.4.1</b> Multinomial Distribution in Exponential Form</a></li>
<li class="chapter" data-level="12.4.2" data-path="tm.html"><a href="tm.html#variational-em"><i class="fa fa-check"></i><b>12.4.2</b> Variational EM</a></li>
<li class="chapter" data-level="12.4.3" data-path="tm.html"><a href="tm.html#expectation-of-pwzbeta-1"><i class="fa fa-check"></i><b>12.4.3</b> Expectation of <span class="math inline">\(p(w|z,\beta)\)</span></a></li>
<li class="chapter" data-level="12.4.4" data-path="tm.html"><a href="tm.html#expectation-of-pzeta"><i class="fa fa-check"></i><b>12.4.4</b> Expectation of <span class="math inline">\(p(z|\eta)\)</span></a></li>
<li class="chapter" data-level="12.4.5" data-path="tm.html"><a href="tm.html#expectation-of-petamusigma"><i class="fa fa-check"></i><b>12.4.5</b> Expectation of <span class="math inline">\(p(\eta|\mu,\sigma)\)</span></a></li>
<li class="chapter" data-level="12.4.6" data-path="tm.html"><a href="tm.html#entropy-of-lambda-nu-and-phi"><i class="fa fa-check"></i><b>12.4.6</b> Entropy of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\nu\)</span>, and <span class="math inline">\(\phi\)</span></a></li>
<li class="chapter" data-level="12.4.7" data-path="tm.html"><a href="tm.html#complete-objective-function-2"><i class="fa fa-check"></i><b>12.4.7</b> Complete Objective Function</a></li>
<li class="chapter" data-level="12.4.8" data-path="tm.html"><a href="tm.html#parameter-optimization-2"><i class="fa fa-check"></i><b>12.4.8</b> Parameter Optimization</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="tm.html"><a href="tm.html#dirichlet-distribution"><i class="fa fa-check"></i><b>12.5</b> Dirichlet Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>13</b> Machine Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="ml.html"><a href="ml.html#cross-valdiation"><i class="fa fa-check"></i><b>13.1</b> Cross Valdiation</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ml.html"><a href="ml.html#loocv"><i class="fa fa-check"></i><b>13.1.1</b> LOOCV</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ml.html"><a href="ml.html#naive-bayes"><i class="fa fa-check"></i><b>13.2</b> Naive Bayes</a></li>
<li class="chapter" data-level="13.3" data-path="ml.html"><a href="ml.html#svm"><i class="fa fa-check"></i><b>13.3</b> SVM</a><ul>
<li class="chapter" data-level="13.3.1" data-path="ml.html"><a href="ml.html#manual-rbf-kernal"><i class="fa fa-check"></i><b>13.3.1</b> Manual rbf kernal</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ml.html"><a href="ml.html#k-means"><i class="fa fa-check"></i><b>13.4</b> K-means</a></li>
<li class="chapter" data-level="13.5" data-path="ml.html"><a href="ml.html#gaussian-mixtures"><i class="fa fa-check"></i><b>13.5</b> Gaussian Mixtures</a></li>
<li class="chapter" data-level="13.6" data-path="ml.html"><a href="ml.html#pca"><i class="fa fa-check"></i><b>13.6</b> PCA</a></li>
<li class="chapter" data-level="13.7" data-path="ml.html"><a href="ml.html#viterbi-algorithm"><i class="fa fa-check"></i><b>13.7</b> Viterbi Algorithm</a></li>
<li class="chapter" data-level="13.8" data-path="ml.html"><a href="ml.html#gradient-descent"><i class="fa fa-check"></i><b>13.8</b> Gradient Descent</a><ul>
<li class="chapter" data-level="13.8.1" data-path="ml.html"><a href="ml.html#linear-regression"><i class="fa fa-check"></i><b>13.8.1</b> Linear Regression</a></li>
<li class="chapter" data-level="13.8.2" data-path="ml.html"><a href="ml.html#logistic-regression"><i class="fa fa-check"></i><b>13.8.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="13.8.3" data-path="ml.html"><a href="ml.html#softmax-regression"><i class="fa fa-check"></i><b>13.8.3</b> Softmax regression</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="ml.html"><a href="ml.html#nonparametric-bayesian-processes"><i class="fa fa-check"></i><b>13.9</b> Nonparametric Bayesian Processes</a><ul>
<li class="chapter" data-level="13.9.1" data-path="ml.html"><a href="ml.html#chinese-restaurant"><i class="fa fa-check"></i><b>13.9.1</b> Chinese Restaurant</a></li>
<li class="chapter" data-level="13.9.2" data-path="ml.html"><a href="ml.html#polyas-urn"><i class="fa fa-check"></i><b>13.9.2</b> Polyas Urn</a></li>
<li class="chapter" data-level="13.9.3" data-path="ml.html"><a href="ml.html#stick-breaking"><i class="fa fa-check"></i><b>13.9.3</b> Stick Breaking</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="ml.html"><a href="ml.html#iteratively-reweighted-least-squares"><i class="fa fa-check"></i><b>13.10</b> Iteratively Reweighted Least Squares</a></li>
<li class="chapter" data-level="13.11" data-path="ml.html"><a href="ml.html#neural-netork"><i class="fa fa-check"></i><b>13.11</b> Neural Netork</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/sw1/" target="blank">Published by Stephen Woloszynek</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Worked Bioninformatics, Statistics, and Machine Learning Examples</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tm" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Topic Models</h1>

<div id="variational-inference" class="section level2">
<h2><span class="header-section-number">12.1</span> Variational Inference</h2>
<div id="evidence-lower-bound-elbo" class="section level3">
<h3><span class="header-section-number">12.1.1</span> Evidence Lower Bound (ELBO)</h3>
<p>We first want to create a variational distribution, which is a distribution over all latent variables parameterized by variational parameters: <span class="math inline">\(q(z_{1:m}|\nu)\)</span>. We want to choose <span class="math inline">\(\nu\)</span> that makes q as cloase as possible to our posterior <span class="math inline">\(p\)</span>. If <span class="math inline">\(q=p\)</span>, then we have typical expectation maximization. The whole put of variational inference, however, is to choose a <span class="math inline">\(q\)</span> that is easier (or <em>possible</em>) to compute.</p>
<p>One thing that we’re going to exploit is Jensen’s Inequality:</p>
<p><img src="figs/tm/jensen.png" /></p>
<p>We can apply this inequality to concave functions. If we have two points (say <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>), and we average the function <span class="math inline">\(f\)</span> that is <em>concave</em> at those two points; it will be less than the function <span class="math inline">\(f\)</span> applied to the average of those two points – that is, <span class="math inline">\(f(\mathbb{E}[X]) \geq \mathbb{E}[f(X)]\)</span>. Let’s assume that <span class="math inline">\(f(v)=\log{v}\)</span>. The weighted average between two points would therefore be <span class="math inline">\(\log{[\alpha v_1 + (1-\alpha)v_2]}\)</span>, and the average of the function at those two points would be <span class="math inline">\(\alpha \log{v_1} + (1-\alpha)\log{v_2}\)</span>. This gives us the following inequality: <span class="math inline">\(\log{[\alpha v_1 + (1-\alpha)v_2]} \ge \alpha \log{v_1} + (1-\alpha)\log{v_2}\)</span>.</p>
<p>We’re going to exploit the use of this inequality to calculate the evidence lower bound (ELBO) for our variational inference procedure. Given the log probability of our data <span class="math inline">\(\log{p(x)}\)</span>, let’s also consider all possible latent variables <span class="math inline">\(z\)</span>. To do this, we’ll marginalize out <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[
\log{p(x)} = \log{\int_z p(x,z)}
\]</span></p>
<p>To introduce our distribution <span class="math inline">\(q\)</span>, we’ll multiply by 1:</p>
<p><span class="math display">\[
\begin{aligned}
\log{p(x)} &amp;= \log{\int_z p(x,z) \frac{q(z)}{q(z)}}\\
            &amp;= \log{\int_z \frac{p(x,z)}{q(z)} q(z)}\\
            &amp;= \log{\mathbb{E}_q [\frac{p(x,z)}{q(z)}]}
\end{aligned}
\]</span></p>
<p>Now we can apply Jensen’s Inequality. We just derived the function applied to the expectation, which was the left term of the inequality. Now, the right term is the expectation of function, giving us:</p>
<p><span class="math display">\[
\begin{aligned}
\log{p(x)} = \log{\mathbb{E}_q [\frac{p(x,z)}{q(z)}]}
&amp;\ge \mathbb{E}_q [\log{\frac{p(x,z)}{q(z)}}]\\
                                        &amp;= \mathbb{E}_q [\log p(x,z)] - \mathbb{E}_q [\log q(z)]
\end{aligned}
\]</span></p>
<p>Turning the log of a quotient into a difference has a useful side effect: <span class="math inline">\(\mathbb{E}_q [\log q(z)]\)</span> is simply the entropy of the variational distribution <span class="math inline">\(q\)</span>. We cannot optimize this part of the equation; however, we <em>can</em> maximize <span class="math inline">\(\mathbb{E}_q [\log p(x,z)]\)</span>. By doing so, we will drive the equation closer and closer to <span class="math inline">\(\log {p(x)}\)</span>, and ideally as far as the entropy term will allow. This term we aim to maximize is the ELBO, and it will give us a tight lower bound of <span class="math inline">\(\log{p(x)}\)</span>.</p>
</div>
<div id="elbo-and-kl-divergence" class="section level3">
<h3><span class="header-section-number">12.1.2</span> ELBO and KL Divergence</h3>
<p>Maximizing the ELBO is equivalent to minimizing the KL divergence. To see this, let’s rewrite out joint probability distribution of our data <span class="math inline">\(x\)</span> and latent parameters <span class="math inline">\(z\)</span> as</p>
<p><span class="math display">\[
p(z|x) = \frac{p(x,x)}{p(x)}\\
\]</span></p>
<p>Now, let’s take the KL divergence between this distribution and the variational distribution <span class="math inline">\(q\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\text{KL}(q(z) || p(z|x)) &amp;= \mathbb{E}_q [\log{\frac{q(z)}{p(z|x)}}]\\
                        &amp;= \mathbb{E}_q [\log{q(z)} - \log{q(z)}]\\
                        &amp;= \mathbb{E}_q [\log{q(z)}] - \mathbb{E}_q [\log{p(z|x)}]\\
                        &amp;= \mathbb{E}_q [\log{q(z)}] - \mathbb{E}_q [\log{\frac{p(x,z)}{p(x)}}]\\
                        &amp;= \mathbb{E}_q [\log{q(z)}] - \mathbb{E}_q [\log{p(x,z)} - \log{p(x)}]\\
                        &amp;= \mathbb{E}_q [\log{q(z)}] - \mathbb{E}_q [\log{p(x,z)}] + \log{p(x)}
\end{aligned}
\]</span></p>
<p>When we optimize, the log probability of our data <span class="math inline">\(\log{p(x)}\)</span> will vanish because it’s a constant, so we can rewrite this as</p>
<p><span class="math display">\[
\begin{aligned}
\text{KL}(q(z) || p(z|x)) &amp;= \mathbb{E}_q [\log{q(z)}] - \mathbb{E}_q [\log{p(x,z)}]\\
                        &amp;= -(\mathbb{E}_q [\log{p(x,z)}] - \mathbb{E}_q [\log{q(z)}])
\end{aligned}
\]</span></p>
<p>Thus, minimizing the KL divergence is the same as maximizing the ELBO.</p>
</div>
<div id="mean-field-method" class="section level3">
<h3><span class="header-section-number">12.1.3</span> Mean Field Method</h3>
<p>One way of writing our variational distribution is via the mean field method where we fully factorize our latent variables such that we assume that they are completely independent of one another. For our variational distribution <span class="math inline">\(q\)</span> over latent parameters <span class="math inline">\(z\)</span> we have</p>
<p><span class="math display">\[
q(z_1,\dots,z_N) = \prod_{i=1}^N q(z_i)
\]</span></p>
<p>It should be obvious that, given this assumption, <span class="math inline">\(p \neq q\)</span> because in <span class="math inline">\(p\)</span>, the latent variables are dependent upon one another. This mean field method is a good starting point for deriving a variational distribution, but sometimes it doesn’t work, so other strategies to forming the variational distribution would be required.</p>
</div>
</div>
<div id="lda" class="section level2">
<h2><span class="header-section-number">12.2</span> LDA</h2>
<p>LDA follows the following generative process:</p>
<p><img src="figs/tm/lda_graphical_model.png" /></p>
<p>The joint distribution over all latent variables <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span> and observed data <span class="math inline">\(w\)</span> is then</p>
<p><span class="math display">\[
p(\theta,z,w|\alpha,\beta) = \prod_d p(\theta_d|\alpha) \prod_n p(w_{d,n}|z_{d,n},\beta) p(z_{d,n}|\theta_d)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;p(\theta_d|\alpha) = \frac{\Gamma (\sum_i \alpha_i)}{\prod_i \Gamma (\alpha_i)} \prod_i \theta_{d,k}^{\alpha_i -1} \text{, (Dirichlet)}\\
&amp;p(z_{d,n}|\theta_d) = \prod_n \prod_i \theta_i^{1[z_n=i]} \text{, (Multinomial)}\\
&amp;p(w_{d,n}|z_{d,n},\beta) = \prod_v \prod_i \beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]} \text{, (Multinomial)}
\end{aligned}
\]</span></p>
<p>Now, our variational distribution for LDA will be a mean field distribution where we assume complete independence between each <span class="math inline">\(\theta\)</span> and between all topic assignments <span class="math inline">\(z\)</span>, i.e., a fully factored form:</p>
<p><span class="math display">\[
q(\theta,z|\gamma,\phi) = \prod_d q(\theta_d|\gamma_d) \prod_n q(z_{d,n}|\phi_{d,n})
\]</span></p>
<p>where <span class="math inline">\(\gamma \thicksim \text{Dirichlet}\)</span> and <span class="math inline">\(\phi \thicksim \text{Multinomial}\)</span> are our variational parameters. <span class="math inline">\(\theta_d\)</span> will be a length <span class="math inline">\(K\)</span> non-negative vector representing the distribution over topics <span class="math inline">\(\gamma_d\)</span> for document <span class="math inline">\(d\)</span>. Note that each vector does <em>not</em> sum to 1. <span class="math inline">\(z_{d,n}\)</span> will also be a length <span class="math inline">\(K\)</span> vector, but instead is a distribution over topic assignments <span class="math inline">\(\phi_{d,n}\)</span> for each token <span class="math inline">\(z_{d,n}\)</span>. Each vector  sum to 1.</p>
<p>Recall that the ELBO is <span class="math inline">\(\mathbb{E}_q [\log p(x,z)] - \mathbb{E}_q [\log q(z)]\)</span>, where <span class="math inline">\(x\)</span> is our observed data and <span class="math inline">\(z\)</span> are our latent variables. In LDA, <span class="math inline">\(w\)</span> is our observed data, and <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span> are our latent variables. Using the joint distribution over latent variables shown above, we have</p>
<p><span class="math display">\[
\begin{aligned}
\log p&amp;(w|\alpha,\beta) \geq L(\gamma,\phi|\alpha,\beta) \\
    &amp;= \mathbb{E}_q [\log  p(\theta,z,w|\alpha,\beta)] - \mathbb{E}_q [\log q(\theta,z)] \\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)p(z|\theta)p(w|z,\beta)] - \mathbb{E}_q [\log q(\theta)q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha) + \log p(z|\theta) + \log p(w|z,\beta)] - \mathbb{E}_q [\log q(\theta) + \log q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)] + \mathbb{E}_q [\log p(z|\theta)] + \mathbb{E}_q [\log p(w|z,\beta)] - \mathbb{E}_q [\log q(\theta)] - \mathbb{E}_q[\log q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)] + \mathbb{E}_q [\log p(z|\theta)] + \mathbb{E}_q [\log p(w|z,\beta)] + \mathbb{H}_q [\gamma] + \mathbb{H}_q[\phi]
\end{aligned} 
\]</span></p>
<p>Before we continue, note that <span class="math inline">\(\mathbb{E}_q\)</span> means the expectation with respect to all parameters in our variational distribution <span class="math inline">\(q\)</span>, so <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span>.</p>
<div id="expectation-of-pthetaalpha" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Expectation of p(<span class="math inline">\(\theta|\alpha)\)</span></h3>
<p>To calculate <span class="math inline">\(\mathbb{E}[\log p(\theta | \alpha)]\)</span>, where <span class="math inline">\(\theta\)</span> is our variational parameter, let’s first recall that it is a Dirichlet distribution; therefore, we have</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(\theta | \alpha)] &amp;= \mathbb{E}_q \log{[\frac{\Gamma (\sum_i \alpha_i)}{\prod_i \Gamma (\alpha_i)} \prod_i \theta_{d,k}^{\alpha_i -1}]}\\
        &amp;=  \mathbb{E}_q \log \Gamma (\sum_i^k \alpha_i) - \mathbb{E}_q \sum_i^k \log \Gamma (\alpha_i) + \mathbb{E}_q \sum_i^k (\alpha_i -1) \log \theta \\
        &amp;= \log \Gamma (\sum_i^k \alpha_i) - \sum_i^k \log \Gamma (\alpha_i) + \sum_i^k (\alpha_i -1) \mathbb{E}_q \log \theta \\
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbb{E}_q [f(\alpha)] = f(\alpha)\)</span> because <span class="math inline">\(\alpha\)</span> is not one of the latent parameters in our variational distribution <span class="math inline">\(q\)</span>. Now we have to calculate <span class="math inline">\(\mathbb{E}_q \log \theta\)</span> where <span class="math inline">\(\theta \thicksim \text{Dirichlet}\)</span>. We must first convert this Dirichlet to its exponential form (given the fact that it belongs to the exponential family). First, we exponentiate the log of <span class="math inline">\(p(\theta | \alpha)\)</span>:</p>
<p><span class="math display">\[
\exp{[\log \Gamma (\sum_i^k \alpha_i) - \sum_i^k \log \Gamma (\alpha_i) + (\sum_i^k (\alpha_i -1) \log \theta)]}
\]</span></p>
<p>A pdf belonging to the exponential family has the form</p>
<p><span class="math display">\[
p(x|\theta) = h(x) \exp{[\theta^T \phi (x) - A(\theta)]}
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> are the natural or canonical parameters, <span class="math inline">\(\phi (x)\)</span> is a vector of sufficient statistics, <span class="math inline">\(h (x)\)</span> is a scaling constant that is often equal to 1, and <span class="math inline">\(A (\theta)\)</span> is the log partition function. If we make one slight adjustment to the Dirichlet in exponential family form, we can these components more easily:</p>
<p><span class="math display">\[
\exp{[(\sum_i^k (\alpha_i -1) \log \theta) - (\sum_i^k \log \Gamma (\alpha_i) + \log \Gamma (\sum_i^k \alpha_i))]}
\]</span></p>
<p>where <span class="math inline">\(\sum_i^k (\alpha_i -1)\)</span> are the natural parameters, <span class="math inline">\(\log \theta\)</span> is the vector of sufficient statistics, and <span class="math inline">\(\sum_i^k \log \Gamma (\alpha_i) + \log \Gamma (\sum_i^k \alpha_i)\)</span> is the log partition function. Taking the derivative of the log partition function with respect to the natural parameters results in the expectation of the sufficient statistic. Recall that we are working with <span class="math inline">\(q(\theta_d|\gamma_d)\)</span> there, our variational distribution, and not <span class="math inline">\(p(\theta_d|\alpha)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q[\log \theta] &amp;= \frac{\partial}{\partial \gamma_i} (\log \Gamma (\gamma_i) + \log \Gamma (\sum_j^k \gamma_j)) \\
                                    &amp;= \Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)
\end{aligned}
\]</span></p>
<p>And so we have</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(\theta | \alpha)] &amp;= \log \Gamma (\sum_i^k \alpha_i) - \sum_i^k \log \Gamma (\alpha_i) + \sum_i^k (\alpha_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
\end{aligned}
\]</span></p>
</div>
<div id="expectation-of-pztheta" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Expectation of p(<span class="math inline">\(z|\theta)\)</span></h3>
<p>For this expectation, we do the following:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z|\theta)] &amp;= \mathbb{E}_q \log [\prod_n \prod_i \theta_i^{1[z_n=i]}]\\
                    &amp;= \mathbb{E}_q [\sum_n \sum_i \log \theta_i^{1[z_n=i]}]\\
                    &amp;= \sum_n \sum_i \mathbb{E}_q [\log \theta_i^{1[z_n=i]}]\\
                    &amp;= \sum_n \sum_i \mathbb{E}_q [1[z_n=i] \log \theta_i]
\end{aligned}
\]</span></p>
<p>Recall that <span class="math inline">\(1[z_n=i]\)</span> is an indicator of whether a particular word’s topic assignment <span class="math inline">\(z_n\)</span> is equal to topic <span class="math inline">\(i\)</span>. If so, then we add the probability <span class="math inline">\(\theta_{d,i}\)</span> of this topic in document <span class="math inline">\(d\)</span> to the summation. Because in our variational distribution <span class="math inline">\(q\)</span> we assume that <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span> are independent, we can do the following:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z|\theta)] &amp;= \sum_n \sum_i \mathbb{E}_q [1[z_n=i] \log \theta_i]\\
                    &amp;= \sum_n \sum_i \mathbb{E}_q 1[z_n=i] \mathbb{E}_q \log \theta_i\\
\end{aligned}
\]</span></p>
<p>The expectation of this indicator function is just a measure of how much a particular token <span class="math inline">\(n\)</span> takes on a topic assignment <span class="math inline">\(i\)</span>, which we’ll call <span class="math inline">\(\phi_{n,i}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z|\theta)] &amp;= \sum_n \sum_i \phi_{n,i} \mathbb{E}_q \log \theta_i\\
                    &amp;= \sum_n \sum_i \phi_{n,i} (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j))
\end{aligned}
\]</span></p>
</div>
<div id="expectation-of-pwzbeta" class="section level3">
<h3><span class="header-section-number">12.2.3</span> Expectation of p(<span class="math inline">\(w|z,\beta)\)</span></h3>
<p>Here we are simply looking up the probability <span class="math inline">\(\beta\)</span> of a topic assignment <span class="math inline">\(z_{d,n}\)</span> for the word <span class="math inline">\(w_{d,n}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(w|z,\beta)] &amp;= \mathbb{E}_q [\log \prod_v \prod_i \beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]}] \\
                                &amp;= \mathbb{E}_q [\sum_v \sum_i \log \beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]}] \\
                                &amp;= \sum_v \sum_i \mathbb{E}_q [\log \beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]}] \\
                                &amp;= \sum_v \sum_i \mathbb{E}_q [1[w_{d,n}=v,z_{d,n}=i] \log \beta_{i,v}] \\
                                &amp;= \sum_v \sum_i \mathbb{E}_q [1[w_{d,n}=v,z_{d,n}=i]] \log \beta_{i,v} \\
                                &amp;= \sum_v \sum_i \mathbb{E}_q [1[w_{d,n}=v]1[z_{d,n}=i]] \log \beta_{i,v} \\
                                &amp;= \sum_v \sum_i 1[w_{d,n}=v] \mathbb{E}_q [1[z_{d,n}=i]] \log \beta_{i,v} \\
\end{aligned}
\]</span></p>
<p>Recall that we already defined <span class="math inline">\(\mathbb{E}_q 1[z_n=i] = \phi_{n,i}\)</span>, leaving us with</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(w|z,\beta)] &amp;= \sum_v \sum_i 1[w_{d,n}=v] \phi_{n,i} \log \beta_{i,v}
\end{aligned}
\]</span></p>
</div>
<div id="entropy-of-gamma-and-phi" class="section level3">
<h3><span class="header-section-number">12.2.4</span> Entropy of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\phi\)</span></h3>
<p>We can simply look up the entropy of a Dirichlet for <span class="math inline">\(\gamma\)</span> and the entropy of a multinomial for <span class="math inline">\(\phi_{d,n}\)</span>, giving us</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{H}_q [\gamma] &amp;= -\log \Gamma (\sum_j \gamma_j) + \sum_i \log \Gamma(\gamma_i) - \sum_i (\gamma_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j))\\
\mathbb{H}_q [\phi_{d,n}] &amp;= -\sum_i \phi_{d,n,i} \log \phi_{d,n,i}
\end{aligned}
\]</span></p>
</div>
<div id="complete-objective-function" class="section level3">
<h3><span class="header-section-number">12.2.5</span> Complete Objective Function</h3>
<p>Now that we calculated the expectations and entropies we need, we can fill in our original equation:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q &amp;[\log  p(\theta,z,w|\alpha,\beta)] - \mathbb{E}_q [\log q(\theta,z)] \\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)] + \mathbb{E}_q [\log p(z|\theta)] + \mathbb{E}_q [\log p(w|z,\beta)] + \mathbb{H}_q [\gamma] + \mathbb{H}_q[\phi_{d,n}] \\
\end{aligned} 
\]</span></p>
<p>Giving us</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}_q &amp;[\log  p(\theta,z,w|\alpha,\beta)] - \mathbb{E}_q [\log q(\theta,z)] = \\
                &amp;\log \Gamma (\sum_i \alpha_j) - \sum_i \log \Gamma (\alpha_i) + \sum_i^k (\alpha_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
                &amp;+ \sum_n \sum_i \phi_{n,i} (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
                &amp;+ \sum_n \sum_i \sum_v 1[w_n=v] \phi_{n,i} \log \beta_{i,v} \\
                &amp;- \log \Gamma (\sum_j \gamma_j) + \sum_i \log \Gamma(\gamma_i) - \sum_i (\gamma_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
                &amp;- \sum_n \sum_i \phi_{n,i} \log \phi_{n,i}
\end{aligned} 
\]</span></p>
</div>
<div id="parameter-optimization" class="section level3">
<h3><span class="header-section-number">12.2.6</span> Parameter Optimization</h3>
<p>To derive our parameter updates for variational EM, we need to maximize the objective function above with respect to our target parameter.</p>
<div id="optimization-for-phi" class="section level4">
<h4><span class="header-section-number">12.2.6.1</span> Optimization for <span class="math inline">\(\phi\)</span></h4>
<p>Using the objective function above, we’ll isolate every term that is a function of <span class="math inline">\(phi\)</span>, and using the constraint <span class="math inline">\(\sum_i \phi_{n,i}=1\)</span>, we’ll perform the following optimization:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \phi_{n,i}} &amp;= \frac{\partial}{\partial \phi_{n,i}}[\phi_{n,i} (\Psi(\gamma_i) - \Psi(\sum_j \gamma_j)) + \sum_j 1[w_n = v] \phi_{n,i} \log \beta_{i,v} \\ 
&amp;\qquad- \phi_{n,i} \log \phi_{n,i} + \lambda_n (\phi_{n,i} - 1)]\\
    &amp;=\Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - \log \phi_{n,i} - 1 + \lambda
\end{aligned}
\]</span></p>
<p>Then, setting to this to zero, we get</p>
<p><span class="math display">\[
\begin{aligned}
  0 &amp;=\Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - \log \phi_{n,i} - 1 + \lambda\\
  \log \phi_{n,i} &amp;= \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - 1 + \lambda \\
  \phi_{n,i} &amp;= \exp [\Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - 1 + \lambda]\\
      &amp;= \beta_{i,v}^{1[w_n = v]} \exp [\Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j)] \exp [- 1 + \lambda]\\
      &amp;= c_{n,i} \beta_{i,v}^{1[w_n = v]} \exp [\Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j)]
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(c_{n,i}=\exp [\lambda - 1]\)</span>. Because we don’t know <span class="math inline">\(\lambda\)</span>, we will simply calculate the unnormalized <span class="math inline">\(\phi_{n,i}\)</span> and then normalize to enforce the constraint that <span class="math inline">\(\sum_i \phi_{n,i}=1\)</span>. Therefore, we have</p>
<p><span class="math display">\[
\begin{aligned}
\phi_{n,i} &amp;\propto \beta_{i,v}^{1[w_n = v]} \exp [\Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j)]
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-gamma_i" class="section level4">
<h4><span class="header-section-number">12.2.6.2</span> Optimization for <span class="math inline">\(\gamma_i\)</span></h4>
<p>Now we’ll take all terms that are a function of <span class="math inline">\(\gamma_i\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \gamma_i} &amp;= \frac{\partial}{\partial \gamma_i}[(\alpha_i -1) (\Psi (\gamma_i) - \Psi (\sum_j \gamma_j)) \\
                &amp;\qquad + \sum_n \phi_{n,i} (\Psi (\gamma_i) - \Psi (\sum_j \gamma_j)) \\
                &amp;\qquad - \log \Gamma (\sum_j \gamma_j) + \log \Gamma(\gamma_i) \\
                &amp;\qquad - (\gamma_i -1) (\Psi (\gamma_i) - \Psi (\sum_j \gamma_j))] \\
                &amp;= \frac{\partial}{\partial \gamma_i} [\alpha_i \Psi (\gamma_i) - \alpha_i \Psi (\sum_j \gamma_j) - \Psi (\gamma_i) + \Psi (\sum_j \gamma_j)\\
                &amp;\qquad + \sum_n \phi_{n,i} \Psi (\gamma_i) - \sum_n \phi_{n,i} \Psi (\sum_j \gamma_j)\\
                &amp;\qquad - \log \Gamma (\sum_j \gamma_j) + \log \Gamma (\gamma_i) \\
                &amp;\qquad - \gamma_i \Psi (\gamma_i) + \gamma_i \Psi (\sum_j \gamma_j) + \Psi (\gamma_i) - \Psi(\sum_j \gamma_j)]\\
                &amp;= \alpha_i \Psi&#39; (\gamma_i) - \alpha_i \Psi&#39; (\sum_j \gamma_j) - \Psi&#39; (\gamma_i) + \Psi&#39; (\sum_j \gamma_j)\\
                &amp;\qquad + \sum_n \phi_{n,i} \Psi&#39; (\gamma_i) - \sum_n \phi_{n,i} \Psi&#39; (\sum_j \gamma_j)\\
                &amp;\qquad - \Psi (\sum_j \gamma_j) + \Psi (\gamma_i) \\
                &amp;\qquad - \gamma_i \Psi&#39; (\gamma_i) - \Psi (\gamma_i) + \gamma_i \Psi&#39; (\sum_j \gamma_j) + \Psi (\sum_j \gamma_j) + \Psi&#39; (\gamma_i) - \Psi&#39; (\sum_j \gamma_j)\\
                &amp;= \alpha_i \Psi&#39; (\gamma_i) - \alpha_i \Psi&#39; (\sum_j \gamma_j) - \Psi&#39; (\gamma_i) + \Psi&#39; (\sum_j \gamma_j)\\
                &amp;\qquad + \sum_n \phi_{n,i} \Psi&#39; (\gamma_i) - \sum_n \phi_{n,i} \Psi&#39; (\sum_j \gamma_j)\\
                &amp;\qquad - \gamma_i \Psi&#39; (\gamma_i) + \gamma_i \Psi&#39; (\sum_j \gamma_j) + \Psi&#39; (\gamma_i) - \Psi&#39; (\sum_j \gamma_j)
\end{aligned}
\]</span></p>
<p>Now combine terms:</p>
<p><span class="math display">\[
\begin{aligned}
\Psi&#39; (\gamma_i)(\alpha_i - 1 + \sum_n \phi_{n,i} - \gamma_i + 1) &amp;= \Psi&#39; (\sum_j \gamma_j)(\alpha_i + \sum_n \phi_{n,i} - \gamma_j)\\
\Psi&#39; (\gamma_i)(\alpha_i + \sum_n \phi_{n,i} - \gamma_i) &amp;= \Psi&#39; (\sum_j \gamma_j)(\alpha_i + \sum_n \phi_{n,i} - \gamma_j)\\
\end{aligned}
\]</span></p>
<p>Then set this to zero:</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= \Psi&#39; (\gamma_i)(\alpha_i + \sum_n \phi_{n,i} - \gamma_i) - \Psi&#39; (\sum_j \gamma_j)(\alpha_i + \sum_n \phi_{n,i} - \gamma_j)\\
\end{aligned}
\]</span></p>
<p>Leaving us with</p>
<p><span class="math display">\[
\begin{aligned}
\gamma_i &amp;= \alpha_i + \sum_n \phi_{n,i}
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-beta" class="section level4">
<h4><span class="header-section-number">12.2.6.3</span> Optimization for <span class="math inline">\(\beta\)</span></h4>
<p>Now, we’ll extract all terms that are a function of <span class="math inline">\(\beta\)</span> and add the constrain <span class="math inline">\(\sum_v \beta_{i,v} = 1\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \beta} &amp;= \frac{\partial}{\partial \beta}[\sum_n \sum_i \sum_v 1[w_n=v]\phi_{n,i} \log \beta_{i,v} + \sum_i \lambda_i (\sum_v \beta_{i,v} - 1)]\\
                &amp;=\sum_n \sum_i \sum_v \frac{1[w_n=v]\phi_{n,i}}{\beta_{i,v}} + \sum_i \lambda_i\\
\end{aligned}
\]</span></p>
<p>Setting this to 0 results in</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;=\sum_n \sum_i \sum_v \frac{1[w_n=v]\phi_{n,i}}{\beta_{i,v}} + \sum_i \lambda_i\\
- \beta_{i,v} \sum_i \lambda_i &amp;=\sum_n \sum_i \sum_v 1[w_n=v]\phi_{n,i}\\
\beta_{i,v} &amp;= c_{i,v} \sum_n \sum_i \sum_v 1[w_n=v]\phi_{n,i}^{(k)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(c_{i,v} = -\frac{1}{\sum_i \lambda_i}\)</span>. Like what we did for <span class="math inline">\(\phi\)</span>, we will calculate unnormalized <span class="math inline">\(\beta\)</span> and then normalize to satisfy the constraint. Our final update is the following:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_{i,v} &amp;\propto \sum_n \sum_i \sum_v 1[w_n=v]\phi_{n,i}
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3">vocab &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;river&quot;</span>,<span class="st">&quot;stream&quot;</span>,<span class="st">&quot;bank&quot;</span>,<span class="st">&quot;money&quot;</span>,<span class="st">&quot;loan&quot;</span>)</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">topic1 &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">333</span>,.<span class="dv">333</span>,.<span class="dv">333</span>,<span class="dv">0</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">topic2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">333</span>,.<span class="dv">333</span>,.<span class="dv">333</span>)</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">topics &lt;-<span class="st"> </span>topic1 <span class="op">+</span><span class="st"> </span>topic2</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">K &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">V &lt;-<span class="st"> </span><span class="kw">length</span>(vocab)</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">N &lt;-<span class="st"> </span><span class="dv">16</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10">corpus &lt;-<span class="st">   </span><span class="kw">rbind</span>(<span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="kw">sample</span>(vocab,N,topic1,<span class="dt">replace=</span>T),<span class="dv">7</span>),<span class="dv">7</span>,N),</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">                 <span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="kw">sample</span>(vocab,N,topics,<span class="dt">replace=</span>T),<span class="dv">10</span>),<span class="dv">10</span>,N),</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">                 <span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="kw">sample</span>(vocab,N,topic2,<span class="dt">replace=</span>T),<span class="dv">5</span>),<span class="dv">5</span>,N))</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">M &lt;-<span class="st"> </span><span class="kw">nrow</span>(corpus)</a>
<a class="sourceLine" id="cb1-14" data-line-number="14"></a>
<a class="sourceLine" id="cb1-15" data-line-number="15">start &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(corpus, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="kw">c</span>(<span class="kw">sum</span>(x<span class="op">==</span>vocab[<span class="dv">1</span>]),<span class="kw">sum</span>(x<span class="op">==</span>vocab[<span class="dv">2</span>]),<span class="kw">sum</span>(x<span class="op">==</span>vocab[<span class="dv">3</span>]),<span class="kw">sum</span>(x<span class="op">==</span>vocab[<span class="dv">4</span>]),<span class="kw">sum</span>(x<span class="op">==</span>vocab[<span class="dv">5</span>]))))</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">docnames &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;D&quot;</span>,<span class="dv">1</span><span class="op">:</span>M,<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="kw">rownames</span>(start) &lt;-<span class="st"> </span>docnames</a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="kw">colnames</span>(start) &lt;-<span class="st"> </span>vocab</a>
<a class="sourceLine" id="cb1-19" data-line-number="19"></a>
<a class="sourceLine" id="cb1-20" data-line-number="20">topicassign &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),M<span class="op">*</span>N,<span class="dt">replace=</span>T),M,N,<span class="dt">byrow=</span>T)</a>
<a class="sourceLine" id="cb1-21" data-line-number="21"></a>
<a class="sourceLine" id="cb1-22" data-line-number="22">df &lt;-<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1-23" data-line-number="23"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">  df &lt;-<span class="st"> </span><span class="kw">rbind</span>(df,<span class="kw">cbind</span>(corpus[i,],<span class="kw">rep</span>(docnames[i]),topicassign[i,]))</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">}</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(df)</a>
<a class="sourceLine" id="cb1-27" data-line-number="27"><span class="kw">names</span>(df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;word&#39;</span>,<span class="st">&#39;doc&#39;</span>,<span class="st">&#39;topic&#39;</span>)</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">df_original &lt;-<span class="st"> </span>df</a>
<a class="sourceLine" id="cb1-29" data-line-number="29"></a>
<a class="sourceLine" id="cb1-30" data-line-number="30">nu &lt;-<span class="st"> </span><span class="fl">.25</span></a>
<a class="sourceLine" id="cb1-31" data-line-number="31">alpha &lt;-<span class="st"> </span><span class="fl">.5</span></a>
<a class="sourceLine" id="cb1-32" data-line-number="32"></a>
<a class="sourceLine" id="cb1-33" data-line-number="33"><span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>){</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">  </a>
<a class="sourceLine" id="cb1-35" data-line-number="35">  <span class="kw">cat</span>(<span class="st">&quot;i=&quot;</span>,iter,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>,<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">  <span class="kw">print</span>(<span class="kw">xtabs</span>(<span class="op">~</span><span class="st"> </span>word<span class="op">+</span>topic,df))</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">  <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>,<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">  </a>
<a class="sourceLine" id="cb1-39" data-line-number="39">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(df)){</a>
<a class="sourceLine" id="cb1-40" data-line-number="40">    </a>
<a class="sourceLine" id="cb1-41" data-line-number="41">    w &lt;-<span class="st"> </span><span class="kw">as.vector</span>(df[i,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb1-42" data-line-number="42">    d &lt;-<span class="st"> </span><span class="kw">as.vector</span>(df[i,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">    </a>
<a class="sourceLine" id="cb1-44" data-line-number="44">    civk &lt;-<span class="st"> </span>df[<span class="op">-</span>i,] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-45" data-line-number="45"><span class="st">      </span><span class="kw">group_by</span>(word,doc,topic) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-46" data-line-number="46"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">civk=</span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb1-47" data-line-number="47">    cik &lt;-<span class="st"> </span>df[<span class="op">-</span>i,] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-48" data-line-number="48"><span class="st">      </span><span class="kw">group_by</span>(doc,topic) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-49" data-line-number="49"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">cik=</span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb1-50" data-line-number="50">    cvk &lt;-<span class="st"> </span>df[<span class="op">-</span>i,] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-51" data-line-number="51"><span class="st">      </span><span class="kw">group_by</span>(word,topic) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-52" data-line-number="52"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">cvk=</span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb1-53" data-line-number="53">    niv &lt;-<span class="st"> </span>df[<span class="op">-</span>i,] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-54" data-line-number="54"><span class="st">      </span><span class="kw">group_by</span>(word,doc) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-55" data-line-number="55"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">niv=</span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb1-56" data-line-number="56">    ck &lt;-<span class="st"> </span>df[<span class="op">-</span>i,] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-57" data-line-number="57"><span class="st">      </span><span class="kw">group_by</span>(topic) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-58" data-line-number="58"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">words=</span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb1-59" data-line-number="59">    Li &lt;-<span class="st"> </span>df[<span class="op">-</span>i,] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-60" data-line-number="60"><span class="st">      </span><span class="kw">group_by</span>(doc) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-61" data-line-number="61"><span class="st">      </span><span class="kw">summarise</span>(<span class="dt">words=</span><span class="kw">n</span>())</a>
<a class="sourceLine" id="cb1-62" data-line-number="62">    </a>
<a class="sourceLine" id="cb1-63" data-line-number="63">    a &lt;-<span class="st"> </span><span class="kw">unlist</span>((cvk[cvk<span class="op">$</span>word<span class="op">==</span>w,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>nu))<span class="op">/</span><span class="kw">unlist</span>((ck[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>V<span class="op">*</span>nu))</a>
<a class="sourceLine" id="cb1-64" data-line-number="64">    b &lt;-<span class="st"> </span><span class="kw">unlist</span>((cik[cik<span class="op">$</span>doc<span class="op">==</span>d,<span class="dv">3</span>] <span class="op">+</span><span class="st"> </span>alpha))<span class="op">/</span><span class="kw">unlist</span>((Li[Li<span class="op">$</span>doc<span class="op">==</span>d,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>K<span class="op">*</span>alpha))</a>
<a class="sourceLine" id="cb1-65" data-line-number="65">    pqilk &lt;-<span class="st"> </span><span class="kw">unlist</span>((a<span class="op">*</span>b)<span class="op">/</span><span class="kw">sum</span>(a<span class="op">*</span>b))</a>
<a class="sourceLine" id="cb1-66" data-line-number="66">    df[i,<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">prob=</span>pqilk)</a>
<a class="sourceLine" id="cb1-67" data-line-number="67">  }</a>
<a class="sourceLine" id="cb1-68" data-line-number="68">}</a>
<a class="sourceLine" id="cb1-69" data-line-number="69"></a>
<a class="sourceLine" id="cb1-70" data-line-number="70">est_alpha &lt;-<span class="st"> </span><span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb1-71" data-line-number="71">alpha &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">50</span><span class="op">/</span>K,K)</a>
<a class="sourceLine" id="cb1-72" data-line-number="72">beta &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(K,<span class="kw">rep</span>(<span class="dv">1</span>,V))</a>
<a class="sourceLine" id="cb1-73" data-line-number="73"><span class="cf">for</span> (m_step <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>){</a>
<a class="sourceLine" id="cb1-74" data-line-number="74">  </a>
<a class="sourceLine" id="cb1-75" data-line-number="75">  gamma &lt;-<span class="st"> </span><span class="kw">matrix</span>(alpha <span class="op">+</span><span class="st"> </span>N<span class="op">/</span>K,M,K,<span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1-76" data-line-number="76">  phi &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span><span class="op">/</span>K,<span class="kw">c</span>(M,N,K))</a>
<a class="sourceLine" id="cb1-77" data-line-number="77">  </a>
<a class="sourceLine" id="cb1-78" data-line-number="78">  <span class="co">## E step</span></a>
<a class="sourceLine" id="cb1-79" data-line-number="79">  <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){</a>
<a class="sourceLine" id="cb1-80" data-line-number="80">    conv &lt;-<span class="st"> </span><span class="ot">Inf</span></a>
<a class="sourceLine" id="cb1-81" data-line-number="81">    tol &lt;-<span class="st"> </span><span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb1-82" data-line-number="82">    <span class="cf">while</span> (tol <span class="op">&lt;</span><span class="st"> </span>conv){</a>
<a class="sourceLine" id="cb1-83" data-line-number="83">      phi0 &lt;-<span class="st"> </span>phi</a>
<a class="sourceLine" id="cb1-84" data-line-number="84">      gamma0 &lt;-<span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb1-85" data-line-number="85">      <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){</a>
<a class="sourceLine" id="cb1-86" data-line-number="86">        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K){</a>
<a class="sourceLine" id="cb1-87" data-line-number="87">          phi[d,n,i] &lt;-<span class="st"> </span>beta[i,corpus[d,n]<span class="op">==</span>vocab] <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="kw">digamma</span>(gamma[d,i]) <span class="op">-</span><span class="st"> </span><span class="kw">digamma</span>(<span class="kw">sum</span>(gamma[d,])))</a>
<a class="sourceLine" id="cb1-88" data-line-number="88">        }</a>
<a class="sourceLine" id="cb1-89" data-line-number="89">        phi[d,n,] &lt;-<span class="st"> </span>phi[d,n,]<span class="op">/</span><span class="kw">sum</span>(phi[d,n,])</a>
<a class="sourceLine" id="cb1-90" data-line-number="90">      }</a>
<a class="sourceLine" id="cb1-91" data-line-number="91">      gamma[d,] &lt;-<span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span><span class="kw">colSums</span>(phi[d,,])</a>
<a class="sourceLine" id="cb1-92" data-line-number="92">      conv &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">c</span>(<span class="kw">max</span>(<span class="kw">abs</span>(phi<span class="op">-</span>phi0)),<span class="kw">max</span>(<span class="kw">abs</span>(gamma[d,]<span class="op">-</span>gamma0[d,]))))</a>
<a class="sourceLine" id="cb1-93" data-line-number="93">    }</a>
<a class="sourceLine" id="cb1-94" data-line-number="94">  }</a>
<a class="sourceLine" id="cb1-95" data-line-number="95">  </a>
<a class="sourceLine" id="cb1-96" data-line-number="96">  <span class="co">## M step</span></a>
<a class="sourceLine" id="cb1-97" data-line-number="97">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K){</a>
<a class="sourceLine" id="cb1-98" data-line-number="98">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>V){</a>
<a class="sourceLine" id="cb1-99" data-line-number="99">      w_dnj &lt;-<span class="st"> </span>corpus <span class="op">==</span><span class="st"> </span>vocab[j]</a>
<a class="sourceLine" id="cb1-100" data-line-number="100">      beta[i,j] &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">any</span>(w_dnj),<span class="kw">sum</span>(phi[,,i][w_dnj]),<span class="fl">1e-20</span>)</a>
<a class="sourceLine" id="cb1-101" data-line-number="101">    }</a>
<a class="sourceLine" id="cb1-102" data-line-number="102">    beta[i,] &lt;-<span class="st"> </span>beta[i,]<span class="op">/</span><span class="kw">sum</span>(beta[i,])</a>
<a class="sourceLine" id="cb1-103" data-line-number="103">  }</a>
<a class="sourceLine" id="cb1-104" data-line-number="104">  </a>
<a class="sourceLine" id="cb1-105" data-line-number="105">  <span class="co">## alpha</span></a>
<a class="sourceLine" id="cb1-106" data-line-number="106">  <span class="cf">if</span> (est_alpha <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>){</a>
<a class="sourceLine" id="cb1-107" data-line-number="107">    conv &lt;-<span class="st"> </span><span class="ot">Inf</span></a>
<a class="sourceLine" id="cb1-108" data-line-number="108">    tol &lt;-<span class="st"> </span><span class="fl">1e-3</span></a>
<a class="sourceLine" id="cb1-109" data-line-number="109">    iter &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-110" data-line-number="110">    alpha_init &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1-111" data-line-number="111">    alpha &lt;-<span class="st"> </span>alpha_init</a>
<a class="sourceLine" id="cb1-112" data-line-number="112">    <span class="cf">while</span> (tol <span class="op">&lt;</span><span class="st"> </span>conv <span class="op">|</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">100</span>){</a>
<a class="sourceLine" id="cb1-113" data-line-number="113">      <span class="cf">if</span> (<span class="kw">any</span>(<span class="kw">is.na</span>(alpha))){</a>
<a class="sourceLine" id="cb1-114" data-line-number="114">        alpha_init &lt;-<span class="st"> </span>alpha_init<span class="op">*</span><span class="dv">10</span></a>
<a class="sourceLine" id="cb1-115" data-line-number="115">        alpha &lt;-<span class="st"> </span>alpha_init</a>
<a class="sourceLine" id="cb1-116" data-line-number="116">      }</a>
<a class="sourceLine" id="cb1-117" data-line-number="117">      alpha0 &lt;-<span class="st"> </span>alpha</a>
<a class="sourceLine" id="cb1-118" data-line-number="118">      d1alpha &lt;-<span class="st"> </span>M<span class="op">*</span>(K<span class="op">*</span><span class="kw">digamma</span>(K<span class="op">*</span>alpha) <span class="op">-</span><span class="st"> </span>K<span class="op">*</span><span class="kw">digamma</span>(alpha)) <span class="op">+</span><span class="st"> </span><span class="kw">colSums</span>(<span class="kw">digamma</span>(gamma) <span class="op">-</span><span class="st"> </span>K<span class="op">*</span><span class="kw">digamma</span>(<span class="kw">rowSums</span>(gamma)))</a>
<a class="sourceLine" id="cb1-119" data-line-number="119">      d2alpha &lt;-<span class="st"> </span>M<span class="op">*</span>(K <span class="op">*</span><span class="st"> </span>K <span class="op">*</span><span class="st"> </span><span class="kw">trigamma</span>(K<span class="op">*</span>alpha) <span class="op">-</span><span class="st"> </span>K<span class="op">*</span><span class="kw">trigamma</span>(alpha)) <span class="co"># note sure how to use the Kronecker-delta function here</span></a>
<a class="sourceLine" id="cb1-120" data-line-number="120">      log_alpha &lt;-<span class="st"> </span><span class="kw">log</span>(alpha) <span class="op">-</span><span class="st"> </span>d1alpha<span class="op">/</span>(d2alpha<span class="op">*</span>alpha <span class="op">+</span><span class="st"> </span>d1alpha) </a>
<a class="sourceLine" id="cb1-121" data-line-number="121">      alpha &lt;-<span class="st"> </span><span class="kw">exp</span>(log_alpha)</a>
<a class="sourceLine" id="cb1-122" data-line-number="122">      conv &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(alpha<span class="op">-</span>alpha0))</a>
<a class="sourceLine" id="cb1-123" data-line-number="123">      iter &lt;-<span class="st"> </span>iter <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-124" data-line-number="124">    }</a>
<a class="sourceLine" id="cb1-125" data-line-number="125">  }</a>
<a class="sourceLine" id="cb1-126" data-line-number="126">  </a>
<a class="sourceLine" id="cb1-127" data-line-number="127">  <span class="co"># Check</span></a>
<a class="sourceLine" id="cb1-128" data-line-number="128">  word_counter &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,V,K)</a>
<a class="sourceLine" id="cb1-129" data-line-number="129">  <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M){</a>
<a class="sourceLine" id="cb1-130" data-line-number="130">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){</a>
<a class="sourceLine" id="cb1-131" data-line-number="131">      word &lt;-<span class="st"> </span>corpus[d,n] <span class="op">==</span><span class="st"> </span>vocab</a>
<a class="sourceLine" id="cb1-132" data-line-number="132">      topic &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dv">1</span>,<span class="dt">prob=</span>beta[,word])</a>
<a class="sourceLine" id="cb1-133" data-line-number="133">      word_counter[word,topic] &lt;-<span class="st"> </span>word_counter[word,topic] <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1-134" data-line-number="134">    }</a>
<a class="sourceLine" id="cb1-135" data-line-number="135">  }</a>
<a class="sourceLine" id="cb1-136" data-line-number="136">  </a>
<a class="sourceLine" id="cb1-137" data-line-number="137">  <span class="kw">print</span>(word_counter)</a>
<a class="sourceLine" id="cb1-138" data-line-number="138">}</a></code></pre></div>
</div>
</div>
</div>
<div id="supervised-lda" class="section level2">
<h2><span class="header-section-number">12.3</span> Supervised LDA</h2>
<p>For sLDA, we have the following generative model</p>
<p><img src="figs/tm/slda_graphical_model.png" /></p>
<p>This has a similar joint distribution to LDA, but note <span class="math inline">\(\eta\)</span>, <span class="math inline">\(\sigma^2\)</span>, and another observed variable <span class="math inline">\(y\)</span>. These are the regression coefficients, model noise, and document labels, respectively:</p>
<p><span class="math display">\[
p(\theta,z,w,y|\eta,\alpha,\beta,\sigma^2) = \prod_d p(\theta_d|\alpha) p(y_d|z_{d,n},\eta,\sigma^2) \prod_n p(z_{d,n}|\theta_d)p(w_{d,n}|z_{d,n},\beta)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;p(\theta_d|\alpha) = \frac{\Gamma (\sum_i \alpha_i)}{\prod_i \Gamma (\alpha_i)} \prod_i \theta_{d,k}^{\alpha_i -1} \text{, (Dirichlet)}\\
&amp;p(z_{d,n}|\theta_d) = \prod_n \prod_i \theta_i^{1[z_n=i]} \text{, (Multinomial)}\\
&amp;p(w_{d,n}|z_{d,n},\beta) = \prod_v \prod_i \beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]} \text{, (Multinomial)}\\
&amp;p(y_d|z_{d,n},\eta,\sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp [-\frac{(y-\bar Z \eta)^2}{2\sigma^2}] \text{, (Normal)}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\bar Z\)</span> is an <span class="math inline">\(K \times D\)</span> matrix of topic proportions such that <span class="math inline">\(\bar Z_{i,d}\)</span> is the proportion topic <span class="math inline">\(i\)</span> is represented out of all topics for document <span class="math inline">\(d\)</span>. Note that <span class="math inline">\(\sum_i \bar Z_{i,d}=1\)</span>.</p>
<p>We’ll again use the mean field method, giving us a fully factored form of the latent variables <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[
q(\theta,z|\gamma,\phi) = \prod_d q(\theta_d | \gamma_d) \prod_n q(z_{d,n}|\phi_{d,n})
\]</span></p>
<p>which allows us to derive our variational objective function</p>
<p><span class="math display">\[
\begin{aligned}
\log p&amp;(w|\alpha,\beta,\eta,\sigma^2) \geq L(\gamma,\phi|\alpha,\beta,\eta,\sigma^2) \\
    &amp;= \mathbb{E}_q [\log  p(\theta,z,w,y|\eta,\alpha,\beta,\sigma^2)] - \mathbb{E}_q [\log q(\theta,z)] \\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)p(y|z,\eta,\sigma^2)p(z|\theta)p(w|z,\beta)] - \mathbb{E}_q [\log q(\theta)q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha) + \log p(y|z,\eta,\sigma^2) + \log p(z|\theta) + \log p(w|z,\beta)] \\
    &amp;\qquad - \mathbb{E}_q [\log q(\theta) + \log q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)] + \mathbb{E}_q[\log p(y|z,\eta,\sigma^2)] + \mathbb{E}_q [\log p(z|\theta)] + \mathbb{E}_q[ \log p(w|z,\beta)] \\
    &amp;\qquad - \mathbb{E}_q [\log q(\theta)] - \mathbb{E}_q [\log q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)] + \mathbb{E}_q[\log p(y|z,\eta,\sigma^2)] + \mathbb{E}_q [\log p(z|\theta)] + \mathbb{E}_q[ \log p(w|z,\beta)] \\
    &amp;\qquad + \mathbb{H}_q [\gamma] + \mathbb{H}_q [\phi]
\end{aligned} 
\]</span></p>
<div id="expectations-of-pthetaalpha-pztheta-and-pwzbeta" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Expectations of <span class="math inline">\(p(\theta|\alpha)\)</span>, <span class="math inline">\(p(z|\theta)\)</span>, and <span class="math inline">\(p(w|z,\beta)\)</span></h3>
<p>These are identical to what we ended up with for LDA:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(\theta | \alpha)] &amp;= \log \Gamma (\sum_i^k \alpha_i) - \sum_i^k \log \Gamma (\alpha_i) + \sum_i^k (\alpha_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
\mathbb{E}_q  [\log p(z|\theta)] &amp;= \sum_n \sum_i \phi_{n,i} (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j))\\
\mathbb{E}_q [\log p(w|z,\beta)] &amp;= \sum_v \sum_i 1[w_{d,n}=v] \phi_{n,i} \log \beta_{i,v}
\end{aligned}
\]</span></p>
</div>
<div id="expectation-of-pyzetasigma2" class="section level3">
<h3><span class="header-section-number">12.3.2</span> Expectation of <span class="math inline">\(p(y|z,\eta,\sigma^2)\)</span></h3>
<p>First, keep in mind that we are tackling this expectation for a single document <span class="math inline">\(d\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(y|z,\eta,\sigma^2)] &amp;= \mathbb{E}_q [\log 1 + \log (2 \pi \sigma^2)^{-\frac{1}{2}} - \frac{(y-\bar Z \eta)^2}{2\sigma^2}]\\
    &amp;= \mathbb{E}_q [-\frac{1}{2} \log (2 \pi \sigma^2) - \frac{y^2 - 2y\bar Z \eta + \bar Z \eta \bar Z^T \eta}{2\sigma^2}]\\
    &amp;= -\frac{1}{2} \log (2 \pi \sigma^2) - \frac{y^2 - 2y\eta^T \mathbb{E}_q [\bar Z] + \eta^T \mathbb{E}_q [\bar Z \bar Z^T] \eta}{2\sigma^2}
\end{aligned}
\]</span></p>
<p>To determine <span class="math inline">\(\mathbb{E}_q [\bar Z\)</span>], recall that it is a <span class="math inline">\(K \times D\)</span> matrix representing the topic frequencies across documents. Because we are focusing on only one document, we are dealing with a K-vector. Recall <span class="math inline">\(z_n\)</span>. If we wrote this in vector form, then for a document <span class="math inline">\(d\)</span> and word <span class="math inline">\(n\)</span>, we’d have a K-vector of indicator values where 1 corresponds to the topic assignment, with all other topics receiving a 0. (If we were to store z, it would be a 3-dimensional array.) Now, we already defined <span class="math inline">\(\mathbb{E}_q 1[z_n=i] = \phi_{n,i}\)</span>, where <span class="math inline">\(\phi\)</span> is an <span class="math inline">\(N \times K\)</span> matrix of word occurrences across topics and it accounts for every document. The mean frequency of topic assignments would be column means of this matrix <span class="math inline">\(\phi\)</span>, which would be a K-vector we’ll call <span class="math inline">\(\bar \phi\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_q [\bar Z] = \bar \phi := \frac{1}{N}\sum_n \phi_{n}
\]</span></p>
<p>For <span class="math inline">\(\mathbb{E}_q [\bar Z \bar Z^T]\)</span>, first we’ll exploit the fact that our variational distribution is fully factorized, allowing us to assume independence between latent variables. For the case where <span class="math inline">\(n \neq m\)</span>, we get <span class="math inline">\(\mathbb{E}_q [ z_n z_m^T] = \mathbb{E}_q [z_n] \mathbb{E}_q [ z_m^T] = \phi_n \phi_m^T\)</span>, thanks to the full factorization. When <span class="math inline">\(n = m\)</span>, we have <span class="math inline">\(\mathbb{E}_q [z_n z_n^T]\)</span>, but recall that <span class="math inline">\(z_n\)</span> is just an indicator vector of length <span class="math inline">\(K\)</span>, consisting of zeros or ones, where <span class="math inline">\(z_n z_n^T\)</span> results in a <span class="math inline">\(K \times K\)</span> matrix, where there will be a <span class="math inline">\(1\)</span> somewhere on the diagonal corresponding to the topic assignment for word <span class="math inline">\(n\)</span>. Therefore, <span class="math inline">\(z_n z_n^T = \text{diag}(z_n)\)</span>. Now we have <span class="math inline">\(\mathbb{E}_q [z_n z_n^T] = \text{diag}(\mathbb{E}_q[z_n])\)</span>. The expectation <span class="math inline">\(\mathbb{E}_q [z_n]\)</span> is simply the topic frequencies for word <span class="math inline">\(n\)</span>; that is, <span class="math inline">\(\text{diag}(\mathbb{E}_q[z_n]) = \text{diag}(\phi_n)\)</span>. Now that we have <span class="math inline">\(\mathbb{E}_q [ z_n z_m^T]\)</span> when <span class="math inline">\(n \neq m\)</span> and <span class="math inline">\(\mathbb{E}_q [ z_n z_n^T]\)</span> when <span class="math inline">\(n = m\)</span>, we can add them together and divide by <span class="math inline">\(N^2\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_q [\bar Z \bar Z^T] = \frac{1}{N^2} (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n))
\]</span></p>
<p>Now, we can write the complete expectation:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(&amp;y|z,\eta,\sigma^2)] \\
    &amp;= -\frac{1}{2} \log (2 \pi \sigma^2) \\
    &amp;\qquad - (\frac{y^2 - \frac{2}{N}y\eta^T \sum_n \phi_{n} + \frac{1}{N^2} \eta^T (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)) \eta}{2\sigma^2})
\end{aligned}
\]</span></p>
</div>
<div id="entropy-of-gamma-and-phi-1" class="section level3">
<h3><span class="header-section-number">12.3.3</span> Entropy of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\phi\)</span></h3>
<p>These will be the same as LDA:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{H}_q [\gamma] &amp;= -\log \Gamma (\sum_j \gamma_j) + \sum_i \log \Gamma(\gamma_i) - \sum_i (\gamma_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j))\\
\mathbb{H}_q [\phi_{d,n}] &amp;= -\sum_i \phi_{d,n,i} \log \phi_{d,n,i}
\end{aligned}
\]</span></p>
</div>
<div id="complete-objective-function-1" class="section level3">
<h3><span class="header-section-number">12.3.4</span> Complete Objective Function</h3>
<p>Let’s fill in the ELBO:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q &amp;[\log  p(\theta,z,w,y|\eta,\alpha,\beta,\sigma^2)] - \mathbb{E}_q [\log q(\theta,z)] \\
    &amp;= \mathbb{E}_q [\log p(\theta|\alpha)] + \mathbb{E}_q[\log p(y|z,\eta,\sigma^2)] + \mathbb{E}_q [\log p(z|\theta)] + \mathbb{E}_q[ \log p(w|z,\beta)] \\
    &amp;\qquad + \mathbb{H}_q [\gamma] + \mathbb{H}_q [\phi]\\
    &amp;= \log \Gamma (\sum_i^k \alpha_i) - \sum_i^k \log \Gamma (\alpha_i) + \sum_i^k (\alpha_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
    &amp;\qquad -\frac{1}{2} \log (2 \pi \sigma^2) \\
    &amp;\qquad - \frac{y^2}{2\sigma^2} + \frac{y\eta^T \sum_n \phi_{n}}{N\sigma^2} - \frac{\eta^T (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)) \eta}{2N^2 \sigma^2} \\
    &amp;\qquad + \sum_n \sum_i \phi_{n,i} (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
    &amp;\qquad + \sum_n \sum_v \sum_i 1[w_{d,n}=v] \phi_{n,i} \log \beta_{i,v} \\
    &amp;\qquad -\log \Gamma (\sum_j \gamma_j) + \sum_i \log \Gamma(\gamma_i) - \sum_i (\gamma_i -1) (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) \\
    &amp;\qquad - \sum_n \sum_i \phi_{d,n,i} \log \phi_{d,n,i}
\end{aligned} 
\]</span></p>
</div>
<div id="parameter-optimization-1" class="section level3">
<h3><span class="header-section-number">12.3.5</span> Parameter Optimization</h3>
<div id="optimization-for-gamma_i-and-beta" class="section level4">
<h4><span class="header-section-number">12.3.5.1</span> Optimization for <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\beta\)</span></h4>
<p>These are the same as LDA since they don’t involve <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\gamma_i &amp;= \alpha_i + \sum_n \phi_{n,i}\\
\beta_{i,v} &amp;\propto \sum_n \sum_i \sum_v 1[w_n=v]\phi_{n,i}
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-phi-1" class="section level4">
<h4><span class="header-section-number">12.3.5.2</span> Optimization for <span class="math inline">\(\phi\)</span></h4>
<p>This will be somewhat similar to LDA, but we have to account for all of the terms associated with <span class="math inline">\(y\)</span>. First, let’s take all of the terms in the objective functions associated with <span class="math inline">\(\phi\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \phi_{n,i}} &amp;= \frac{\partial}{\partial \phi_{n,i}}[\frac{y\eta^T \sum_n \phi_{n}}{N\sigma^2} - \frac{\eta^T (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)) \eta}{2N^2 \sigma^2} \\
&amp;\qquad + \sum_n \sum_i \phi_{n,i} (\Psi (\gamma_i) - \Psi (\sum_j^k \gamma_j)) + \sum_n \sum_v \sum_i 1[w_{d,n}=v] \phi_{n,i} \log \beta_{i,v} \\
&amp;\qquad - \sum_n \sum_i \phi_{d,n,i} \log \phi_{d,n,i}]\\
&amp;= \frac{\partial}{\partial \phi_{n,i}}[\frac{y\eta^T \sum_n \phi_{n}}{N\sigma^2} - \frac{\eta^T (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)) \eta}{2N^2 \sigma^2}] \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - \log \phi_{n,i} - 1 + \lambda
\end{aligned} 
\]</span></p>
<p>So we easily optimized the terms we had to deal with for LDA, leaving us with our new terms that are associated with <span class="math inline">\(y\)</span>. The first terms is easy, so we’ll get it out of the way:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \phi_{n,i}} &amp;= \frac{y\eta}{N\sigma^2} - \frac{1}{2N^2 \sigma^2} \frac{\partial}{\partial \phi_{n,i}}[\eta^T (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)) \eta] \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - \log \phi_{n,i} - 1 + \lambda
\end{aligned} 
\]</span></p>
<p>We should stop here and talk about this remaining unoptimized term. If we are dealing with <span class="math inline">\(y \thicksim \text{Normal}(\mu,\sigma^2)\)</span> or <span class="math inline">\(y \thicksim \text{Poisson}(\lambda)\)</span>, then we can solve this and obtain and exact update for coordinate descent. Other types of labels will require a gradient optimization procedure, which would be the more generalized version of this variational EM strategy.</p>
<p>To calculate this partial derivative, we’ll focus on <span class="math inline">\(\phi_n\)</span> that corresponds to a single word <span class="math inline">\(n\)</span>, This is a vector of length <span class="math inline">\(K\)</span>. We’ll call this <span class="math inline">\(\phi_j\)</span>. This allows us to rewrite <span class="math inline">\(\eta^T (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)) \eta\)</span> as</p>
<p><span class="math display">\[
\begin{aligned}
f(\phi_j) &amp;= \eta^T [\phi_j \phi_{-j}^T + \phi_{-j} \phi_j^T + \text{diag}(\phi_j)] \eta + \text{const}\\
        &amp;= \eta^T \phi_j \phi_{-j}^T \eta + \eta^T \phi_{-j} \phi_j^T \eta + \eta^T \text{diag}(\phi_j) \eta + \text{const}
\end{aligned}
\]</span></p>
<p>First, notice that <span class="math inline">\(\eta^T \phi_j = \phi_j^T \eta\)</span> because they are both scalars (i.e., <span class="math inline">\(1 \times 1\)</span>). The same is true for <span class="math inline">\(\phi_{-j}^T \eta = \eta^T \phi_{-j}\)</span>. Therefore, we can rewrite <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[
\begin{aligned}
f(\phi_j) &amp;= 2 \eta^T \phi_{-j} \eta^T \phi_j + \eta^T \text{diag}(\phi_j) \eta + \text{const}
\end{aligned}
\]</span></p>
<p>Second, <span class="math inline">\(\eta^T \text{diag}(\phi_j) \eta\)</span> is also a scalar. It’s worth making some vectors in R and testing this, but <span class="math inline">\(\eta^T \text{diag}(\phi_j)\)</span> simply multiplies each element in <span class="math inline">\(\eta\)</span> with each element in <span class="math inline">\(\phi\)</span> and then returns a row vector of length <span class="math inline">\(K\)</span>. Then we calculate its dot product with <span class="math inline">\(\eta\)</span>, giving us a scalar. This is exactly the same as doing <span class="math inline">\((\eta \circ \eta)^T \phi\)</span>; therefore, we have the following:</p>
<p><span class="math display">\[
\begin{aligned}
f(\phi_j) &amp;= 2 \eta^T \phi_{-j} \eta^T \phi_j + (\eta \circ \eta)^T \phi_j + \text{const}
\end{aligned}
\]</span></p>
<p>Now, in this form, we can easily compute the gradient:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial f}{\partial \phi_j} &amp;= \frac{\partial}{\partial \phi_l} [2\eta^T \phi_{-j} \eta^T \phi_j + (\eta \circ \eta)^T \phi_j + \text{const}]\\
            &amp;= 2\eta^T \phi_{-j} \eta + (\eta \circ \eta)
\end{aligned}
\]</span></p>
<p>Substituting this into our original equation, we get</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \phi_{n,i}} &amp;= \frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - \log \phi_{n,i} - 1 + \lambda
\end{aligned}
\]</span></p>
<p>And then we set this equal to <span class="math inline">\(0\)</span> and solve:</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= \frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - \log \phi_{n,i} - 1 + \lambda\\
\log \phi_{n,i} &amp;= \frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - 1 + \lambda\\
\phi_{n,i} &amp;= \exp[\frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v} - 1 + \lambda]\\
&amp;= \exp[- 1 + \lambda]\exp[\frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v}]\\
&amp;= c_{n,i}\exp[\frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} \\
&amp;\qquad + \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v}]\\
&amp;\propto \exp[\frac{y\eta}{N\sigma^2} - \frac{2\eta^T \phi_{-j} \eta + (\eta \circ \eta)}{2N^2 \sigma^2} 
+ \Psi &#39; (\gamma) - \Psi &#39; (\sum_j \gamma_j) + 1[w_n = v] \log \beta_{i,v}]
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-eta" class="section level4">
<h4><span class="header-section-number">12.3.5.3</span> Optimization for <span class="math inline">\(\eta\)</span></h4>
<p>Again, we’ll isolate the terms involving <span class="math inline">\(\eta\)</span>, but let’s first recall that the only expectation that had <span class="math inline">\(\eta\)</span> terms was <span class="math inline">\(\mathbb{E}_q [\log p(y|z,\eta,\sigma^2]\)</span>. Also recall that <span class="math inline">\(\eta\)</span> are regression coefficients where <span class="math inline">\(p(y_d | z_{d,n},\eta,\sigma^2) \thicksim \text{Normal}(\bar Z \eta, \sigma^2)\)</span>, which is the familiar regression model. This should make our lives easier, so let’s use this, which we’ll call <span class="math inline">\(g\)</span> for now:</p>
<p><span class="math display">\[
\begin{aligned}
g(y) &amp;= \mathbb{E}_q [-\frac{1}{2} \log (2 \pi \sigma^2) - \frac{(y-\bar Z \eta)^2}{2\sigma^2}]
\end{aligned}
\]</span>
Now we’ll rewrite it for all documents (remember that for our previous derivations, we assumed only one document):</p>
<p><span class="math display">\[
\begin{aligned}
g(y_{1:D}) &amp;= \mathbb{E}_q [\sum_d [-\frac{1}{2} \log (2 \pi \sigma^2) - \frac{(y_d-\bar Z \eta)^2}{2\sigma^2}]]\\
    &amp;= \mathbb{E}_q [\sum_d [-\frac{1}{2} \log (2 \pi \sigma^2)] - \sum_d [\frac{(y_d-\bar Z \eta)^2}{2\sigma^2}]]\\
    &amp;= \mathbb{E}_q [-\frac{D}{2} \log (2 \pi \sigma^2) - \frac{\sum_d(y_d-\bar Z \eta)^2}{2\sigma^2}]\\
    &amp;= \mathbb{E}_q [-\frac{D}{2} \log (2 \pi \sigma^2) - \frac{\sum_d(y_d-\bar Z \eta)(y_d-\bar Z \eta)}{2\sigma^2}]\\
    &amp;= \mathbb{E}_q [-\frac{D}{2} \log (2 \pi \sigma^2) - \frac{\sum_d(y_d-\bar Z \eta)(y_d-\bar Z \eta)}{2\sigma^2}]
\end{aligned}
\]</span></p>
<p>Let’s rewrite this in matrix form where <span class="math inline">\(y_{1:D}= y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
g( y) &amp;= \mathbb{E}_q [-\frac{D}{2} \log (2 \pi \sigma^2) - \frac{( y-\bar Z \eta)^T( y-\bar Z \eta)}{2\sigma^2}]\\
&amp;= -\frac{D}{2} \log (2 \pi \sigma^2) - \frac{1}{2\sigma^2}\mathbb{E}_q [( y-\bar Z \eta)^T( y-\bar Z \eta)]
\end{aligned}
\]</span></p>
<p>Finally, using <span class="math inline">\(g\)</span>, let’s take the partial with respect to <span class="math inline">\(\eta\)</span> and maximize. This is completely analogous to finding the MLE in a linear regression model:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial g}{\partial \eta} &amp;= \frac{\partial}{\partial \eta} [-\frac{D}{2} \log (2 \pi \sigma^2) - \frac{1}{2\sigma^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)]\\
&amp;= \frac{\partial}{\partial \eta} [ - \frac{1}{2\sigma^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)]\\
&amp;= -\frac{1}{2\sigma^2}\mathbb{E}_q[\frac{\partial}{\partial \eta} ( y-\bar Z \eta)^T( y-\bar Z \eta)]\\
&amp;= -\frac{1}{2\sigma^2}\mathbb{E}_q[( y - \bar Z \eta)^T (-\bar Z) + ( y - \bar Z \eta)^T (-\bar Z)]\\
&amp;= -\frac{1}{2\sigma^2}\mathbb{E}_q[-2( y - \bar Z \eta)^T \bar Z]\\
&amp;= \frac{1}{\sigma^2}\mathbb{E}_q[( y - \bar Z \eta)^T \bar Z]\\
&amp;= \frac{1}{\sigma^2}\mathbb{E}_q[( y^T - (\bar Z \eta)^T) \bar Z]\\
&amp;= \frac{1}{\sigma^2}\mathbb{E}_q[( y^T - \eta^T \bar Z^T) \bar Z]\\
&amp;= \frac{1}{\sigma^2}\mathbb{E}_q[( y^T \bar Z - \eta^T \bar Z^T \bar Z)]\\
&amp;= \frac{1}{\sigma^2}( y^T \mathbb{E}_q[\bar Z] - \eta^T\mathbb{E}_q[\bar Z^T \bar Z])
\end{aligned}
\]</span></p>
<p>Setting this equal to zero, we have</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= \frac{1}{\sigma^2}( y^T \mathbb{E}_q[\bar Z] - \eta^T\mathbb{E}_q[\bar Z^T \bar Z])\\
0 &amp;=  y^T \mathbb{E}_q[\bar Z] - \eta^T\mathbb{E}_q[\bar Z^T \bar Z]\\
\eta^T\mathbb{E}_q[\bar Z^T \bar Z] &amp;=  y^T \mathbb{E}_q[\bar Z]\\
\eta^T &amp;=  y^T \mathbb{E}_q[\bar Z] (\mathbb{E}_q[\bar Z^T \bar Z])^{-1}\\
\eta &amp;= ( y^T \mathbb{E}_q[\bar Z] (\mathbb{E}_q[\bar Z^T \bar Z])^{-1})^T\\
\eta &amp;= ( y^T \mathbb{E}_q[\bar Z])^T ((\mathbb{E}_q[\bar Z^T \bar Z])^{-1})^T\\
\eta &amp;= \mathbb{E}_q[\bar Z^T \bar Z]^{-1} \mathbb{E}_q[\bar Z]^T  y\\
&amp;= (\frac{1}{N^2} (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)))^{-1} (\frac{1}{N} \sum_n \phi_n)^T  y
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-sigma2" class="section level4">
<h4><span class="header-section-number">12.3.5.4</span> Optimization for <span class="math inline">\(\sigma^2\)</span></h4>
<p>For the dispersion parameter, since it’s only related to the regression part of our model, we’ll reuse our <span class="math inline">\(g( y)\)</span> function, but we’ll define <span class="math inline">\(\delta := \sigma^2\)</span> just so notation is easier to follow.</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial g}{\partial \delta} &amp;= \frac{\partial}{\partial \delta} [-\frac{D}{2} \log (2 \pi \delta) - \frac{1}{2\delta}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)]\\
&amp;= -\frac{D}{2\delta} + \frac{1}{2\delta^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)
\end{aligned}
\]</span></p>
<p>Setting this equal to zero, we get</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= -\frac{D}{2\delta} + \frac{1}{2\delta^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)\\
&amp;= \frac{1}{2}[-\frac{D}{\delta} + \frac{1}{\delta^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)]\\
&amp;= -\frac{D}{\delta} + \frac{1}{\delta^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)\\
\frac{D}{\delta} &amp;= \frac{1}{\delta^2}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)\\
D &amp;= \frac{1}{\delta}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)\\
\delta &amp;= \frac{1}{D}\mathbb{E}_q( y-\bar Z \eta)^T( y-\bar Z \eta)
\end{aligned}
\]</span></p>
<p>And now we’ll expand.</p>
<p><span class="math display">\[
\begin{aligned}
\delta &amp;= \frac{1}{D}\mathbb{E}_q[( y-\bar Z \eta)^T( y-\bar Z \eta)]\\
&amp;= \frac{1}{D}\mathbb{E}_q[ y^T  y -  y^T \bar Z \eta - \eta^T \bar Z^T  y + \eta^T \bar Z^T \bar Z \eta]\\
&amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \eta - \eta^T \mathbb{E}_q[\bar Z^T]  y + \eta^T \mathbb{E}_q[\bar Z^T \bar Z] \eta)
\end{aligned}
\]</span></p>
<p>Now, in our optimization for <span class="math inline">\(\eta\)</span>, we saw:</p>
<p><span class="math display">\[
\eta^T\mathbb{E}_q[\bar Z^T \bar Z] =  y^T \mathbb{E}_q[\bar Z]
\]</span></p>
<p>Which can be rewritten as</p>
<p><span class="math display">\[
\begin{aligned}
\eta^T\mathbb{E}_q[\bar Z^T \bar Z] &amp;=  y^T \mathbb{E}_q[\bar Z]\\
(\eta^T\mathbb{E}_q[\bar Z^T \bar Z])^T &amp;= ( y^T \mathbb{E}_q[\bar Z])^T\\
\mathbb{E}_q[\bar Z^T \bar Z] \eta &amp;= \mathbb{E}_q[\bar Z]^T  y
\end{aligned}
\]</span></p>
<p>Now let’s substitute this into our equation for <span class="math inline">\(\delta\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\delta &amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \eta - \eta^T \mathbb{E}_q[\bar Z^T]  y + \eta^T \mathbb{E}_q[\bar Z^T \bar Z] \eta)\\
&amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \eta - \eta^T \mathbb{E}_q[\bar Z^T]  y + \eta^T \mathbb{E}_q[\bar Z]^T  y)\\
&amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \eta)
\end{aligned}
\]</span></p>
<p>We know that <span class="math inline">\(\eta = \mathbb{E}_q[\bar Z^T \bar Z]^{-1} \mathbb{E}_q[\bar Z]^T y\)</span>, giving us</p>
<p><span class="math display">\[
\begin{aligned}
\delta &amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \eta)\\
&amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \mathbb{E}_q[\bar Z^T \bar Z]^{-1} \mathbb{E}_q[\bar Z]^T  y)
\end{aligned}
\]</span></p>
<p>Finally, we’ll replace <span class="math inline">\(\delta\)</span> with <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\sigma^2 &amp;= \frac{1}{D}( y^T  y -  y^T \mathbb{E}_q[\bar Z] \mathbb{E}_q[\bar Z^T \bar Z]^{-1} \mathbb{E}_q[\bar Z]^T  y)\\
&amp;= \frac{1}{D}( y^T  y -  y^T (\frac{1}{N} \sum_n \phi_n) (\sum_n \sum_{n \neq m} \phi_n \phi_m^T + \sum_n \text{diag}(\phi_n)))^{-1} (\frac{1}{N} \sum_n \phi_n)^T  y)
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div id="the-correlated-topic-model" class="section level2">
<h2><span class="header-section-number">12.4</span> The Correlated Topic Model</h2>
<p>This model is similar to LDA, except we replace the Dirichlet distribution of topics over documents with a logistic Normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> of length <span class="math inline">\(K\)</span> and dimensions <span class="math inline">\(K \times K\)</span>, respectively. The generative model is the following:</p>
<p><img src="figs/tm/clda_graphical_model.png" /></p>
<p>The joint distribution over all latent variables <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span> and observed data <span class="math inline">\(w\)</span> is then</p>
<p><span class="math display">\[
p(\theta,z,w|\alpha,\beta) = \prod_d p(\eta_d|\mu,\Sigma) \prod_n p(w_{d,n}|z_{d,n},\beta) p(z_{d,n}|\eta_d)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;p(\eta_d|\mu,\Sigma) = \frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp [-\frac{1}{2}(\eta - \mu)^T \Sigma^{-1} (\eta - \mu)] \text{, (Multivariate Normal)}\\
&amp;p(z_{d,n}|\eta_d) = \prod_n \prod_i \theta_i^{1[z_n=i]} \text{, (Multinomial)}\\
&amp;p(w_{d,n}|z_{d,n},\beta) = \prod_v \prod_i \beta_{i,v}^{1[w_{d,n}=v,z_{d,n}=i]} \text{, (Multinomial)}
\end{aligned}
\]</span></p>
<div id="multinomial-distribution-in-exponential-form" class="section level3">
<h3><span class="header-section-number">12.4.1</span> Multinomial Distribution in Exponential Form</h3>
<p>Let’s convert this multinomial <span class="math inline">\(p(z|\eta)\)</span> into its natural parameter form by exploiting the fact that it belongs to the exponential family. The game is that we need to express this distribution in the form <span class="math inline">\(p(x|\eta) = h(z) \exp [\eta^T \phi (x) - A(\eta)]\)</span>. Note, we’ll work with only one word <span class="math inline">\(n\)</span> to simplify things, so</p>
<p><span class="math display">\[
\begin{aligned}
p(z|\theta) &amp;= \prod_i^K \theta_i^{1[z_n=i]}\\
    &amp;= \prod_i^K \exp [\log \theta_i^{1[z_n=i]}]\\
    &amp;= \exp [\sum_i^K \log \theta_i^{1[z_n=i]}]\\
    &amp;= \exp [\sum_i^K 1[z_n=i] \log \theta_i]
\end{aligned}
\]</span></p>
<p>Now we’ll split the sum from all <span class="math inline">\(\sum_i^K\)</span> to <span class="math inline">\(\sum_i^{K-1} + 1-\sum_i^{K-1}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
p(z|\theta) &amp;= \exp [\sum_i 1[z_n=i] \log \theta_i]\\
    &amp;= \exp [\sum_i^{K-1} 1[z_n=i] \log \theta_i + (1-\sum_i^{K-1} 1[z_n=i]) \log (1-\sum_i^{K-1}\theta_i)]
\end{aligned}
\]</span></p>
<p>Then we expand and group terms:</p>
<p><span class="math display">\[
\begin{aligned}
p(z|\theta) &amp;= \exp [\sum_i^{K-1} 1[z_n=i] \log \theta_i + \log (1-\sum_i^{K-1}\theta_i) -\sum_i^{K-1} 1[z_n=i] \log (1-\sum_i^{K-1}\theta_i)]\\
    &amp;= \exp [\sum_i^{K-1} 1[z_n=i] (\log \theta_i - \log (1-\sum_i^{K-1}\theta_i)) + \log (1-\sum_i^{K-1}\theta_i)]\\
    &amp;= \exp [\sum_i^{K-1} 1[z_n=i] \log (\frac{\theta_i}{1-\sum_j^{K-1}\theta_j}) + \log (1-\sum_i^{K-1}\theta_i)]\\
\end{aligned}
\]</span></p>
<p>We can finally see the exponential form taking shape where</p>
<p><span class="math display">\[
\eta_i = \log(\frac{\theta_i}{1-\sum_j^{K-1}\theta_j}) = \log(\frac{\theta_i}{\theta_i})
\]</span></p>
<p>Now we can determine <span class="math inline">\(\theta_i\)</span> by expressing this equation in terms of <span class="math inline">\(\eta_i\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\eta_i &amp;= \log (\frac{\theta_i}{\theta_i})\\
\exp [\eta_i] &amp;= \frac{\theta_i}{\theta_i}\\
\theta_i &amp;= \theta_i \exp [\eta_i]\\
 &amp;= (1-\sum_j^{K-1}\theta_j) \exp [\eta_i]\\
 &amp;= \frac{1}{\frac{1}{(1-\sum_j^{K-1}\theta_j)}} \exp [\eta_i]
\end{aligned}
\]</span></p>
<p>For convenience, we’ll assume <span class="math inline">\(\theta_i=\)</span>, so <span class="math inline">\(\sum_i^{K-1}\theta_i = \sum_i^{K-1}\theta_i + \theta_i = \sum_i^{K-1}\theta_i + 0 = \sum_i^{K}\theta_i\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\theta_i &amp;= \frac{1}{\frac{1}{(1-\sum_j^{K-1}\theta_j)}} \exp [\eta_i]\\
&amp;= \frac{1}{\frac{1}{(1-\sum_j^{K}\theta_j)}} \exp [\eta_i]
\end{aligned}
\]</span></p>
<p>Then we can take advantage of the following constraint: <span class="math inline">\(\sum_j^K \theta_j = 1\)</span>, so</p>
<p><span class="math display">\[
\begin{aligned}
\theta_i &amp;= \frac{1}{\frac{1}{(1-\sum_j^{K}\theta_j)}} \exp [\eta_i]\\
&amp;= \frac{1}{\frac{\sum_j^K \theta_j}{(1-\sum_j^{K}\theta_j)}} \exp [\eta_i]\\
&amp;= \frac{1}{\sum_j^K\frac{ \theta_j}{(1-\sum_j^{K}\theta_j)}} \exp [\eta_i]\\
&amp;= \frac{1}{\sum_j^K \exp[\log \frac{ \theta_j}{(1-\sum_j^{K}\theta_j)}]} \exp [\eta_i]\\
&amp;= \frac{1}{\sum_j^K \exp[\log \frac{ \theta_j}{\theta_i}]} \exp [\eta_i]\\
&amp;= \frac{\exp [\eta_i]}{\sum_j^K \exp[\eta_j]}\\
\end{aligned}
\]</span></p>
<p>which is the softmax function. Now we can extract our natural parameters:</p>
<p><span class="math display">\[
\begin{aligned}
h(z) &amp;= 1\\
\phi(x) &amp;= [1[z_n=1], ..., 1[z_n=K-1]]\\
\eta &amp;= [\log(\frac{\theta_1}{1-\sum_j^{K-1}\theta_j}), ..., \log(\frac{\theta_{K-1}}{1-\sum_j^{K-1}\theta_j}),0]\\
    &amp;= [\log(\frac{\theta_1}{\theta^K}),...,\log(\frac{\theta_{K-1}}{\theta^K}),0]
\end{aligned}
\]</span></p>
<p>We’ll use the following notation <span class="math inline">\(\phi (x) := z\)</span>. For the cumulant, let’s again use <span class="math inline">\(\eta_i = \log (\frac{\theta_i}{\theta_i})\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\eta_i &amp;= \log (\frac{\theta_i}{\theta_i})\\
\theta_i &amp;= \theta_i \exp [\eta_i]\\
\sum_i^{K-1} \theta_i &amp;= \theta_i \sum_i^{K-1} \exp [\eta_i]\\
1-(1-\sum_i^{K-1} \theta_i) &amp;= \theta_i \sum_i^{K-1} \exp [\eta_i]\\
1-\theta_i &amp;= \theta_i \sum_i^{K-1} \exp [\eta_i]\\
1 &amp;= \theta_i (1+\sum_i^{K-1} \exp [\eta_i]\\
\theta_i &amp;= \frac{1}{1+\sum_i^{K-1} \exp [\eta_i]}\\
1-\sum_i^{K-1} \theta_i &amp;= \frac{1}{1+\sum_i^{K-1} \exp [\eta_i]}
\end{aligned}
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
\begin{aligned}
A(\eta) &amp;= -\log (1-\sum_i^{K-1} \theta_i)\\
&amp;= -\log (\frac{1}{1+\sum_i^{K-1} \exp [\eta_i]})\\
&amp;= \log (1+\sum_i^{K-1} \exp [\eta_i])\\
\end{aligned}
\]</span></p>
<p>And if we assume that <span class="math inline">\(\theta_i = 0\)</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
A(\eta) &amp;= \log (1+\sum_i^{K-1} \exp [\eta_i])\\
&amp;= \log (\sum_i^{K} \exp [\eta_i])\\
\end{aligned}
\]</span></p>
<p>Thus, <span class="math inline">\(p(z|\eta)\)</span> in its natural paramterization is</p>
<p><span class="math display">\[
\begin{aligned}
p(z_n|\eta) &amp;= 1 \times \exp [\eta^T z_n - A(\eta)]\\
            &amp;= \exp [\eta^T z_n - \log (\sum_i^{K} \exp [\eta_i])]
\end{aligned}
\]</span></p>
</div>
<div id="variational-em" class="section level3">
<h3><span class="header-section-number">12.4.2</span> Variational EM</h3>
<p>Now, our variational distribution for CTM will be a again be a mean field distribution where we assume complete independence between each <span class="math inline">\(\eta\)</span> and between all topic assignments <span class="math inline">\(z\)</span>, i.e., a fully factored form. Note, unlike before, we’re showing the variational distribution for only one document:</p>
<p><span class="math display">\[
q(\eta,z|\lambda,\nu,\phi) = \prod_i q(\eta_i|\lambda_i,\nu_i^2) \prod_n q(z_n|\phi_n)
\]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is <span class="math inline">\(K \times N\)</span>, as before, and <span class="math inline">\(\eta_i \thicksim \text{Normal}(\gamma_i,\nu_i)\)</span> – that is, each <span class="math inline">\(\eta_i\)</span> is distributed by its own <em>univariate</em> Gaussian.</p>
<p>For the joint distribution over latent variables, we have</p>
<p><span class="math display">\[
\begin{aligned}
\log p&amp;(w|\mu, \Sigma, \beta) \geq L(\gamma,\nu,\phi|\mu, \Sigma, \beta) \\
    &amp;= \mathbb{E}_q [\log  p(\eta,z,w|\mu, \Sigma, \beta)] - \mathbb{E}_q [\log q(\eta,z)] \\
    &amp;= \mathbb{E}_q [\log p(\eta|\mu,\Sigma)p(z|\theta)p(w|z,\beta)] - \mathbb{E}_q [\log q(\eta)q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\eta|\mu,\Sigma) + \log p(z|\eta) + \log p(w|z,\beta)] - \mathbb{E}_q [\log q(\eta) + \log q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\eta|\mu,\Sigma)] + \mathbb{E}_q [\log p(z|\eta)] + \mathbb{E}_q [\log p(w|z,\beta)] \\
    &amp;\qquad - \mathbb{E}_q [\log q(\eta)] - \mathbb{E}_q[\log q(z)]\\
    &amp;= \mathbb{E}_q [\log p(\eta|\mu,\Sigma)] + \mathbb{E}_q [\log p(z|\eta)] + \mathbb{E}_q [\log p(w|z,\beta)] + \mathbb{H}_q [\lambda,\nu^2] + \mathbb{H}_q[\phi]
\end{aligned} 
\]</span></p>
</div>
<div id="expectation-of-pwzbeta-1" class="section level3">
<h3><span class="header-section-number">12.4.3</span> Expectation of <span class="math inline">\(p(w|z,\beta)\)</span></h3>
<p>This is analogous to LDA:</p>
<p><span class="math display">\[
\mathbb{E}_q [\log p(w|z,\beta)] = \sum_v \sum_i 1[w_{d,n}=v] \phi_{n,i} \log \beta_{i,v}
\]</span></p>
</div>
<div id="expectation-of-pzeta" class="section level3">
<h3><span class="header-section-number">12.4.4</span> Expectation of <span class="math inline">\(p(z|\eta)\)</span></h3>
<p>In LDA, working with <span class="math inline">\(p(z|\theta)\)</span> was simple because of the conjugacy between Multinomial and Dirichlet distributions, respectively. Now, however, we are working with a Normal prior, which is <em>not</em> conjugate with the Multinomial. Similar to before, <span class="math inline">\(z_n \thicksim \text{Multinomial}(f(\eta))\)</span>. Now recall that <span class="math inline">\(\eta_i \thicksim \text{Normal}(\gamma_i,\nu_i)\)</span> and then mapped onto the simplex via <span class="math inline">\(f(\eta_i)= \exp \eta_i / \sum_j \exp \eta_j\)</span>. You can see this relationship by returning to the natural parameterization of the Multinomial distribution <span class="math inline">\(p(z|\eta)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\log p(z|\eta) &amp;= \log \exp [\eta^T z_n - \log (\sum_i^{K} \exp [\eta_i])]\\
    &amp;= \eta^T z_n - \log (\sum_i^{K} \exp [\eta_i])\\
    &amp;= \log \exp [\eta^T z_n] - \log (\sum_i^{K} \exp [\eta_i])\\
    &amp;= \log [\frac{\exp [\eta^T z_n]}{\sum_i^{K} \exp [\eta_i]}]\\
\end{aligned}
\]</span></p>
<p>which shows <span class="math inline">\(\eta\)</span> being mapped to the simplex. Now, let’s derive the expectation:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z_n|\eta)] &amp;= \mathbb{E}_q [\log \exp [\eta^T z_n - \log(\sum_i^{K} \exp [\eta_i])]] \\ 
    &amp;= \mathbb{E}_q [\eta^T z_n - \log(\sum_i^{K} \exp [\eta_i])] \\ 
    &amp;= \mathbb{E}_q [\eta^T z_n] - \mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])] \\ 
\end{aligned}
\]</span></p>
<p>The left side is easy. Recall that <span class="math inline">\(z_n\)</span> are simply indicator functions for a particular topic assignment, so for a given document and word <span class="math inline">\(n\)</span>, <span class="math inline">\(z_n\)</span> is a K-vector of indicator values where the topic <span class="math inline">\(k\)</span> is equal to <span class="math inline">\(1\)</span> and all other topics are <span class="math inline">\(0\)</span>. Its expectation is the corresponding variational parameter <span class="math inline">\(\phi_{n,i}\)</span>, which is simply a matrix of values that reflects the frequency in which a token <span class="math inline">\(n\)</span> takes on the topic assignment <span class="math inline">\(i\)</span>. Because <span class="math inline">\(\eta_i\)</span> is a Gaussian with mean <span class="math inline">\(\lambda_i\)</span> that corresponds to topic <span class="math inline">\(i\)</span>, its expectation is simply <span class="math inline">\(\lambda\)</span>. Thus, the expectation of <span class="math inline">\(\eta^T z_n\)</span> is simply the sum across tokens of the product between the average prior value <span class="math inline">\(\lambda_i\)</span> for token <span class="math inline">\(i\)</span> and the number of times token <span class="math inline">\(n\)</span> was assigned to topic <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\eta^T z_n] = \sum_n \sum_i \lambda_i \phi_{n,i} \\ 
\end{aligned}
\]</span></p>
<p>Giving us</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z_n|\eta)] &amp;= \mathbb{E}_q [\eta^T z_n] - \mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])]\\
    &amp;= \sum_n \sum_i \lambda_i \phi_{n,i} - \mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])] 
\end{aligned}
\]</span></p>
<p>The right side, on the other hand, is intractable, but we can introduce an upper bound to <span class="math inline">\(\mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])]\)</span>. Think about it like this. For variational inference, we are maximizing a lower bound of the log probability of our model. The term we are focusing on is <span class="math inline">\(\mathbb{E}_q [\eta^T z_n] - \mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])]\)</span>, which we’ll rewrite as <span class="math inline">\(\mathbb{E}_q [A - B]\)</span>. We need to ensure that our lower bound from the variational distribution remains the lower bound. Therefore, our approximation of <span class="math inline">\(\mathbb{E}_q [A - B]\)</span> must be smaller, which would give us a smaller approximated lower bound and hence smaller than the true lower bound. Since <span class="math inline">\(B\)</span> is the part that is intractable, we only need to approximate it, and not the left term. If we increase <span class="math inline">\(B\)</span>, we’ll consequently decrease our approximation of <span class="math inline">\(\mathbb{E}_q [\log p(z_n|\eta)]\)</span>, giving us an approximated lower bound that’s smaller than the true lower bound, which is what we want. Therefore, we need to find <span class="math inline">\(C\)</span> such that <span class="math inline">\(\mathbb{E}_q [A - B] \leq C\)</span>.</p>
<p>To find “<span class="math inline">\(C\)</span>”, we do the following. We’ll approximate the intractable sum as <span class="math inline">\(\log x \approx x-1\)</span>, which is the case when <span class="math inline">\(x\)</span> is around <span class="math inline">\(1\)</span>. Also, <span class="math inline">\(\log x\)</span> is never greater than <span class="math inline">\(x-1\)</span> and hence <span class="math inline">\(x-1\)</span> serves as a tight upper bound:</p>
<p><img src="figs/tm/xminus1.png" /></p>
<p>We will also introduce a new variational parameter <span class="math inline">\(\xi\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])] &amp;=  \mathbb{E}_q [\log(\xi \xi^{-1} \sum_i^{K} \exp [\eta_i])]\\
    &amp;= \mathbb{E}_q [\log \xi + \log (\xi^{-1} \sum_i^{K} \exp [\eta_i])]\\
    &amp;= \log \xi + \mathbb{E}_q [\log (\xi^{-1} \sum_i^{K} \exp [\eta_i])]\\
    &amp;\leq \log \xi + \mathbb{E}_q [\xi^{-1} \sum_i^{K} \exp [\eta_i] - 1]\\
    &amp;\leq \log \xi + \xi^{-1} \sum_i^{K} \mathbb{E}_q [\exp [\eta_i]] - 1\\
\end{aligned}
\]</span></p>
<p>And so we have our almost final form for this expectation:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z_n|\eta)] &amp;= \sum_n (\sum_i \lambda_i \phi_{n,i} - \mathbb{E}_q [\log(\sum_i^{K} \exp [\eta_i])])\\
    &amp;=\sum_n (\sum_i \lambda_i \phi_{n,i} - \log \xi + \mathbb{E}_q [\log (\xi^{-1} \sum_i^{K} \exp [\eta_i])])\\
    &amp;\geq \sum_n (\sum_i \lambda_i \phi_{n,i} - (\log \xi + \xi^{-1} \sum_i^{K} \mathbb{E}_q [\exp [\eta_i]] - 1))
\end{aligned}
\]</span></p>
<p>Now, because <span class="math inline">\(\eta_i \thicksim \text{Normal} (\lambda_i,\nu_i^2)\)</span>, <span class="math inline">\(\mathbb{E}_q [\exp [\eta_i]] = \exp [\lambda_i + \nu_i^2 /2]\)</span>, which allows us to write our final form:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [\log p(z_n|\eta)] &amp;\geq \sum_n (\sum_i \lambda_i \phi_{n,i} - (\log \xi + \xi^{-1} \sum_i^{K} \mathbb{E}_q [\exp [\eta_i]] - 1))\\
&amp;\geq \sum_n (\sum_i \lambda_i \phi_{n,i} - (\log \xi + \xi^{-1} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}] - 1))\\
&amp;\geq \sum_n (\sum_i \lambda_i \phi_{n,i} - \log \xi - \xi^{-1} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}] + 1)
\end{aligned}
\]</span></p>
</div>
<div id="expectation-of-petamusigma" class="section level3">
<h3><span class="header-section-number">12.4.5</span> Expectation of <span class="math inline">\(p(\eta|\mu,\sigma)\)</span></h3>
<p>We know that <span class="math inline">\(p(\eta|\mu,\Sigma) \thicksim \text{Normal}_i (\mu,\Sigma)\)</span>, so</p>
<p><span class="math display">\[
\begin{aligned}
\log p(\eta|\mu,\Sigma) &amp;= \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2\pi - \frac{1}{2} \mathbb{E}_q [(\eta - \mu)^T \Sigma^{-1} (\eta - \mu)] 
\end{aligned}
\]</span></p>
<p>We can rewrite <span class="math inline">\((\eta - \mu)^T \Sigma^{-1} (\eta - \mu)\)</span> in a more manageable form. Using <span class="math inline">\(x^T A x = \text{tr}(x^T A x) = \text{tr}(A x^T x)\)</span>, we have the following:</p>
<p><span class="math display">\[
\begin{aligned}
(\eta - \mu)^T \Sigma^{-1} (\eta - \mu) &amp;= \text{tr}(\Sigma^{-1}  (\eta - \mu)^T (\eta - \mu))\\
    &amp;= \text{tr}(\Sigma^{-1}  \sum_i (\eta_i - \mu) (\eta_i - \mu))\\
    &amp;= \text{tr}(\Sigma^{-1}  \sum_i (\eta_i + (\lambda_i - \lambda_i) - \mu) (\eta_i + (\lambda_i - \lambda_i) - \mu))\\
    &amp;= \text{tr}(\Sigma^{-1}  \sum_i (\eta_i + \lambda_i - \lambda_i - \mu) (\eta_i + \lambda_i - \lambda_i - \mu))\\
    &amp;= \text{tr}(\Sigma^{-1}  (\sum_i (\eta_i - \lambda_i)(\eta_i - \lambda_i ) + \sum_i (\lambda_i - \mu)(\lambda_i - \mu)))\\
    &amp;= \text{tr}(\Sigma^{-1}\sum_i (\eta_i - \lambda_i)(\eta_i - \lambda_i ) + \Sigma^{-1}\sum_i (\lambda_i - \mu)(\lambda_i - \mu))\\
    &amp;= \text{tr}(\Sigma^{-1}\sum_i (\eta_i - \lambda_i)(\eta_i - \lambda_i )) + \text{tr}(\Sigma^{-1}\sum_i (\lambda_i - \mu)(\lambda_i - \mu))\\
    &amp;= \text{tr}(\Sigma^{-1}\sum_i (\eta_i - \lambda_i)^2) + \text{tr}(\Sigma^{-1}(\lambda - \mu)(\lambda - \mu)^T)\\
    &amp;= \text{tr}(\Sigma^{-1}\sum_i (\eta_i - \lambda_i)^2) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu)
\end{aligned}
\]</span></p>
<p>Now, let’s take the expectation:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [(\eta - \mu)^T &amp;\Sigma^{-1} (\eta - \mu)] \\
    &amp;= \mathbb{E}_q [\text{tr}(\Sigma^{-1}\sum_i (\eta_i - \lambda_i)^2) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu)] \\
    &amp;= \text{tr}(\Sigma^{-1} \mathbb{E}_q[\sum_i (\eta_i - \lambda_i)^2]) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) 
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}_q[\sum_i (\eta_i - \lambda_i)^2]\)</span> is simply the variance of <span class="math inline">\(eta\)</span> – i.e., <span class="math inline">\(\nu^2\)</span> – which we’ll place on the diagonal of a square matrix.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q [(\eta - \mu)^T &amp;\Sigma^{-1} (\eta - \mu)] \\
    &amp;= \text{tr}(\Sigma^{-1} \mathbb{E}_q[\sum_i (\eta_i - \lambda_i)^2]) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) \\
    &amp;= \text{tr}(\text{diag}(\nu^2)\Sigma^{-1}) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu)
\end{aligned}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
\log p(\eta|\mu,\Sigma) &amp;= \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2\pi - \frac{1}{2} \mathbb{E}_q [(\eta - \mu)^T \Sigma^{-1} (\eta - \mu)] \\
&amp;= \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2\pi - \frac{1}{2} (\text{tr}(\text{diag}(\nu^2)\Sigma^{-1}) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu))\\
&amp;= \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2\pi - \frac{1}{2} (\text{tr}(\text{diag}(\nu^2)\Sigma^{-1}) + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu))\\
&amp;= \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2\pi - \frac{1}{2} \text{tr}(\text{diag}(\nu^2)\Sigma^{-1}) - \frac{1}{2}(\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu)
\end{aligned}
\]</span></p>
</div>
<div id="entropy-of-lambda-nu-and-phi" class="section level3">
<h3><span class="header-section-number">12.4.6</span> Entropy of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\nu\)</span>, and <span class="math inline">\(\phi\)</span></h3>
<p>Again, we can look these up.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{H}_q [\lambda,\nu^2] &amp;= \frac{1}{2} \sum_i \log[2 \pi e \nu_i^2] \\
        &amp;= \frac{1}{2} \sum_i  (\log 2\pi + 1 + \log \nu_i^2)\\
\mathbb{H}_q [\phi_{d,n}] &amp;= -\sum_i \phi_{d,n,i} \log \phi_{d,n,i}
\end{aligned} 
\]</span></p>
</div>
<div id="complete-objective-function-2" class="section level3">
<h3><span class="header-section-number">12.4.7</span> Complete Objective Function</h3>
<p>Our objective function is then</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_q &amp;[\log  p(\eta,z,w|\mu, \Sigma, \beta)] - \mathbb{E}_q [\log q(\eta,z)] \\
    &amp;= \mathbb{E}_q [\log p(\eta|\mu,\Sigma)] + \mathbb{E}_q [\log p(z|\eta)] + \mathbb{E}_q [\log p(w|z,\beta)] + \mathbb{H}_q [\lambda,\nu^2] + \mathbb{H}_q[\phi] \\
    &amp;= \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2\pi - \frac{1}{2} \text{tr}(\text{diag}(\nu^2)\Sigma^{-1}) - \frac{1}{2}(\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) \\
    &amp;\qquad + \sum_n (\sum_i \lambda_i \phi_{n,i} - \log \xi - \xi^{-1} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}] + 1) \\
    &amp;\qquad + \sum_v \sum_i 1[w_{d,n}=v] \phi_{n,i} \log \beta_{i,v} \\
    &amp;\qquad + \frac{1}{2} \sum_i  (\log 2\pi + 1 + \log \nu_i^2) - \sum_i \phi_{d,n,i} \log \phi_{d,n,i}
\end{aligned} 
\]</span></p>
</div>
<div id="parameter-optimization-2" class="section level3">
<h3><span class="header-section-number">12.4.8</span> Parameter Optimization</h3>
<div id="optimization-for-beta-1" class="section level4">
<h4><span class="header-section-number">12.4.8.1</span> Optimization for <span class="math inline">\(\beta\)</span></h4>
<p>This is the same as LDA:</p>
<p><span class="math display">\[
\beta_{i,v} \propto \sum_n \sum_i \sum_v 1[w_n = v] \phi_{n,i}
\]</span></p>
</div>
<div id="optimization-for-mu" class="section level4">
<h4><span class="header-section-number">12.4.8.2</span> Optimization for <span class="math inline">\(\mu\)</span>}</h4>
<p>Extracting the terms involving <span class="math inline">\(\mu\)</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \mu} &amp;= \frac{\partial}{\partial \mu}[- \frac{1}{2} \sum_d (\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)] \\
    &amp;= \frac{\partial}{\partial \mu}[- \frac{1}{2}\sum_d (\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)] \\
    &amp;= (-\frac{1}{2})(-2) \Sigma^{-1}\sum_d(\lambda_d-\mu)\\
    &amp;= \Sigma^{-1}\sum_d(\lambda_d-\mu)
\end{aligned} 
\]</span></p>
<p>Set it to zero.</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= \Sigma^{-1}\sum_d(\lambda_d-\mu)\\
    &amp;= \sum_d \lambda_d - \sum_d \mu \\
    &amp;= \sum_d \lambda_d - D \mu \\
D \mu &amp;= \sum_d \lambda_d \\
\mu &amp;= \frac{1}{D} \sum_d \lambda_d
\end{aligned} 
\]</span></p>
</div>
<div id="optimization-for-sigma" class="section level4">
<h4><span class="header-section-number">12.4.8.3</span> Optimization for <span class="math inline">\(\Sigma\)</span></h4>
<p>Let’s repeat the process, but for terms associated with <span class="math inline">\(\Sigma\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \Sigma} &amp;= \frac{\partial}{\partial \Sigma}[\sum_d(-\frac{1}{2} \log |\Sigma| - \frac{1}{2} \text{tr}(\text{diag}(\nu_d^2)\Sigma^{-1}) - \frac{1}{2}(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu))] \\
&amp;= -\frac{1}{2}\frac{\partial}{\partial \Sigma} [\sum_d (\log |\Sigma| + \text{tr}(\text{diag}(\nu_d^2)\Sigma^{-1}) + (\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu))] \\
&amp;= -\frac{1}{2}\frac{\partial}{\partial \Sigma} [D \log |\Sigma| + \text{tr}(\sum_d\text{diag}(\nu_d^2)\Sigma^{-1}) + \sum_d(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)] \\
&amp;= -\frac{1}{2}(D \frac{\partial}{\partial \Sigma}[\log |\Sigma|] + \frac{\partial}{\partial \Sigma}[\text{tr}(\sum_d\text{diag}(\nu_d^2)\Sigma^{-1})] \\
&amp;\qquad + \frac{\partial}{\partial \Sigma}[\sum_d(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)]) 
\end{aligned} 
\]</span></p>
<p>Let’s first focus on <span class="math inline">\(\frac{\partial}{\partial \Sigma} [\sum_d(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)]\)</span>. We can use the trace trick we saw above to make this derivative easier to work with:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial \Sigma} &amp;[\sum_d(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)] \\
&amp;= \frac{\partial}{\partial \Sigma} [\text{tr}(\sum_d(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu))]\\
&amp;= \frac{\partial}{\partial \Sigma} [\text{tr}(\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T\Sigma^{-1})]\\
&amp;= \text{tr}(\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T\frac{\partial}{\partial \Sigma}\Sigma^{-1})\\
&amp;= \text{tr}(\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T (-\Sigma^{-1}\partial \Sigma \Sigma^{-1} ))\\
&amp;= \text{tr}(-\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1})
\end{aligned}
\]</span></p>
<p>Now, let’s deal with <span class="math inline">\(\frac{\partial}{\partial \Sigma}[\log |\Sigma|]\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial \Sigma}[\log |\Sigma|] &amp;= \text{tr}(\Sigma^{-1} \partial \Sigma)
\end{aligned}
\]</span></p>
<p>And finally, <span class="math inline">\(\frac{\partial}{\partial \Sigma}[\text{tr}(\sum_d\text{diag}(\nu_d^2)\Sigma^{-1})]\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial \Sigma}[\text{tr}(\sum_d\text{diag}(\nu_d^2)\Sigma^{-1})] &amp;= \text{tr}(\sum_d\text{diag}(\nu_d^2) (-\Sigma^{-1}\partial \Sigma \Sigma^{-1} ))\\
&amp;= \text{tr}(-\sum_d\text{diag}(\nu_d^2) (\Sigma^{-1}\partial \Sigma \Sigma^{-1} ))
\end{aligned}
\]</span></p>
<p>Now let’s replace these derivatives in our original equation.</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \Sigma} &amp;= -\frac{1}{2}(D \frac{\partial}{\partial \Sigma}[\log |\Sigma|] + \frac{\partial}{\partial \Sigma}[\text{tr}(\sum_d\text{diag}(\nu_d^2)\Sigma^{-1})] \\
&amp;\qquad + \frac{\partial}{\partial \Sigma}[\sum_d(\lambda_d - \mu)^T\Sigma^{-1}(\lambda_d - \mu)])\\
&amp;= -\frac{1}{2}(D \text{tr}(\Sigma^{-1} \partial \Sigma) + \text{tr}(-\sum_d\text{diag}(\nu_d^2) (\Sigma^{-1}\partial \Sigma \Sigma^{-1} )) \\
&amp;\qquad + \text{tr}(-\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1}))\\
&amp;= -\frac{1}{2}(\text{tr}(D\Sigma^{-1} \partial \Sigma) + \text{tr}(-\sum_d\text{diag}(\nu_d^2) (\Sigma^{-1}\partial \Sigma \Sigma^{-1} )) \\
&amp;\qquad + \text{tr}(-\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1}))\\
&amp;= -\frac{1}{2}(\text{tr}(D\Sigma^{-1} \partial \Sigma - \sum_d\text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma \Sigma^{-1}  \\
&amp;\qquad - \sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1}))\\
\end{aligned} 
\]</span></p>
<p>Then we set this to zero and solve.</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= -\frac{1}{2}(\text{tr}(D\Sigma^{-1} \partial \Sigma - \sum_d\text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma \Sigma^{-1}  \\
&amp;\qquad - \sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1}))\\
&amp;= \text{tr}(D\Sigma^{-1} \partial \Sigma - \sum_d\text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma \Sigma^{-1}  \\
&amp;\qquad - \sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1})\\
&amp;= \text{tr}(D\Sigma^{-1} \partial \Sigma) - \text{tr}(\sum_d\text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma \Sigma^{-1})  \\
&amp;\qquad - \text{tr}(\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1})\\
&amp;= \text{tr}(D\Sigma^{-1} \partial \Sigma) - \text{tr}(\sum_d\text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma \Sigma^{-1})  \\
&amp;\qquad - \text{tr}(\sum_d(\lambda_d - \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1})\\
\end{aligned} 
\]</span></p>
<p>Then we exploit <span class="math inline">\(\text{tr}(ABCD) = \text{tr}(DABC)\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\text{tr}(D\Sigma^{-1} \partial \Sigma) &amp;= \text{tr}(\sum_d\text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma \Sigma^{-1}) + \text{tr}(\sum_d(\lambda_d + \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma \Sigma^{-1})\\
&amp;= \text{tr}(\sum_d \Sigma^{-1}) \text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma  + \text{tr}(\sum_d \Sigma^{-1} (\lambda_d + \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma)\\
D\Sigma^{-1} \partial \Sigma &amp;= \sum_d \Sigma^{-1} \text{diag}(\nu_d^2) \Sigma^{-1}\partial \Sigma  + \sum_d \Sigma^{-1} (\lambda_d + \mu)(\lambda_d - \mu)^T \Sigma^{-1}\partial \Sigma\\
D\Sigma^{-1} \partial \Sigma &amp;= (\sum_d \Sigma^{-1} \text{diag}(\nu_d^2) \Sigma^{-1}  + \sum_d \Sigma^{-1} (\lambda_d + \mu)(\lambda_d - \mu)^T \Sigma^{-1})\partial \Sigma\\
D\Sigma^{-1} &amp;= \sum_d \Sigma^{-1} \text{diag}(\nu_d^2) \Sigma^{-1}  + \sum_d \Sigma^{-1} (\lambda_d + \mu)(\lambda_d - \mu)^T \Sigma^{-1}\\
D &amp;= \sum_d \Sigma^{-1} \text{diag}(\nu_d^2)  + \sum_d \Sigma^{-1} (\lambda_d + \mu)(\lambda_d - \mu)^T\\
\Sigma D &amp;= \sum_d \text{diag}(\nu_d^2)  + \sum_d (\lambda_d + \mu)(\lambda_d - \mu)^T\\
\Sigma &amp;= \frac{1}{D}(\sum_d \text{diag}(\nu_d^2)  + (\lambda_d + \mu)(\lambda_d - \mu)^T)
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-xi" class="section level4">
<h4><span class="header-section-number">12.4.8.4</span> Optimization for <span class="math inline">\(\xi\)</span></h4>
<p>Isolating terms, we get</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \xi} &amp;= \frac{\partial}{\partial \xi}[ - \log \xi - \xi^{-1} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}]]\\
&amp;= - \frac{1}{\xi} + \frac{1}{\xi^{2}} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}]
\end{aligned}
\]</span></p>
<p>Then we set this equal to zero:</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= - \frac{1}{\xi} + \frac{1}{\xi^{2}} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}]\\
\frac{1}{\xi} &amp;= \frac{1}{\xi^{2}} \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}]\\
\xi &amp;=  \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}]
\end{aligned}
\]</span></p>
</div>
<div id="optimization-for-phi-2" class="section level4">
<h4><span class="header-section-number">12.4.8.5</span> Optimization for <span class="math inline">\(\phi\)</span></h4>
<p>Again, isolating terms and adding the constraint <span class="math inline">\(\sum_n \phi_{n,i} = 1\)</span>, we get</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \phi_{n,i}} &amp;= \frac{\partial}{\partial \phi_{n,i}}[\gamma_i \phi_{n,i} + \sum_v 1[w_{n}=v]\phi_{n,i} \log \beta_{i,v} -\phi_{n,i} \log \phi_{n,i} + \zeta(\sum_n \phi_{n,i}-1)]\\
&amp;= \gamma_i + \sum_v 1[w_{n}=v]\log \beta_{i,v} - \log \phi_{n,i} - 1 + \zeta
\end{aligned}
\]</span></p>
<p>which we set to zero</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;= \gamma_i + \sum_v 1[w_{n}=v]\log \beta_{i,v} - \log \phi_{n,i} - 1 + \zeta\\
\log \phi_{n,i} &amp;= \gamma_i + \sum_v 1[w_{n}=v]\log \beta_{i,v} - 1 + \zeta\\
\phi_{n,i} &amp;= \exp[\gamma_i]\exp[\log \beta_{i,v}^{1[w_{n}=v]}]\exp[ - 1 + \zeta]\\
&amp;= \exp[\gamma_i]\beta_{i,v}^{1[w_{n}=v]}\exp[ - 1 + \zeta]\\
&amp;= \exp[ - 1 + \zeta]\beta_{i,v}^{1[w_{n}=v]}\exp[\gamma_i]\\
&amp;= c_{n,i}\beta_{i,v}^{1[w_{n}=v]}\exp[\gamma_i]\\
&amp;\propto \beta_{i,v}^{1[w_{n}=v]}\exp[\gamma_i]
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(c_{n,i}=\exp[ - 1 + \zeta]\)</span>. As in LDA, we will calculate <span class="math inline">\(\phi\)</span> and then normalize to satisfy this constraint.</p>
</div>
<div id="optimization-for-lambda" class="section level4">
<h4><span class="header-section-number">12.4.8.6</span> Optimization for <span class="math inline">\(\lambda\)</span></h4>
<p>Isolating terms and rewriting some of the sums as vectors, we get the following expression.</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \lambda} &amp;= \frac{\partial}{\partial \lambda}[- \frac{1}{2}(\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) + \sum_n \sum_i \lambda_i \phi_{n,i} - \xi^{-1} \sum_n \sum_i^{K} \exp [\lambda_i + \frac{\nu_i^2}{2}]]\\
&amp;= \frac{\partial}{\partial \lambda}[- \frac{1}{2}(\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) + \sum_n \lambda \phi_{n,\cdot} - \xi^{-1} \sum_n \exp [\lambda + \frac{\nu^2}{2}]]\\
&amp;= \frac{\partial}{\partial \lambda}[- \frac{1}{2}(\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) + \sum_n \lambda \phi_{n,\cdot} - \frac{N}{\xi} \exp [\lambda + \frac{\nu^2}{2}]]\\
\end{aligned} 
\]</span></p>
<p>Now, let’s compute the derivative:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \lambda} &amp;= \frac{\partial}{\partial \lambda}[- \frac{1}{2}(\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu) + \sum_n \lambda \phi_{n,\cdot} - \frac{N}{\xi} \exp [\lambda + \frac{\nu^2}{2}]]\\
&amp;= - \frac{1}{2}(2\Sigma^{-1}(\lambda - \mu)) + \sum_n \phi_{n,\cdot} - \frac{N}{\xi} \exp [\lambda + \frac{\nu^2}{2}]\\
&amp;= - \Sigma^{-1}(\lambda - \mu) + \sum_n \phi_{n,\cdot} - \frac{N}{\xi} \exp [\lambda + \frac{\nu^2}{2}]
\end{aligned} 
\]</span></p>
<p>This cannot be solved analytically, but we can apply gradient descent.</p>
</div>
<div id="optimization-for-nu2" class="section level4">
<h4><span class="header-section-number">12.4.8.7</span> Optimization for <span class="math inline">\(\nu^2\)</span></h4>
<p>For our last optimization, we’ll again isolate terms. Let’s rewrite <span class="math inline">\(\nu_i^2\)</span> as <span class="math inline">\(\delta_i := \nu_i^2\)</span> to prevent confusion.</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \delta}  &amp;= \frac{\partial}{\partial \delta_i}[- \frac{1}{2} \delta_i\Sigma_{ii}^{-1} 
    - \xi^{-1} \sum_n \exp [\lambda_i + \frac{\delta_i}{2}] + \frac{1}{2} \log \delta_i ]\\
    &amp;= \frac{\partial}{\partial \delta_i}[- \frac{1}{2} \delta_i\Sigma_{ii}^{-1}
    - \frac{N}{\xi}\exp [\lambda_i + \frac{\delta_i}{2}] + \frac{1}{2} \log \delta_i ]\\
    &amp;= -\frac{1}{2}\Sigma_{ii}^{-1} - \frac{N}{2\xi}\exp [\lambda_i + \frac{\delta_i}{2}] + \frac{1}{2\delta}\\
\end{aligned} 
\]</span></p>
<p>which, in terms of <span class="math inline">\(\nu^2\)</span>, is</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \nu^2} &amp;= -\frac{1}{2}\Sigma_{ii}^{-1}   - \frac{N}{2\xi}\exp [\lambda_i + \frac{\nu^2}{2}] + \frac{1}{2 \nu^2}\\
\end{aligned} 
\]</span></p>
<p>Like <span class="math inline">\(\frac{\partial L}{\partial \lambda}\)</span>, this cannot be solved analytically. We can use Newton’s method, but we first need to solve for the Hessian:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial^2 L}{\partial \delta^2}  &amp;= \frac{\partial}{\partial \delta_i}[-\frac{1}{2}\Sigma_{ii}^{-1}  - \frac{N}{2\xi}\exp [\lambda_i + \frac{\delta_i}{2}] + \frac{1}{2\delta}]\\
&amp;= - \frac{N}{4\xi}\exp [\lambda_i + \frac{\delta_i}{2}] - \frac{1}{2\delta^2}\\
\end{aligned} 
\]</span></p>
<p>which in terms of <span class="math inline">\(\nu^2\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial^2 L}{\partial (\nu^2)^2} &amp;= - \frac{N}{4\xi}\exp [\lambda_i + \frac{\nu^2}{2}] - \frac{1}{2(\nu^2)^2}\\
\end{aligned} 
\]</span></p>
</div>
</div>
</div>
<div id="dirichlet-distribution" class="section level2">
<h2><span class="header-section-number">12.5</span> Dirichlet Distribution</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a></code></pre></div>
<pre><code>## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ ggplot2 3.2.1     ✓ purrr   0.3.3
## ✓ tibble  2.1.3     ✓ dplyr   0.8.4
## ✓ tidyr   1.0.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">library</span>(ggtern)</a></code></pre></div>
<pre><code>## Registered S3 methods overwritten by &#39;ggtern&#39;:
##   method           from   
##   +.gg             ggplot2
##   grid.draw.ggplot ggplot2
##   plot.ggplot      ggplot2
##   print.ggplot     ggplot2</code></pre>
<pre><code>## --
## Remember to cite, run citation(package = &#39;ggtern&#39;) for further info.
## --</code></pre>
<pre><code>## 
## Attaching package: &#39;ggtern&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, aes, annotate, calc_element, ggplot, ggplot_build, ggplotGrob,
##     ggplot_gtable, ggsave, layer_data, theme, theme_bw, theme_classic,
##     theme_dark, theme_gray, theme_light, theme_linedraw, theme_minimal,
##     theme_void</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">library</span>(gtools)</a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="kw">library</span>(cowplot)</a></code></pre></div>
<pre><code>## 
## ********************************************************</code></pre>
<pre><code>## Note: As of version 1.0.0, cowplot does not change the</code></pre>
<pre><code>##   default ggplot2 theme anymore. To recover the previous</code></pre>
<pre><code>##   behavior, execute:
##   theme_set(theme_cowplot())</code></pre>
<pre><code>## ********************************************************</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">dsim &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">alpha=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),draws){</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">  </a>
<a class="sourceLine" id="cb17-3" data-line-number="3">  <span class="cf">if</span> (<span class="kw">length</span>(alpha)<span class="op">==</span><span class="dv">1</span>) alpha &lt;-<span class="st"> </span><span class="kw">rep</span>(alpha,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">  </a>
<a class="sourceLine" id="cb17-5" data-line-number="5">  elements &lt;-<span class="st"> </span><span class="kw">length</span>(alpha)</a>
<a class="sourceLine" id="cb17-6" data-line-number="6">  </a>
<a class="sourceLine" id="cb17-7" data-line-number="7">  dis &lt;-<span class="st"> </span><span class="kw">rdirichlet</span>(draws, alpha)</a>
<a class="sourceLine" id="cb17-8" data-line-number="8">  disz &lt;-<span class="st"> </span><span class="kw">ddirichlet</span>(dis, alpha)</a>
<a class="sourceLine" id="cb17-9" data-line-number="9">  triangle &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(dis,disz))</a>
<a class="sourceLine" id="cb17-10" data-line-number="10">  <span class="kw">names</span>(triangle) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>,<span class="st">&quot;C&quot;</span>,<span class="st">&quot;d&quot;</span>)</a>
<a class="sourceLine" id="cb17-11" data-line-number="11">  </a>
<a class="sourceLine" id="cb17-12" data-line-number="12">  df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="st">&quot;Atom&quot;</span>=<span class="kw">as.factor</span>(<span class="kw">rep</span>(LETTERS[<span class="dv">1</span><span class="op">:</span>elements], <span class="dt">each=</span>draws)),</a>
<a class="sourceLine" id="cb17-13" data-line-number="13">                   <span class="st">&quot;Probability&quot;</span>=<span class="kw">matrix</span>(dis,<span class="dt">ncol=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb17-14" data-line-number="14">                   <span class="st">&quot;Draw&quot;</span>=<span class="kw">as.factor</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>draws,elements)))</a>
<a class="sourceLine" id="cb17-15" data-line-number="15">  </a>
<a class="sourceLine" id="cb17-16" data-line-number="16">  p &lt;-<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">subset</span>(df,Draw <span class="op">%in%</span><span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">15</span>), <span class="kw">aes</span>(<span class="dt">x=</span>Atom,<span class="dt">y=</span>Probability,<span class="dt">ymin=</span><span class="dv">0</span>,<span class="dt">ymax=</span>Probability)) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-17" data-line-number="17"><span class="st">    </span><span class="kw">geom_linerange</span>(<span class="dt">colour=</span><span class="st">&quot;Blue&quot;</span>,<span class="dt">size=</span><span class="fl">1.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-18" data-line-number="18"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">colour=</span><span class="st">&quot;Blue&quot;</span>,<span class="dt">size=</span><span class="dv">4</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-19" data-line-number="19"><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">lim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-20" data-line-number="20"><span class="st">    </span><span class="kw">facet_wrap</span>(<span class="op">~</span>Draw,<span class="dt">ncol=</span><span class="dv">5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-21" data-line-number="21"><span class="st">    </span><span class="kw">theme</span>(<span class="dt">panel.background =</span> <span class="kw">element_rect</span>(),</a>
<a class="sourceLine" id="cb17-22" data-line-number="22">          <span class="dt">title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>),</a>
<a class="sourceLine" id="cb17-23" data-line-number="23">          <span class="dt">strip.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">13</span>),</a>
<a class="sourceLine" id="cb17-24" data-line-number="24">          <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">13</span>),</a>
<a class="sourceLine" id="cb17-25" data-line-number="25">          <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">18</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-26" data-line-number="26"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="kw">bquote</span>(alpha <span class="op">==</span><span class="st"> </span>(.(<span class="kw">paste</span>(alpha,<span class="dt">collapse=</span><span class="st">&quot; &quot;</span>)))))</a>
<a class="sourceLine" id="cb17-27" data-line-number="27">  </a>
<a class="sourceLine" id="cb17-28" data-line-number="28">  <span class="cf">if</span> (elements<span class="op">==</span><span class="dv">3</span>){</a>
<a class="sourceLine" id="cb17-29" data-line-number="29">    q &lt;-<span class="st">    </span><span class="kw">ggtern</span>(<span class="dt">data=</span>triangle,<span class="kw">aes</span>(<span class="dt">x=</span>C,<span class="dt">y=</span>A,<span class="dt">z=</span>B)) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-30" data-line-number="30"><span class="st">      </span><span class="kw">geom_density_tern</span>(<span class="kw">aes</span>(<span class="dt">weight=</span>d,<span class="dt">color=</span>..level..),<span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-31" data-line-number="31"><span class="st">      </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;black&quot;</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">alpha=</span>.<span class="dv">5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-32" data-line-number="32"><span class="st">      </span><span class="kw">scale_color_gradient</span>(<span class="dt">low=</span><span class="st">&quot;red&quot;</span>,<span class="dt">high=</span><span class="st">&quot;white&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-33" data-line-number="33"><span class="st">      </span><span class="kw">theme_rgbw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb17-34" data-line-number="34"><span class="st">      </span><span class="kw">theme</span>(<span class="dt">panel.background =</span> <span class="kw">element_rect</span>(),</a>
<a class="sourceLine" id="cb17-35" data-line-number="35">            <span class="dt">tern.axis.arrow.text =</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">28</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>),</a>
<a class="sourceLine" id="cb17-36" data-line-number="36">            <span class="dt">tern.axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">18</span>)</a>
<a class="sourceLine" id="cb17-37" data-line-number="37">            ) <span class="op">+</span></a>
<a class="sourceLine" id="cb17-38" data-line-number="38"><span class="st">      </span><span class="kw">guides</span>(<span class="dt">color=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb17-39" data-line-number="39">    </a>
<a class="sourceLine" id="cb17-40" data-line-number="40">    <span class="kw">print</span>(<span class="kw">plot_grid</span>(q,p,<span class="dt">ncol=</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb17-41" data-line-number="41">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb17-42" data-line-number="42">    <span class="kw">print</span>(p)</a>
<a class="sourceLine" id="cb17-43" data-line-number="43">  }</a>
<a class="sourceLine" id="cb17-44" data-line-number="44">}</a></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">dsim</span>(<span class="dv">1</span>,<span class="dt">draws=</span><span class="dv">100</span>)</a></code></pre></div>
<p><img src="bioinformatics_class_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">dsim</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">1</span>),<span class="dt">draws=</span><span class="dv">100</span>)</a></code></pre></div>
<p><img src="bioinformatics_class_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">dsim</span>(.<span class="dv">05</span>,<span class="dt">draws=</span><span class="dv">100</span>)</a></code></pre></div>
<p><img src="bioinformatics_class_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lasso.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ml.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bioinformatics_class.pdf", "bioinformatics_class.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
